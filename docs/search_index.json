[["index.html", "Quantitative Agricultural Economics Chapter 1 Down the rabbit-hole", " Quantitative Agricultural Economics Raymond L. Tremblay 2024-12-26 Chapter 1 Down the rabbit-hole Día 1: Introducción a RStudio y paquetes disponibles •⁠ ⁠Interacción con la Plataforma (Chapter 2) •⁠ ⁠¿Cómo buscar y bajar los paquetes estadísticos?(CHapter3) •⁠ ⁠¿Cómo entender las programaciones predeterminadas de los paquetes estadísticos? •⁠ ⁠¿Cómo buscar ayuda? •⁠ ⁠¿Como subir los datos, hacer cambio de nombre de variables, eliminar datos, encontrar missing values, ver estadística descriptiva, ver tipo de variables, etc.) •⁠ ⁠Prueba Shapito-Wilk •⁠ ⁠Prueba T •⁠ ⁠Prueba •⁠ ⁠Prueba Kruskal Wallis Día 2: Visualización de datos - gráficas •⁠ ⁠Scatter plot (simple y con multiples variables) •⁠ ⁠Scatter plot matrix •⁠ ⁠Gráfica linear (simple y con dos variables) •⁠ ⁠Pie chart •⁠ ⁠Histogramas (simple y con múltiples variables ) •⁠ ⁠Kernel density •⁠ ⁠Box plot •⁠ ⁠Otras gáficas Día 3: Modelos econométricos y suposiciones •⁠ ⁠Regresión por mínimos cuadrados ordinarios •⁠ ⁠Verificación de supuestos •⁠ ⁠Regresión cuantílica •⁠ ⁠Regresión logit y pruebas/verificaciones postestimación (% de predicciones correctas, efectos marginales, goodness of fit, descripción de los datos utilizados, etc.) Día 4: Modelos econométricos y suposiciones •⁠ ⁠Regresión logit multinomial y pruebas/verificaciones postestimación •⁠ ⁠Regresión beta •⁠ ⁠Modelos VAR (Modelos vectoriales autoregresivos) "],["introduction-to-r-and-posit-rstudio.html", "Chapter 2 Introduction to R and POSIT (RStudio)", " Chapter 2 Introduction to R and POSIT (RStudio) "],["packages-and-functions-in-r.html", "Chapter 3 Packages and Functions in R 3.1 What is an R package 3.2 Understanding funcions in R 3.3 Installing and loading packages 3.4 Surfing the web for packages and functions", " Chapter 3 Packages and Functions in R 3.1 What is an R package 3.2 Understanding funcions in R 3.3 Installing and loading packages 3.4 Surfing the web for packages and functions "],["data.html", "Chapter 4 Data 4.1 Adding data directly to the document 4.2 Loading data from a file 4.3 Loading data from the web 4.4 ", " Chapter 4 Data 4.1 Adding data directly to the document 4.1.1 Using the function tibble 4.2 Loading data from a file 4.2.1 .csv file 4.2.2 .excel file 4.2.3 .txt file 4.2.4 .spss file 4.3 Loading data from the web 4.4 "],["data-wrangling.html", "Chapter 5 Data wrangling 5.1 cleaning the names of the variables with janitor 5.2 Changing the names to variables 5.3 Deleting variables 5.4 changing names of levels/factors 5.5 Finding missing values", " Chapter 5 Data wrangling 5.1 cleaning the names of the variables with janitor 5.2 Changing the names to variables 5.3 Deleting variables 5.4 changing names of levels/factors 5.5 Finding missing values "],["understanding-your-data.html", "Chapter 6 Understanding your data 6.1 Functions to evaluate your data 6.2 dimensions of the data 6.3 structure of the data 6.4 Levels of the variables", " Chapter 6 Understanding your data 6.1 Functions to evaluate your data 6.2 dimensions of the data 6.3 structure of the data 6.4 Levels of the variables "],["test-of-normality.html", "Chapter 7 Test of normality 7.1 Shapiro-Wilk normality test 7.2 The shapiro-Francia normality test 7.3 Anderson-Darling normality test 7.4 Cramer-von Mises test of normality 7.5 Lilliefors test of normality 7.6 Visualizing the data 7.7 Remove the outlier from the day1 hygiene score", " Chapter 7 Test of normality Add and activate packages. NOTE THE NEW FUNCTION to install all packages ONLY IF Needed…..pacman::p_load(car, pastecs, psych, ggplot2) library(readr) DownloadFestival &lt;- read_csv(&quot;Data/DownloadFestival.csv&quot;) ## Rows: 810 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (4): ticknumb, day1, day2, day3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(DownloadFestival) ## # A tibble: 6 × 5 ## ticknumb gender day1 day2 day3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2111 Male 2.64 1.35 1.61 ## 2 2229 Female 0.97 1.41 0.29 ## 3 2338 Male 0.84 NA NA ## 4 2384 Female 3.03 NA NA ## 5 2401 Female 0.88 0.08 NA ## 6 2405 Male 0.85 NA NA dlf &lt;- DownloadFestival 7.0.0.1 Understanding the data METADATA The data is from a festival in UK. The data is about the hygiene score of the participants. The hygiene score is a score from 0 to 4. The scale is as follows: 0 = You smell like a rotting corpse 4 = You smell like of sweet roses Multiple variables #Two alternative ways to describe multiple variables. psych::describe(cbind(dlf$day1, dlf$day2, dlf$day3)) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 810 1.79 0.94 1.79 1.77 0.70 0.02 20.02 20.00 8.83 168.97 0.03 ## X2 2 264 0.96 0.72 0.79 0.87 0.61 0.00 3.44 3.44 1.08 0.76 0.04 ## X3 3 123 0.98 0.71 0.76 0.90 0.61 0.02 3.41 3.39 1.01 0.59 0.06 psych::describe(dlf[,c(&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;)]) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## day1 1 810 1.79 0.94 1.79 1.77 0.70 0.02 20.02 20.00 8.83 168.97 0.03 ## day2 2 264 0.96 0.72 0.79 0.87 0.61 0.00 3.44 3.44 1.08 0.76 0.04 ## day3 3 123 0.98 0.71 0.76 0.90 0.61 0.02 3.41 3.39 1.01 0.59 0.06 7.1 Shapiro-Wilk normality test Test of normality, Shapiro Wilks Test is a test of normality in frequentist statistics. It tests the null hypothesis that the data was drawn from a normal distribution. The test is based on the correlation between the data and the corresponding normal scores. The test is considered to be the most powerful test for normality, particularly for small sample sizes. However it is sensitive to the presence of outliers. and tends to be conservative when the sample size is large. in other words it tends to reject the null model when sample size are large and the data is sufficient normal. library(pastecs) stat.desc(dlf$day3, basic = FALSE, norm = TRUE) # &quot;norm=TRUE&quot;&quot; is to calculate the Shapiro Wilk Test ## median mean SE.mean CI.mean.0.95 var std.dev ## 7.600000e-01 9.765041e-01 6.404352e-02 1.267805e-01 5.044934e-01 7.102770e-01 ## coef.var skewness skew.2SE kurtosis kurt.2SE normtest.W ## 7.273672e-01 1.007813e+00 2.309035e+00 5.945454e-01 6.862946e-01 9.077516e-01 ## normtest.p ## 3.804486e-07 stat.desc(cbind(dlf$day1, dlf$day2, dlf$day3), basic = FALSE, norm = TRUE) ## V1 V2 V3 ## median 1.790000e+00 7.900000e-01 7.600000e-01 ## mean 1.793358e+00 9.609091e-01 9.765041e-01 ## SE.mean 3.318617e-02 4.436095e-02 6.404352e-02 ## CI.mean.0.95 6.514115e-02 8.734781e-02 1.267805e-01 ## var 8.920705e-01 5.195239e-01 5.044934e-01 ## std.dev 9.444949e-01 7.207801e-01 7.102770e-01 ## coef.var 5.266627e-01 7.501022e-01 7.273672e-01 ## skewness 8.832504e+00 1.082811e+00 1.007813e+00 ## skew.2SE 5.140707e+01 3.611574e+00 2.309035e+00 ## kurtosis 1.689671e+02 7.554615e-01 5.945454e-01 ## kurt.2SE 4.923139e+02 1.264508e+00 6.862946e-01 ## normtest.W 6.539142e-01 9.083191e-01 9.077516e-01 ## normtest.p 1.545986e-37 1.281630e-11 3.804486e-07 7.2 The shapiro-Francia normality test The Shapiro-Francia test is known to perform well, see also the comments by Royston (1993). The expected ordered quantiles from the standard normal distribution are approximated by qnorm(ppoints(x, a = 3/8)), being slightly different from the approximation qnorm(ppoints(x, a = 1/2)) used for the normal quantile-quantile plot by qqnorm for sample sizes greater than 10. Royston, P. (1993): A pocket-calculator algorithm for the Shapiro-Francia test for non-normality: an application to medicine. Statistics in Medicine, 12, 181–184. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. sf &lt;- sf.test(dlf$day3) sf ## ## Shapiro-Francia normality test ## ## data: dlf$day3 ## W = 0.90903, p-value = 2.348e-06 7.3 Anderson-Darling normality test The Anderson-Darling test is a modification of the Kolmogorov-Smirnov test that gives more weight to the tails of the distribution. It is a more powerful test than the Kolmogorov-Smirnov test, but it is also more sensitive to departures from normality in the center of the distribution. Stephens, M.A. (1986): Tests based on EDF statistics. In: D’Agostino, R.B. and Stephens, M.A., eds.: Goodness-of-Fit Techniques. Marcel Dekker, New York. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. ad.test(dlf$day1) ## ## Anderson-Darling normality test ## ## data: dlf$day1 ## A = 15.406, p-value &lt; 2.2e-16 7.4 Cramer-von Mises test of normality The Cramer-von Mises test is a modification of the Anderson-Darling test that gives more weight to the center of the distribution. It is a more powerful test than the Anderson-Darling test, but it is also more sensitive to departures from normality in the tails of the distribution. Stephens, M.A. (1986): Tests based on EDF statistics. In: D’Agostino, R.B. and Stephens, M.A., eds.: Goodness-of-Fit Techniques. Marcel Dekker, New York. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. cvm.test(dlf$day1) ## Warning in cvm.test(dlf$day1): p-value is smaller than 7.37e-10, cannot be ## computed more accurately ## ## Cramer-von Mises normality test ## ## data: dlf$day1 ## W = 1.9628, p-value = 7.37e-10 7.5 Lilliefors test of normality The Lilliefors (Kolomorov-Smirnov) test is the most famous EDF omnibus test for normality. Compared to the Anderson-Darling test and the Cramer-von Mises test it is known to perform worse. Although the test statistic obtained from lillie.test(x) is the same as that obtained from ks.test(x, “pnorm”, mean(x), sd(x)), it is not correct to use the p-value from the latter for the composite hypothesis of normality (mean and variance unknown), since the distribution of the test statistic is different when the parameters are estimated. Dallal, G.E. and Wilkinson, L. (1986): An analytic approximation to the distribution of Lilliefors’ test for normality. The American Statistician, 40, 294–296. Stephens, M.A. (1974): EDF statistics for goodness of fit and some comparisons. Journal of the American Statistical Association, 69, 730–737. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. lillie.test(dlf$day1) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: dlf$day1 ## D = 0.082706, p-value = 2.539e-14 7.6 Visualizing the data 7.6.1 qq-plot Add a straight line on the qqplot # This function is to add a straight line through the qqplot qqplot.data &lt;- function (vec) # argument: vector of numbers { # following four lines from base R&#39;s qqline() y &lt;- quantile(vec[!is.na(vec)], c(0.25, 0.75)) x &lt;- qnorm(c(0.25, 0.75)) slope &lt;- diff(y)/diff(x) int &lt;- y[1L] - slope * x[1L] d &lt;- data.frame(resids = vec) ggplot(d, aes(sample = resids)) + stat_qq() + geom_abline(slope = slope, intercept = int, color=&quot;red&quot;) } qqplot.data(dlf$day3) ## Warning: Removed 687 rows containing non-finite outside the scale range ## (`stat_qq()`). ggsave(&quot;qqplot.png&quot;) ## Saving 7 x 5 in image ## Warning: Removed 687 rows containing non-finite outside the scale range ## (`stat_qq()`). 7.6.2 Histogram dlf=subset(DownloadFestival) head(dlf) ## # A tibble: 6 × 5 ## ticknumb gender day1 day2 day3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2111 Male 2.64 1.35 1.61 ## 2 2229 Female 0.97 1.41 0.29 ## 3 2338 Male 0.84 NA NA ## 4 2384 Female 3.03 NA NA ## 5 2401 Female 0.88 0.08 NA ## 6 2405 Male 0.85 NA NA tail(dlf) ## # A tibble: 6 × 5 ## ticknumb gender day1 day2 day3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4749 Female 0.52 NA NA ## 2 4756 Female 2.91 0.94 NA ## 3 4758 Female 2.61 1.44 NA ## 4 4759 Female 1.47 NA NA ## 5 4760 Male 1.28 NA NA ## 6 4765 Female 1.26 NA NA hist.day1 &lt;- ggplot(dlf, aes(day1)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;white&quot;) hist.day1+ labs(x=&quot;Hygiene score on day 1&quot;, y = &quot;Density&quot;) ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. hist.day1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggsave(&quot;histogram_festival_hygiene.pdf&quot;) # Can be either be a device function (e.g. png()), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex), &quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only) ## Saving 7 x 5 in image ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 7.6.3 Density plot The Normal Distribution https://en.wikipedia.org/wiki/Normal_distribution visualization if the observed distribution follows a theoretical normal distribution Test to determine if the observed distribution follows a theoretical distribution \\[P(x)=\\frac{1}{{\\sigma\\sqrt{ 2\\pi}}}{e}^{\\frac{{(x-µ)}^{2}}{{2\\sigma}^{2}}}\\] # Ahora añadir la linea de distribución normal hist.day1 + stat_function(fun = dnorm, args = list(mean = mean(dlf$day1,na.rm = TRUE), sd = sd(dlf$day1 , na.rm = TRUE)), colour = &quot;blue&quot;, size = 1) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. shapiro.test(dlf$day1) # don&#39;t use with more than 40 a 200 data point ## ## Shapiro-Wilk normality test ## ## data: dlf$day1 ## W = 0.65391, p-value &lt; 2.2e-16 length(dlf$day1) ## [1] 810 Visualize the distribution of the data “Histogram” 7.7 Remove the outlier from the day1 hygiene score dlf\\(day1 &lt;- ifelse(dlf\\)day1 &gt; 5, NA, dlf\\(day1) df\\)Column = ifelse(df$column_to_be evaluated, replace_with_NA, otherwise_leave_as_before) dlf$day1 &lt;- ifelse(dlf$day1 &gt; 5, NA, dlf$day1) # Note here that we use ..density.. # CUAL ES LA DIFERENCIA entre densidad y frecuencia? hist.day1 &lt;- ggplot(dlf, aes(day1)) + theme(legend.position = &quot;none&quot;) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;white&quot;) + labs(x=&quot;Hygiene score on day 1&quot;, y = &quot;Density&quot;) hist.day1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_bin()`). dlf=DownloadFestival #Quantifying normality with numbers library(psych) #load the &quot;psych&quot; library, if you haven&#39;t already, for the describe() function. #Using the describe() function for a single variable. psych::describe(dlf$day2) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 264 0.96 0.72 0.79 0.87 0.61 0 3.44 3.44 1.08 0.76 0.04 Que es la varianza? The variance \\[s^{ 2 }=\\frac { \\sum _{ i=1 }^{ n }{ (x_{ i }-\\bar { x } ) } ^{ 2 } }{ n-1 } \\] Que es la desviación estandard The standard deviation of the mean \\[s=\\sqrt { s^{ 2 } } \\] Dicover the Mode. La moda en R # the mode library(modeest) ## Registered S3 method overwritten by &#39;rmutil&#39;: ## method from ## plot.residuals psych mfv(dlf$day1, method=&quot;mfv&quot;) ## [1] 2 mfv(dlf$day2, method=&quot;mfv&quot;) ## [1] NA mfv(dlf$day3, method=&quot;mfv&quot;) ## [1] NA Test of normality, Shapiro Wilks Test library(pastecs) stat.desc(dlf$day3, basic = FALSE, norm = TRUE) # &quot;norm=TRUE&quot;&quot; is to calculate the Shapiro Wilk Test ## median mean SE.mean CI.mean.0.95 var std.dev ## 7.600000e-01 9.765041e-01 6.404352e-02 1.267805e-01 5.044934e-01 7.102770e-01 ## coef.var skewness skew.2SE kurtosis kurt.2SE normtest.W ## 7.273672e-01 1.007813e+00 2.309035e+00 5.945454e-01 6.862946e-01 9.077516e-01 ## normtest.p ## 3.804486e-07 round(stat.desc(dlf[, c(&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;)], basic = FALSE, norm = TRUE), digits = 3) ## day1 day2 day3 ## median 1.790 0.790 0.760 ## mean 1.793 0.961 0.977 ## SE.mean 0.033 0.044 0.064 ## CI.mean.0.95 0.065 0.087 0.127 ## var 0.892 0.520 0.504 ## std.dev 0.944 0.721 0.710 ## coef.var 0.527 0.750 0.727 ## skewness 8.833 1.083 1.008 ## skew.2SE 51.407 3.612 2.309 ## kurtosis 168.967 0.755 0.595 ## kurt.2SE 492.314 1.265 0.686 ## normtest.W 0.654 0.908 0.908 ## normtest.p 0.000 0.000 0.000 "],["tests-of-equality-of-variances.html", "Chapter 8 Tests of Equality of variances", " Chapter 8 Tests of Equality of variances Most parametric test require that when testing differences among groups, these have more or less the same variance around the mean. The Levene’s test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group means. The test is considered to be robust against departures from normality, but it is sensitive to the presence of outliers. 8.0.1 The F-test of equality of variance The F-test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the ratio of the variances in the different groups. This test is EXTREMELY sensitive to non-normality and outliers. Consequently the F-test is considered to be less robust than the Levene’s test, Bartlett’s test or the Brown-Forsythe test . Sokal, R. R., Rohlf, F. J. (1995). Biometry: The Principles and Practice of Statistics in Biological Research. W. H. Freeman and Company \\[F=s_x^2/s_y^2\\], where \\(s_x^2\\) and \\(s_y^2\\) are the variances of the two groups, and x is the variance of the group with the larger variance. var.test(dlf$day1, dlf$day2) ## ## F test to compare two variances ## ## data: dlf$day1 and dlf$day2 ## F = 1.7171, num df = 809, denom df = 263, p-value = 3.23e-07 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 1.402898 2.080705 ## sample estimates: ## ratio of variances ## 1.717092 8.0.2 Bartlett’s test of equality of variance Bartlett’s test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group means. The test is considered to be robust against departures from normality, but it is sensitive to the presence of outliers. Bartlett, M. S. (1937). Properties of sufficiency and statistical tests. Proceedings of the Royal Society of London Series A 160, 268–282. doi:10.1098/rspa.1937.0109. Note: that the data needs to be manipulated so it appear in in the long form subset(dlf, select = c(&quot;day1&quot;, &quot;day2&quot;)) -&gt; dlfsub # create a subset of the data you need dlfsub1 &lt;- stack(dlfsub) # reorganize the data in the long form dlfsub1 # the data is now in the long form ## values ind ## 1 2.64 day1 ## 2 0.97 day1 ## 3 0.84 day1 ## 4 3.03 day1 ## 5 0.88 day1 ## 6 0.85 day1 ## 7 1.56 day1 ## 8 3.02 day1 ## 9 2.29 day1 ## 10 1.11 day1 ## 11 2.17 day1 ## 12 0.82 day1 ## 13 1.41 day1 ## 14 1.76 day1 ## 15 1.38 day1 ## 16 2.79 day1 ## 17 1.50 day1 ## 18 1.91 day1 ## 19 2.32 day1 ## 20 2.05 day1 ## 21 2.17 day1 ## 22 2.05 day1 ## 23 1.61 day1 ## 24 1.66 day1 ## 25 2.30 day1 ## 26 2.76 day1 ## 27 1.44 day1 ## 28 1.06 day1 ## 29 3.23 day1 ## 30 0.97 day1 ## 31 2.57 day1 ## 32 0.26 day1 ## 33 0.47 day1 ## 34 1.73 day1 ## 35 1.94 day1 ## 36 1.91 day1 ## 37 2.08 day1 ## 38 1.91 day1 ## 39 1.42 day1 ## 40 1.50 day1 ## 41 0.11 day1 ## 42 1.67 day1 ## 43 2.08 day1 ## 44 2.05 day1 ## 45 2.00 day1 ## 46 1.52 day1 ## 47 1.58 day1 ## 48 1.28 day1 ## 49 1.88 day1 ## 50 1.32 day1 ## 51 2.09 day1 ## 52 2.00 day1 ## 53 2.64 day1 ## 54 0.85 day1 ## 55 2.47 day1 ## 56 1.79 day1 ## 57 1.64 day1 ## 58 1.32 day1 ## 59 2.97 day1 ## 60 1.44 day1 ## 61 2.02 day1 ## 62 1.79 day1 ## 63 1.34 day1 ## 64 2.29 day1 ## 65 1.66 day1 ## 66 0.60 day1 ## 67 1.76 day1 ## 68 1.50 day1 ## 69 2.08 day1 ## 70 1.00 day1 ## 71 1.73 day1 ## 72 1.05 day1 ## 73 2.81 day1 ## 74 1.52 day1 ## 75 1.47 day1 ## 76 2.64 day1 ## 77 2.20 day1 ## 78 0.55 day1 ## 79 2.29 day1 ## 80 2.00 day1 ## 81 2.23 day1 ## 82 2.45 day1 ## 83 1.20 day1 ## 84 2.91 day1 ## 85 1.14 day1 ## 86 1.88 day1 ## 87 0.94 day1 ## 88 1.85 day1 ## 89 2.58 day1 ## 90 0.61 day1 ## 91 0.70 day1 ## 92 1.38 day1 ## 93 1.94 day1 ## 94 2.29 day1 ## 95 1.59 day1 ## 96 2.46 day1 ## 97 1.67 day1 ## 98 2.02 day1 ## 99 1.50 day1 ## 100 2.70 day1 ## 101 1.61 day1 ## 102 2.29 day1 ## 103 0.97 day1 ## 104 1.85 day1 ## 105 2.76 day1 ## 106 1.64 day1 ## 107 1.17 day1 ## 108 1.57 day1 ## 109 2.23 day1 ## 110 2.05 day1 ## 111 2.05 day1 ## 112 2.94 day1 ## 113 2.39 day1 ## 114 1.94 day1 ## 115 2.12 day1 ## 116 1.11 day1 ## 117 0.97 day1 ## 118 1.35 day1 ## 119 2.81 day1 ## 120 2.50 day1 ## 121 1.87 day1 ## 122 1.33 day1 ## 123 1.26 day1 ## 124 1.44 day1 ## 125 0.55 day1 ## 126 1.75 day1 ## 127 2.08 day1 ## 128 0.85 day1 ## 129 2.52 day1 ## 130 3.00 day1 ## 131 1.41 day1 ## 132 1.08 day1 ## 133 1.20 day1 ## 134 1.94 day1 ## 135 2.26 day1 ## 136 1.41 day1 ## 137 2.50 day1 ## 138 2.17 day1 ## 139 1.82 day1 ## 140 1.44 day1 ## 141 1.66 day1 ## 142 1.82 day1 ## 143 1.26 day1 ## 144 2.67 day1 ## 145 1.47 day1 ## 146 1.84 day1 ## 147 2.58 day1 ## 148 1.73 day1 ## 149 1.23 day1 ## 150 2.32 day1 ## 151 2.67 day1 ## 152 1.02 day1 ## 153 1.66 day1 ## 154 1.88 day1 ## 155 1.91 day1 ## 156 1.64 day1 ## 157 1.34 day1 ## 158 1.85 day1 ## 159 2.08 day1 ## 160 1.02 day1 ## 161 1.79 day1 ## 162 1.94 day1 ## 163 3.26 day1 ## 164 1.14 day1 ## 165 1.50 day1 ## 166 2.03 day1 ## 167 2.24 day1 ## 168 1.11 day1 ## 169 2.21 day1 ## 170 1.94 day1 ## 171 2.41 day1 ## 172 0.88 day1 ## 173 1.17 day1 ## 174 2.23 day1 ## 175 1.64 day1 ## 176 2.14 day1 ## 177 0.11 day1 ## 178 2.17 day1 ## 179 1.67 day1 ## 180 1.00 day1 ## 181 0.88 day1 ## 182 2.20 day1 ## 183 2.17 day1 ## 184 2.32 day1 ## 185 1.64 day1 ## 186 3.00 day1 ## 187 2.38 day1 ## 188 1.60 day1 ## 189 1.58 day1 ## 190 2.61 day1 ## 191 1.44 day1 ## 192 1.57 day1 ## 193 2.32 day1 ## 194 1.14 day1 ## 195 1.93 day1 ## 196 2.47 day1 ## 197 2.29 day1 ## 198 1.00 day1 ## 199 1.58 day1 ## 200 2.44 day1 ## 201 0.83 day1 ## 202 2.71 day1 ## 203 1.73 day1 ## 204 1.58 day1 ## 205 1.50 day1 ## 206 1.05 day1 ## 207 2.05 day1 ## 208 2.63 day1 ## 209 2.55 day1 ## 210 2.00 day1 ## 211 2.00 day1 ## 212 1.32 day1 ## 213 3.14 day1 ## 214 1.44 day1 ## 215 1.85 day1 ## 216 1.41 day1 ## 217 1.94 day1 ## 218 2.91 day1 ## 219 1.85 day1 ## 220 1.70 day1 ## 221 2.23 day1 ## 222 1.11 day1 ## 223 1.47 day1 ## 224 2.20 day1 ## 225 1.82 day1 ## 226 1.42 day1 ## 227 2.44 day1 ## 228 2.66 day1 ## 229 1.52 day1 ## 230 1.35 day1 ## 231 1.29 day1 ## 232 2.32 day1 ## 233 0.78 day1 ## 234 2.84 day1 ## 235 0.97 day1 ## 236 1.52 day1 ## 237 1.70 day1 ## 238 0.94 day1 ## 239 1.41 day1 ## 240 1.79 day1 ## 241 1.08 day1 ## 242 1.47 day1 ## 243 1.79 day1 ## 244 2.00 day1 ## 245 0.76 day1 ## 246 2.20 day1 ## 247 0.94 day1 ## 248 1.38 day1 ## 249 1.38 day1 ## 250 0.32 day1 ## 251 2.58 day1 ## 252 0.51 day1 ## 253 0.32 day1 ## 254 0.91 day1 ## 255 1.51 day1 ## 256 1.47 day1 ## 257 2.50 day1 ## 258 2.26 day1 ## 259 2.81 day1 ## 260 1.87 day1 ## 261 2.00 day1 ## 262 2.23 day1 ## 263 2.00 day1 ## 264 1.41 day1 ## 265 1.64 day1 ## 266 1.64 day1 ## 267 1.26 day1 ## 268 1.52 day1 ## 269 2.44 day1 ## 270 2.18 day1 ## 271 3.02 day1 ## 272 1.02 day1 ## 273 2.88 day1 ## 274 1.54 day1 ## 275 1.64 day1 ## 276 2.44 day1 ## 277 1.29 day1 ## 278 1.61 day1 ## 279 1.77 day1 ## 280 0.91 day1 ## 281 0.85 day1 ## 282 0.85 day1 ## 283 1.50 day1 ## 284 1.05 day1 ## 285 3.38 day1 ## 286 1.42 day1 ## 287 1.85 day1 ## 288 1.91 day1 ## 289 0.82 day1 ## 290 1.32 day1 ## 291 2.23 day1 ## 292 1.47 day1 ## 293 2.70 day1 ## 294 1.58 day1 ## 295 1.00 day1 ## 296 1.44 day1 ## 297 2.00 day1 ## 298 1.60 day1 ## 299 2.32 day1 ## 300 3.41 day1 ## 301 2.02 day1 ## 302 0.64 day1 ## 303 3.58 day1 ## 304 1.50 day1 ## 305 1.08 day1 ## 306 1.52 day1 ## 307 1.26 day1 ## 308 1.68 day1 ## 309 1.47 day1 ## 310 1.47 day1 ## 311 1.67 day1 ## 312 2.47 day1 ## 313 1.82 day1 ## 314 2.17 day1 ## 315 3.21 day1 ## 316 1.60 day1 ## 317 0.32 day1 ## 318 0.55 day1 ## 319 1.42 day1 ## 320 1.14 day1 ## 321 2.64 day1 ## 322 2.58 day1 ## 323 2.02 day1 ## 324 2.00 day1 ## 325 2.90 day1 ## 326 1.82 day1 ## 327 0.50 day1 ## 328 1.53 day1 ## 329 2.48 day1 ## 330 2.05 day1 ## 331 2.52 day1 ## 332 1.88 day1 ## 333 2.73 day1 ## 334 2.88 day1 ## 335 1.67 day1 ## 336 1.93 day1 ## 337 1.67 day1 ## 338 1.20 day1 ## 339 2.75 day1 ## 340 1.94 day1 ## 341 0.59 day1 ## 342 1.50 day1 ## 343 1.58 day1 ## 344 2.23 day1 ## 345 2.35 day1 ## 346 2.55 day1 ## 347 1.55 day1 ## 348 2.31 day1 ## 349 2.23 day1 ## 350 0.67 day1 ## 351 2.51 day1 ## 352 1.08 day1 ## 353 2.44 day1 ## 354 0.23 day1 ## 355 2.17 day1 ## 356 1.90 day1 ## 357 1.67 day1 ## 358 2.00 day1 ## 359 2.44 day1 ## 360 1.44 day1 ## 361 0.82 day1 ## 362 2.50 day1 ## 363 1.82 day1 ## 364 1.97 day1 ## 365 2.52 day1 ## 366 0.05 day1 ## 367 2.08 day1 ## 368 2.39 day1 ## 369 1.45 day1 ## 370 2.58 day1 ## 371 2.12 day1 ## 372 2.02 day1 ## 373 1.78 day1 ## 374 0.73 day1 ## 375 2.26 day1 ## 376 2.79 day1 ## 377 0.43 day1 ## 378 0.52 day1 ## 379 2.32 day1 ## 380 2.22 day1 ## 381 0.58 day1 ## 382 2.00 day1 ## 383 0.70 day1 ## 384 1.00 day1 ## 385 0.30 day1 ## 386 1.52 day1 ## 387 1.58 day1 ## 388 2.34 day1 ## 389 0.79 day1 ## 390 2.26 day1 ## 391 2.35 day1 ## 392 1.70 day1 ## 393 3.09 day1 ## 394 1.52 day1 ## 395 0.35 day1 ## 396 2.70 day1 ## 397 1.64 day1 ## 398 0.82 day1 ## 399 2.73 day1 ## 400 2.23 day1 ## 401 1.06 day1 ## 402 2.05 day1 ## 403 1.73 day1 ## 404 0.93 day1 ## 405 2.50 day1 ## 406 1.44 day1 ## 407 2.88 day1 ## 408 0.67 day1 ## 409 1.85 day1 ## 410 1.21 day1 ## 411 1.06 day1 ## 412 0.61 day1 ## 413 2.00 day1 ## 414 1.17 day1 ## 415 1.48 day1 ## 416 1.55 day1 ## 417 3.29 day1 ## 418 1.47 day1 ## 419 0.96 day1 ## 420 1.00 day1 ## 421 1.47 day1 ## 422 2.55 day1 ## 423 0.44 day1 ## 424 2.35 day1 ## 425 1.71 day1 ## 426 1.84 day1 ## 427 1.11 day1 ## 428 1.38 day1 ## 429 0.88 day1 ## 430 0.94 day1 ## 431 1.91 day1 ## 432 2.76 day1 ## 433 1.55 day1 ## 434 2.67 day1 ## 435 1.03 day1 ## 436 2.50 day1 ## 437 1.64 day1 ## 438 2.26 day1 ## 439 2.14 day1 ## 440 0.52 day1 ## 441 1.08 day1 ## 442 1.69 day1 ## 443 2.73 day1 ## 444 1.91 day1 ## 445 1.73 day1 ## 446 3.21 day1 ## 447 2.11 day1 ## 448 2.05 day1 ## 449 2.17 day1 ## 450 2.17 day1 ## 451 2.30 day1 ## 452 2.56 day1 ## 453 2.11 day1 ## 454 1.70 day1 ## 455 1.23 day1 ## 456 3.20 day1 ## 457 2.02 day1 ## 458 2.64 day1 ## 459 2.52 day1 ## 460 1.61 day1 ## 461 1.50 day1 ## 462 1.15 day1 ## 463 1.82 day1 ## 464 1.50 day1 ## 465 2.32 day1 ## 466 2.92 day1 ## 467 1.41 day1 ## 468 1.35 day1 ## 469 0.61 day1 ## 470 0.73 day1 ## 471 2.23 day1 ## 472 1.32 day1 ## 473 2.94 day1 ## 474 1.61 day1 ## 475 1.00 day1 ## 476 3.15 day1 ## 477 2.88 day1 ## 478 2.09 day1 ## 479 1.32 day1 ## 480 1.47 day1 ## 481 1.61 day1 ## 482 2.20 day1 ## 483 2.78 day1 ## 484 2.06 day1 ## 485 0.47 day1 ## 486 2.87 day1 ## 487 1.14 day1 ## 488 3.32 day1 ## 489 2.08 day1 ## 490 2.38 day1 ## 491 2.08 day1 ## 492 1.85 day1 ## 493 1.38 day1 ## 494 1.14 day1 ## 495 1.58 day1 ## 496 1.23 day1 ## 497 2.53 day1 ## 498 0.67 day1 ## 499 0.73 day1 ## 500 1.34 day1 ## 501 2.14 day1 ## 502 1.00 day1 ## 503 1.35 day1 ## 504 1.94 day1 ## 505 0.50 day1 ## 506 3.08 day1 ## 507 2.88 day1 ## 508 1.91 day1 ## 509 1.41 day1 ## 510 2.02 day1 ## 511 0.76 day1 ## 512 1.94 day1 ## 513 0.67 day1 ## 514 2.41 day1 ## 515 2.17 day1 ## 516 2.67 day1 ## 517 1.94 day1 ## 518 2.05 day1 ## 519 2.17 day1 ## 520 0.47 day1 ## 521 0.62 day1 ## 522 2.00 day1 ## 523 0.45 day1 ## 524 2.29 day1 ## 525 2.55 day1 ## 526 0.82 day1 ## 527 3.12 day1 ## 528 2.50 day1 ## 529 1.79 day1 ## 530 2.28 day1 ## 531 0.58 day1 ## 532 2.50 day1 ## 533 1.41 day1 ## 534 2.14 day1 ## 535 0.76 day1 ## 536 1.79 day1 ## 537 1.02 day1 ## 538 2.62 day1 ## 539 0.88 day1 ## 540 1.58 day1 ## 541 2.20 day1 ## 542 1.14 day1 ## 543 1.47 day1 ## 544 1.41 day1 ## 545 1.44 day1 ## 546 1.23 day1 ## 547 1.82 day1 ## 548 2.44 day1 ## 549 1.94 day1 ## 550 2.41 day1 ## 551 2.27 day1 ## 552 1.79 day1 ## 553 1.88 day1 ## 554 1.85 day1 ## 555 2.21 day1 ## 556 1.97 day1 ## 557 2.51 day1 ## 558 2.05 day1 ## 559 1.29 day1 ## 560 2.05 day1 ## 561 2.23 day1 ## 562 1.76 day1 ## 563 1.05 day1 ## 564 1.79 day1 ## 565 1.02 day1 ## 566 2.76 day1 ## 567 1.67 day1 ## 568 2.85 day1 ## 569 0.23 day1 ## 570 1.90 day1 ## 571 1.23 day1 ## 572 1.97 day1 ## 573 1.50 day1 ## 574 3.69 day1 ## 575 0.50 day1 ## 576 2.18 day1 ## 577 2.17 day1 ## 578 1.58 day1 ## 579 2.88 day1 ## 580 2.52 day1 ## 581 2.20 day1 ## 582 1.73 day1 ## 583 2.23 day1 ## 584 1.97 day1 ## 585 1.20 day1 ## 586 2.00 day1 ## 587 1.91 day1 ## 588 0.81 day1 ## 589 1.31 day1 ## 590 0.38 day1 ## 591 1.97 day1 ## 592 0.38 day1 ## 593 2.11 day1 ## 594 3.20 day1 ## 595 0.02 day1 ## 596 2.56 day1 ## 597 2.02 day1 ## 598 2.30 day1 ## 599 2.02 day1 ## 600 2.05 day1 ## 601 1.70 day1 ## 602 1.61 day1 ## 603 0.73 day1 ## 604 2.50 day1 ## 605 2.18 day1 ## 606 2.46 day1 ## 607 1.50 day1 ## 608 1.73 day1 ## 609 1.44 day1 ## 610 1.64 day1 ## 611 20.02 day1 ## 612 1.20 day1 ## 613 0.38 day1 ## 614 1.58 day1 ## 615 1.67 day1 ## 616 1.00 day1 ## 617 2.58 day1 ## 618 2.82 day1 ## 619 2.29 day1 ## 620 1.14 day1 ## 621 1.64 day1 ## 622 1.82 day1 ## 623 3.32 day1 ## 624 3.32 day1 ## 625 1.85 day1 ## 626 2.29 day1 ## 627 1.47 day1 ## 628 2.08 day1 ## 629 2.20 day1 ## 630 1.06 day1 ## 631 0.97 day1 ## 632 2.00 day1 ## 633 1.67 day1 ## 634 2.94 day1 ## 635 1.55 day1 ## 636 0.88 day1 ## 637 1.35 day1 ## 638 0.61 day1 ## 639 1.00 day1 ## 640 1.52 day1 ## 641 1.00 day1 ## 642 1.76 day1 ## 643 2.52 day1 ## 644 2.00 day1 ## 645 2.63 day1 ## 646 0.73 day1 ## 647 1.58 day1 ## 648 0.58 day1 ## 649 1.67 day1 ## 650 1.47 day1 ## 651 1.81 day1 ## 652 1.91 day1 ## 653 1.06 day1 ## 654 1.47 day1 ## 655 2.52 day1 ## 656 1.85 day1 ## 657 3.44 day1 ## 658 1.55 day1 ## 659 2.29 day1 ## 660 1.76 day1 ## 661 1.90 day1 ## 662 2.52 day1 ## 663 2.52 day1 ## 664 2.82 day1 ## 665 2.02 day1 ## 666 1.29 day1 ## 667 1.26 day1 ## 668 0.94 day1 ## 669 2.00 day1 ## 670 0.73 day1 ## 671 2.26 day1 ## 672 2.23 day1 ## 673 2.35 day1 ## 674 0.55 day1 ## 675 1.85 day1 ## 676 0.67 day1 ## 677 1.85 day1 ## 678 1.23 day1 ## 679 2.35 day1 ## 680 1.35 day1 ## 681 1.94 day1 ## 682 1.55 day1 ## 683 1.29 day1 ## 684 2.17 day1 ## 685 1.91 day1 ## 686 2.88 day1 ## 687 2.36 day1 ## 688 2.36 day1 ## 689 2.20 day1 ## 690 2.17 day1 ## 691 0.52 day1 ## 692 0.32 day1 ## 693 1.52 day1 ## 694 2.00 day1 ## 695 1.32 day1 ## 696 2.05 day1 ## 697 1.73 day1 ## 698 1.94 day1 ## 699 1.81 day1 ## 700 0.90 day1 ## 701 1.58 day1 ## 702 2.29 day1 ## 703 2.57 day1 ## 704 1.58 day1 ## 705 2.33 day1 ## 706 3.15 day1 ## 707 2.29 day1 ## 708 0.82 day1 ## 709 1.93 day1 ## 710 1.82 day1 ## 711 1.96 day1 ## 712 1.32 day1 ## 713 1.02 day1 ## 714 1.14 day1 ## 715 2.32 day1 ## 716 2.16 day1 ## 717 2.42 day1 ## 718 1.14 day1 ## 719 1.55 day1 ## 720 1.17 day1 ## 721 1.00 day1 ## 722 1.05 day1 ## 723 1.38 day1 ## 724 1.93 day1 ## 725 2.73 day1 ## 726 2.02 day1 ## 727 2.81 day1 ## 728 2.47 day1 ## 729 1.35 day1 ## 730 2.08 day1 ## 731 2.50 day1 ## 732 2.45 day1 ## 733 2.17 day1 ## 734 1.70 day1 ## 735 0.70 day1 ## 736 1.51 day1 ## 737 1.23 day1 ## 738 2.14 day1 ## 739 1.14 day1 ## 740 0.96 day1 ## 741 1.52 day1 ## 742 0.52 day1 ## 743 1.56 day1 ## 744 3.29 day1 ## 745 0.45 day1 ## 746 2.63 day1 ## 747 1.70 day1 ## 748 3.11 day1 ## 749 1.82 day1 ## 750 1.58 day1 ## 751 2.73 day1 ## 752 1.50 day1 ## 753 1.78 day1 ## 754 2.02 day1 ## 755 0.67 day1 ## 756 1.41 day1 ## 757 0.90 day1 ## 758 1.23 day1 ## 759 2.70 day1 ## 760 1.97 day1 ## 761 0.84 day1 ## 762 1.79 day1 ## 763 2.84 day1 ## 764 2.02 day1 ## 765 1.64 day1 ## 766 1.08 day1 ## 767 2.97 day1 ## 768 0.94 day1 ## 769 2.97 day1 ## 770 0.97 day1 ## 771 1.47 day1 ## 772 2.61 day1 ## 773 1.73 day1 ## 774 3.38 day1 ## 775 3.17 day1 ## 776 2.20 day1 ## 777 2.14 day1 ## 778 1.29 day1 ## 779 3.21 day1 ## 780 2.67 day1 ## 781 1.85 day1 ## 782 1.35 day1 ## 783 2.14 day1 ## 784 1.24 day1 ## 785 2.02 day1 ## 786 2.32 day1 ## 787 1.08 day1 ## 788 1.14 day1 ## 789 2.14 day1 ## 790 2.88 day1 ## 791 1.35 day1 ## 792 1.00 day1 ## 793 2.02 day1 ## 794 0.64 day1 ## 795 0.29 day1 ## 796 1.73 day1 ## 797 1.82 day1 ## 798 2.11 day1 ## 799 1.23 day1 ## 800 0.64 day1 ## 801 2.23 day1 ## 802 2.44 day1 ## 803 1.17 day1 ## 804 0.61 day1 ## 805 0.52 day1 ## 806 2.91 day1 ## 807 2.61 day1 ## 808 1.47 day1 ## 809 1.28 day1 ## 810 1.26 day1 ## 811 1.35 day2 ## 812 1.41 day2 ## 813 NA day2 ## 814 NA day2 ## 815 0.08 day2 ## 816 NA day2 ## 817 NA day2 ## 818 NA day2 ## 819 NA day2 ## 820 0.44 day2 ## 821 NA day2 ## 822 0.20 day2 ## 823 NA day2 ## 824 1.64 day2 ## 825 0.02 day2 ## 826 NA day2 ## 827 NA day2 ## 828 2.05 day2 ## 829 NA day2 ## 830 NA day2 ## 831 0.70 day2 ## 832 NA day2 ## 833 NA day2 ## 834 0.85 day2 ## 835 NA day2 ## 836 NA day2 ## 837 NA day2 ## 838 NA day2 ## 839 NA day2 ## 840 0.38 day2 ## 841 0.11 day2 ## 842 NA day2 ## 843 NA day2 ## 844 NA day2 ## 845 0.82 day2 ## 846 NA day2 ## 847 0.91 day2 ## 848 NA day2 ## 849 NA day2 ## 850 NA day2 ## 851 NA day2 ## 852 NA day2 ## 853 NA day2 ## 854 NA day2 ## 855 NA day2 ## 856 NA day2 ## 857 NA day2 ## 858 0.38 day2 ## 859 NA day2 ## 860 NA day2 ## 861 NA day2 ## 862 NA day2 ## 863 NA day2 ## 864 0.32 day2 ## 865 0.23 day2 ## 866 NA day2 ## 867 NA day2 ## 868 NA day2 ## 869 NA day2 ## 870 0.14 day2 ## 871 NA day2 ## 872 NA day2 ## 873 NA day2 ## 874 1.90 day2 ## 875 NA day2 ## 876 NA day2 ## 877 0.76 day2 ## 878 NA day2 ## 879 0.70 day2 ## 880 0.55 day2 ## 881 NA day2 ## 882 0.38 day2 ## 883 NA day2 ## 884 NA day2 ## 885 NA day2 ## 886 NA day2 ## 887 1.18 day2 ## 888 0.79 day2 ## 889 NA day2 ## 890 NA day2 ## 891 NA day2 ## 892 NA day2 ## 893 NA day2 ## 894 2.08 day2 ## 895 1.00 day2 ## 896 NA day2 ## 897 NA day2 ## 898 NA day2 ## 899 NA day2 ## 900 NA day2 ## 901 NA day2 ## 902 NA day2 ## 903 NA day2 ## 904 NA day2 ## 905 NA day2 ## 906 NA day2 ## 907 0.14 day2 ## 908 0.58 day2 ## 909 NA day2 ## 910 1.70 day2 ## 911 NA day2 ## 912 NA day2 ## 913 1.06 day2 ## 914 NA day2 ## 915 NA day2 ## 916 NA day2 ## 917 NA day2 ## 918 NA day2 ## 919 NA day2 ## 920 1.58 day2 ## 921 NA day2 ## 922 NA day2 ## 923 NA day2 ## 924 NA day2 ## 925 NA day2 ## 926 NA day2 ## 927 NA day2 ## 928 NA day2 ## 929 2.08 day2 ## 930 NA day2 ## 931 NA day2 ## 932 NA day2 ## 933 NA day2 ## 934 NA day2 ## 935 NA day2 ## 936 NA day2 ## 937 NA day2 ## 938 NA day2 ## 939 NA day2 ## 940 NA day2 ## 941 NA day2 ## 942 NA day2 ## 943 1.38 day2 ## 944 1.44 day2 ## 945 1.73 day2 ## 946 NA day2 ## 947 NA day2 ## 948 NA day2 ## 949 1.11 day2 ## 950 1.14 day2 ## 951 NA day2 ## 952 NA day2 ## 953 NA day2 ## 954 NA day2 ## 955 NA day2 ## 956 NA day2 ## 957 NA day2 ## 958 NA day2 ## 959 NA day2 ## 960 NA day2 ## 961 NA day2 ## 962 NA day2 ## 963 2.12 day2 ## 964 NA day2 ## 965 NA day2 ## 966 NA day2 ## 967 NA day2 ## 968 NA day2 ## 969 NA day2 ## 970 NA day2 ## 971 NA day2 ## 972 NA day2 ## 973 1.97 day2 ## 974 0.58 day2 ## 975 0.70 day2 ## 976 NA day2 ## 977 NA day2 ## 978 NA day2 ## 979 NA day2 ## 980 NA day2 ## 981 NA day2 ## 982 NA day2 ## 983 1.35 day2 ## 984 NA day2 ## 985 NA day2 ## 986 NA day2 ## 987 0.29 day2 ## 988 NA day2 ## 989 NA day2 ## 990 NA day2 ## 991 NA day2 ## 992 NA day2 ## 993 NA day2 ## 994 NA day2 ## 995 NA day2 ## 996 NA day2 ## 997 0.85 day2 ## 998 1.02 day2 ## 999 NA day2 ## 1000 NA day2 ## 1001 0.05 day2 ## 1002 NA day2 ## 1003 NA day2 ## 1004 NA day2 ## 1005 NA day2 ## 1006 NA day2 ## 1007 NA day2 ## 1008 NA day2 ## 1009 NA day2 ## 1010 NA day2 ## 1011 NA day2 ## 1012 0.78 day2 ## 1013 NA day2 ## 1014 NA day2 ## 1015 NA day2 ## 1016 NA day2 ## 1017 NA day2 ## 1018 NA day2 ## 1019 2.29 day2 ## 1020 NA day2 ## 1021 NA day2 ## 1022 NA day2 ## 1023 NA day2 ## 1024 NA day2 ## 1025 0.23 day2 ## 1026 0.44 day2 ## 1027 NA day2 ## 1028 NA day2 ## 1029 NA day2 ## 1030 NA day2 ## 1031 NA day2 ## 1032 NA day2 ## 1033 NA day2 ## 1034 NA day2 ## 1035 NA day2 ## 1036 NA day2 ## 1037 NA day2 ## 1038 NA day2 ## 1039 NA day2 ## 1040 0.47 day2 ## 1041 NA day2 ## 1042 NA day2 ## 1043 NA day2 ## 1044 NA day2 ## 1045 NA day2 ## 1046 NA day2 ## 1047 NA day2 ## 1048 1.17 day2 ## 1049 NA day2 ## 1050 NA day2 ## 1051 0.44 day2 ## 1052 NA day2 ## 1053 0.47 day2 ## 1054 NA day2 ## 1055 NA day2 ## 1056 NA day2 ## 1057 0.17 day2 ## 1058 NA day2 ## 1059 0.85 day2 ## 1060 NA day2 ## 1061 NA day2 ## 1062 NA day2 ## 1063 NA day2 ## 1064 1.11 day2 ## 1065 NA day2 ## 1066 NA day2 ## 1067 NA day2 ## 1068 NA day2 ## 1069 NA day2 ## 1070 NA day2 ## 1071 NA day2 ## 1072 0.41 day2 ## 1073 0.76 day2 ## 1074 NA day2 ## 1075 NA day2 ## 1076 NA day2 ## 1077 NA day2 ## 1078 0.55 day2 ## 1079 1.02 day2 ## 1080 NA day2 ## 1081 NA day2 ## 1082 NA day2 ## 1083 NA day2 ## 1084 NA day2 ## 1085 NA day2 ## 1086 2.50 day2 ## 1087 NA day2 ## 1088 0.32 day2 ## 1089 NA day2 ## 1090 0.17 day2 ## 1091 0.20 day2 ## 1092 0.52 day2 ## 1093 NA day2 ## 1094 0.23 day2 ## 1095 NA day2 ## 1096 0.52 day2 ## 1097 NA day2 ## 1098 0.84 day2 ## 1099 0.26 day2 ## 1100 0.76 day2 ## 1101 0.85 day2 ## 1102 1.52 day2 ## 1103 NA day2 ## 1104 NA day2 ## 1105 NA day2 ## 1106 NA day2 ## 1107 NA day2 ## 1108 NA day2 ## 1109 2.53 day2 ## 1110 NA day2 ## 1111 NA day2 ## 1112 0.52 day2 ## 1113 3.35 day2 ## 1114 NA day2 ## 1115 NA day2 ## 1116 NA day2 ## 1117 NA day2 ## 1118 NA day2 ## 1119 1.08 day2 ## 1120 NA day2 ## 1121 1.55 day2 ## 1122 1.97 day2 ## 1123 NA day2 ## 1124 NA day2 ## 1125 NA day2 ## 1126 1.38 day2 ## 1127 NA day2 ## 1128 NA day2 ## 1129 NA day2 ## 1130 NA day2 ## 1131 NA day2 ## 1132 NA day2 ## 1133 NA day2 ## 1134 NA day2 ## 1135 NA day2 ## 1136 NA day2 ## 1137 NA day2 ## 1138 NA day2 ## 1139 NA day2 ## 1140 NA day2 ## 1141 NA day2 ## 1142 NA day2 ## 1143 NA day2 ## 1144 NA day2 ## 1145 NA day2 ## 1146 NA day2 ## 1147 NA day2 ## 1148 NA day2 ## 1149 NA day2 ## 1150 0.97 day2 ## 1151 NA day2 ## 1152 NA day2 ## 1153 0.94 day2 ## 1154 0.11 day2 ## 1155 NA day2 ## 1156 0.82 day2 ## 1157 NA day2 ## 1158 NA day2 ## 1159 NA day2 ## 1160 0.50 day2 ## 1161 NA day2 ## 1162 0.58 day2 ## 1163 NA day2 ## 1164 0.14 day2 ## 1165 NA day2 ## 1166 1.17 day2 ## 1167 0.44 day2 ## 1168 0.58 day2 ## 1169 NA day2 ## 1170 NA day2 ## 1171 NA day2 ## 1172 NA day2 ## 1173 NA day2 ## 1174 NA day2 ## 1175 NA day2 ## 1176 NA day2 ## 1177 NA day2 ## 1178 NA day2 ## 1179 0.82 day2 ## 1180 NA day2 ## 1181 NA day2 ## 1182 0.76 day2 ## 1183 1.14 day2 ## 1184 0.17 day2 ## 1185 0.90 day2 ## 1186 NA day2 ## 1187 0.67 day2 ## 1188 0.38 day2 ## 1189 NA day2 ## 1190 NA day2 ## 1191 NA day2 ## 1192 NA day2 ## 1193 NA day2 ## 1194 NA day2 ## 1195 NA day2 ## 1196 NA day2 ## 1197 0.35 day2 ## 1198 NA day2 ## 1199 NA day2 ## 1200 NA day2 ## 1201 NA day2 ## 1202 NA day2 ## 1203 NA day2 ## 1204 NA day2 ## 1205 NA day2 ## 1206 NA day2 ## 1207 NA day2 ## 1208 NA day2 ## 1209 NA day2 ## 1210 NA day2 ## 1211 NA day2 ## 1212 0.20 day2 ## 1213 1.44 day2 ## 1214 0.91 day2 ## 1215 2.44 day2 ## 1216 NA day2 ## 1217 NA day2 ## 1218 0.23 day2 ## 1219 0.35 day2 ## 1220 0.79 day2 ## 1221 0.76 day2 ## 1222 0.26 day2 ## 1223 NA day2 ## 1224 0.73 day2 ## 1225 0.79 day2 ## 1226 NA day2 ## 1227 NA day2 ## 1228 NA day2 ## 1229 NA day2 ## 1230 1.11 day2 ## 1231 NA day2 ## 1232 2.38 day2 ## 1233 0.06 day2 ## 1234 2.41 day2 ## 1235 0.85 day2 ## 1236 0.58 day2 ## 1237 0.23 day2 ## 1238 NA day2 ## 1239 NA day2 ## 1240 NA day2 ## 1241 NA day2 ## 1242 NA day2 ## 1243 0.32 day2 ## 1244 NA day2 ## 1245 0.29 day2 ## 1246 NA day2 ## 1247 NA day2 ## 1248 NA day2 ## 1249 NA day2 ## 1250 NA day2 ## 1251 NA day2 ## 1252 NA day2 ## 1253 NA day2 ## 1254 NA day2 ## 1255 NA day2 ## 1256 NA day2 ## 1257 NA day2 ## 1258 NA day2 ## 1259 NA day2 ## 1260 NA day2 ## 1261 NA day2 ## 1262 NA day2 ## 1263 0.41 day2 ## 1264 NA day2 ## 1265 NA day2 ## 1266 NA day2 ## 1267 NA day2 ## 1268 NA day2 ## 1269 0.14 day2 ## 1270 NA day2 ## 1271 1.20 day2 ## 1272 0.45 day2 ## 1273 NA day2 ## 1274 NA day2 ## 1275 NA day2 ## 1276 NA day2 ## 1277 NA day2 ## 1278 NA day2 ## 1279 0.14 day2 ## 1280 NA day2 ## 1281 1.88 day2 ## 1282 0.91 day2 ## 1283 1.79 day2 ## 1284 NA day2 ## 1285 NA day2 ## 1286 3.00 day2 ## 1287 NA day2 ## 1288 1.21 day2 ## 1289 1.70 day2 ## 1290 0.35 day2 ## 1291 NA day2 ## 1292 1.50 day2 ## 1293 NA day2 ## 1294 NA day2 ## 1295 NA day2 ## 1296 NA day2 ## 1297 NA day2 ## 1298 3.21 day2 ## 1299 1.38 day2 ## 1300 2.50 day2 ## 1301 NA day2 ## 1302 NA day2 ## 1303 NA day2 ## 1304 NA day2 ## 1305 NA day2 ## 1306 0.70 day2 ## 1307 NA day2 ## 1308 NA day2 ## 1309 NA day2 ## 1310 NA day2 ## 1311 0.70 day2 ## 1312 NA day2 ## 1313 NA day2 ## 1314 0.79 day2 ## 1315 NA day2 ## 1316 NA day2 ## 1317 NA day2 ## 1318 NA day2 ## 1319 NA day2 ## 1320 NA day2 ## 1321 NA day2 ## 1322 NA day2 ## 1323 0.28 day2 ## 1324 NA day2 ## 1325 NA day2 ## 1326 0.41 day2 ## 1327 0.64 day2 ## 1328 0.85 day2 ## 1329 NA day2 ## 1330 NA day2 ## 1331 0.76 day2 ## 1332 NA day2 ## 1333 NA day2 ## 1334 0.91 day2 ## 1335 NA day2 ## 1336 NA day2 ## 1337 2.20 day2 ## 1338 2.23 day2 ## 1339 NA day2 ## 1340 NA day2 ## 1341 1.05 day2 ## 1342 1.29 day2 ## 1343 NA day2 ## 1344 NA day2 ## 1345 0.26 day2 ## 1346 1.11 day2 ## 1347 0.35 day2 ## 1348 NA day2 ## 1349 NA day2 ## 1350 0.20 day2 ## 1351 NA day2 ## 1352 NA day2 ## 1353 0.52 day2 ## 1354 0.23 day2 ## 1355 1.76 day2 ## 1356 1.17 day2 ## 1357 NA day2 ## 1358 NA day2 ## 1359 1.20 day2 ## 1360 NA day2 ## 1361 NA day2 ## 1362 0.23 day2 ## 1363 NA day2 ## 1364 0.64 day2 ## 1365 NA day2 ## 1366 1.94 day2 ## 1367 NA day2 ## 1368 NA day2 ## 1369 1.00 day2 ## 1370 NA day2 ## 1371 NA day2 ## 1372 NA day2 ## 1373 NA day2 ## 1374 NA day2 ## 1375 0.73 day2 ## 1376 1.58 day2 ## 1377 0.55 day2 ## 1378 NA day2 ## 1379 0.84 day2 ## 1380 NA day2 ## 1381 0.52 day2 ## 1382 NA day2 ## 1383 NA day2 ## 1384 NA day2 ## 1385 NA day2 ## 1386 NA day2 ## 1387 NA day2 ## 1388 NA day2 ## 1389 NA day2 ## 1390 NA day2 ## 1391 NA day2 ## 1392 NA day2 ## 1393 NA day2 ## 1394 NA day2 ## 1395 0.67 day2 ## 1396 NA day2 ## 1397 NA day2 ## 1398 NA day2 ## 1399 NA day2 ## 1400 NA day2 ## 1401 NA day2 ## 1402 NA day2 ## 1403 0.76 day2 ## 1404 NA day2 ## 1405 NA day2 ## 1406 NA day2 ## 1407 NA day2 ## 1408 NA day2 ## 1409 NA day2 ## 1410 NA day2 ## 1411 NA day2 ## 1412 NA day2 ## 1413 NA day2 ## 1414 1.64 day2 ## 1415 1.75 day2 ## 1416 1.08 day2 ## 1417 0.91 day2 ## 1418 0.94 day2 ## 1419 NA day2 ## 1420 0.32 day2 ## 1421 2.44 day2 ## 1422 0.17 day2 ## 1423 0.02 day2 ## 1424 1.54 day2 ## 1425 0.50 day2 ## 1426 0.48 day2 ## 1427 1.35 day2 ## 1428 2.61 day2 ## 1429 2.05 day2 ## 1430 NA day2 ## 1431 0.76 day2 ## 1432 0.08 day2 ## 1433 2.91 day2 ## 1434 NA day2 ## 1435 1.00 day2 ## 1436 NA day2 ## 1437 0.47 day2 ## 1438 0.70 day2 ## 1439 NA day2 ## 1440 1.45 day2 ## 1441 0.14 day2 ## 1442 NA day2 ## 1443 0.38 day2 ## 1444 NA day2 ## 1445 NA day2 ## 1446 0.26 day2 ## 1447 2.32 day2 ## 1448 0.20 day2 ## 1449 NA day2 ## 1450 2.72 day2 ## 1451 NA day2 ## 1452 0.41 day2 ## 1453 NA day2 ## 1454 0.88 day2 ## 1455 NA day2 ## 1456 0.85 day2 ## 1457 0.23 day2 ## 1458 NA day2 ## 1459 NA day2 ## 1460 NA day2 ## 1461 NA day2 ## 1462 NA day2 ## 1463 NA day2 ## 1464 1.23 day2 ## 1465 NA day2 ## 1466 0.20 day2 ## 1467 NA day2 ## 1468 1.32 day2 ## 1469 2.70 day2 ## 1470 NA day2 ## 1471 NA day2 ## 1472 2.55 day2 ## 1473 NA day2 ## 1474 0.17 day2 ## 1475 NA day2 ## 1476 NA day2 ## 1477 NA day2 ## 1478 NA day2 ## 1479 1.13 day2 ## 1480 NA day2 ## 1481 0.79 day2 ## 1482 NA day2 ## 1483 NA day2 ## 1484 0.38 day2 ## 1485 NA day2 ## 1486 NA day2 ## 1487 1.00 day2 ## 1488 0.20 day2 ## 1489 NA day2 ## 1490 NA day2 ## 1491 NA day2 ## 1492 NA day2 ## 1493 NA day2 ## 1494 NA day2 ## 1495 NA day2 ## 1496 NA day2 ## 1497 NA day2 ## 1498 NA day2 ## 1499 NA day2 ## 1500 0.47 day2 ## 1501 NA day2 ## 1502 NA day2 ## 1503 0.55 day2 ## 1504 0.94 day2 ## 1505 1.02 day2 ## 1506 NA day2 ## 1507 NA day2 ## 1508 NA day2 ## 1509 NA day2 ## 1510 0.64 day2 ## 1511 0.67 day2 ## 1512 1.87 day2 ## 1513 NA day2 ## 1514 NA day2 ## 1515 0.82 day2 ## 1516 NA day2 ## 1517 NA day2 ## 1518 NA day2 ## 1519 NA day2 ## 1520 NA day2 ## 1521 NA day2 ## 1522 0.64 day2 ## 1523 NA day2 ## 1524 NA day2 ## 1525 NA day2 ## 1526 NA day2 ## 1527 1.70 day2 ## 1528 NA day2 ## 1529 0.79 day2 ## 1530 NA day2 ## 1531 0.58 day2 ## 1532 0.11 day2 ## 1533 NA day2 ## 1534 2.42 day2 ## 1535 NA day2 ## 1536 NA day2 ## 1537 NA day2 ## 1538 NA day2 ## 1539 NA day2 ## 1540 NA day2 ## 1541 NA day2 ## 1542 NA day2 ## 1543 NA day2 ## 1544 0.00 day2 ## 1545 0.23 day2 ## 1546 NA day2 ## 1547 NA day2 ## 1548 0.85 day2 ## 1549 1.14 day2 ## 1550 NA day2 ## 1551 1.14 day2 ## 1552 NA day2 ## 1553 NA day2 ## 1554 0.26 day2 ## 1555 NA day2 ## 1556 NA day2 ## 1557 NA day2 ## 1558 NA day2 ## 1559 NA day2 ## 1560 0.14 day2 ## 1561 NA day2 ## 1562 1.14 day2 ## 1563 1.02 day2 ## 1564 NA day2 ## 1565 0.94 day2 ## 1566 0.55 day2 ## 1567 NA day2 ## 1568 1.11 day2 ## 1569 NA day2 ## 1570 NA day2 ## 1571 NA day2 ## 1572 NA day2 ## 1573 NA day2 ## 1574 NA day2 ## 1575 0.70 day2 ## 1576 NA day2 ## 1577 NA day2 ## 1578 NA day2 ## 1579 1.94 day2 ## 1580 NA day2 ## 1581 NA day2 ## 1582 NA day2 ## 1583 0.20 day2 ## 1584 3.44 day2 ## 1585 1.00 day2 ## 1586 0.91 day2 ## 1587 NA day2 ## 1588 1.58 day2 ## 1589 2.85 day2 ## 1590 NA day2 ## 1591 0.79 day2 ## 1592 NA day2 ## 1593 0.76 day2 ## 1594 0.56 day2 ## 1595 1.78 day2 ## 1596 NA day2 ## 1597 0.23 day2 ## 1598 1.35 day2 ## 1599 1.82 day2 ## 1600 NA day2 ## 1601 0.17 day2 ## 1602 1.70 day2 ## 1603 NA day2 ## 1604 1.32 day2 ## 1605 0.14 day2 ## 1606 0.94 day2 ## 1607 1.52 day2 ## 1608 NA day2 ## 1609 NA day2 ## 1610 NA day2 ## 1611 1.41 day2 ## 1612 0.32 day2 ## 1613 0.58 day2 ## 1614 0.44 day2 ## 1615 NA day2 ## 1616 0.94 day2 ## 1617 1.44 day2 ## 1618 NA day2 ## 1619 NA day2 ## 1620 NA day2 bartlett.test(values~ind, data=dlfsub1) ## ## Bartlett test of homogeneity of variances ## ## data: values by ind ## Bartlett&#39;s K-squared = 26.273, df = 1, p-value = 2.964e-07 8.0.3 Brown-Forsythe test of equality of variance The Brown-Forsythe test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group medians. The test is considered to be robust against departures from normality and outliers. Brown, M. B., Forsythe. A. B. (1974a). The small sample behavior of some statistics which test the equality of several means. Technometrics, 16, 129-132. Dag, O., Dolgun, A., Konar, N.M. (2018). onewaytests: An R Package for One-Way Tests in Independent Groups Designs. The R Journal, 10:1, 175-199. library(onewaytests) # install and activate the package ## ## Attaching package: &#39;onewaytests&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## describe bf.test(values~ind, data=dlfsub1) ## ## Brown-Forsythe Test (alpha = 0.05) ## ------------------------------------------------------------- ## data : values and ind ## ## statistic : 225.7812 ## num df : 1 ## denom df : 580.6262 ## p.value : 2.392677e-43 ## ## Result : Difference is statistically significant. ## ------------------------------------------------------------- 8.0.4 Levene’s Test: testing for assumption of normality among groups Levene’s test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group means. The test is considered to be robust against departures from normality, but it is sensitive to the presence of outliers. leveneTest(dlf$day1, dlf$day2) ## Warning in leveneTest.default(dlf$day1, dlf$day2): dlf$day2 coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 101 11.661 &lt; 2.2e-16 *** ## 162 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ANOTHER alternative Levene’s test #Levene&#39;s test for comparison of variances of exam scores in the two universities. library(ggplot2) library(car) library(readr) RExam &lt;- read_csv(&quot;Data/RExam.csv&quot;) ## Rows: 100 Columns: 5 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): exam, computer, lectures, numeracy, uni ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. rexam=RExam head(rexam) ## # A tibble: 6 × 5 ## exam computer lectures numeracy uni ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 54 75 7 0 ## 2 30 47 8.5 1 0 ## 3 40 58 69.5 6 0 ## 4 30 37 67 6 0 ## 5 40 53 44.5 2 0 ## 6 15 48 76.5 8 0 ggplot(rexam, aes(numeracy, fill=as.factor(uni)))+ geom_histogram()+ facet_wrap(~uni) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. leveneTest(rexam$lectures, rexam$uni, center=median) # using the median as the center ## Warning in leveneTest.default(rexam$lectures, rexam$uni, center = median): ## rexam$uni coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 1.4222 0.2359 ## 98 leveneTest(rexam$lectures, rexam$uni, center = mean) # using the mean as the center ## Warning in leveneTest.default(rexam$lectures, rexam$uni, center = mean): ## rexam$uni coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 1 1.7306 0.1914 ## 98 "],["non-parametric-test.html", "Chapter 9 Non-parametric test 9.1 Altenative to non-parametric tests", " Chapter 9 Non-parametric test 9.1 Altenative to non-parametric tests A very short intro to GLM (Generalized Linear Models) in R "],["scatter-plots.html", "Chapter 10 Scatter Plots 10.1 Basic scatter plots 10.2 the lm() function 10.3 Visualization of the regression 10.4 The impact of outliers 10.5 Selling music records 10.6 Linear Regression Assumptions 10.7 Cook’s Distance 10.8 Steps to do a linear regression analysis 10.9 Scatter plots with jitter 10.10 Scatter plots with marginal histograms 10.11 Scatter plots for multiple variables", " Chapter 10 Scatter Plots 10.1 Basic scatter plots Fecha de la ultima revisión ## [1] &quot;2024-12-26&quot; The following command verifies that you have the necessary libraries installed and activates them if available on your system. The package pacman evaluates if you have them installed and installs them if necessary. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(QuantPsyc, car, ggplot2, tidyverse, sjPlot, janitor) library(QuantPsyc) # Package for Quantitative Psychology library(car) # Companion to applied regression library(ggplot2) # Data Visualization library(tidyverse) # Data Manipulation library(sjPlot) # Data Visualization library(janitor) # Data Cleaning Linear regression is the basic model to evaluate whether there is a relationship linear that is, a straight line between two variables. This relationship between the variables can be positive or negative. There are other types of regression, which includes nonlinear regression such as quadratic \\(y ~ ax^2+bx+c\\) or cubic \\(y ~ ax^3 +bx^2+cx+d\\), logarithmic \\(y ~ a + b*log(x)\\) among many other alternatives. There are also alternatives are you will see in that are nonlinear regressions module. Here we will be evaluating only linear regression We see a fictitious example The data is from some neighborhoods of Macondo where the numbers of bars in a neighborhood and the number of homicides in that neighborhood. The data you will find the data section below library(readr) pubs &lt;- read_csv(&quot;Data/pubs.csv&quot;) ## Rows: 8 Columns: 2 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): pubs, mortality ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. pubs ## # A tibble: 8 × 2 ## pubs mortality ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10 1000 ## 2 20 2000 ## 3 30 3000 ## 4 40 4000 ## 5 50 5000 ## 6 60 6000 ## 7 70 7000 ## 8 500 10000 10.2 the lm() function Let us do a simple linear regression, using the function lm(), for “linear model”= linear model. A regression takes two continuous variables. It is important that these variables have a normal distribution. The difference between a correlation and a regression is that the first is an analysis that describes the general pattern between the variables and the second is that not only is the pattern described but a prediction is made about the relationship between the variables. Using regression one also calculates a line that describes the relationship between the variables. This variable can be described as \\(y=m_x+b\\) where m represents the slope and b represents the intercept. You can also see it in books in the following way \\(y=\\alpha+\\beta_x\\) where the \\(\\beta\\) beta represents the slope and the \\(\\alpha\\) the intercept, this is the preferred method in the books on English. The linear regression function in R is lm() is composed of lm(y~x, data= “df”). Note the accent ~. There are two tests, the first is for determine if \\(\\alpha\\) is non-zero. The null hypothesis is Ho: the intercept \\(\\alpha\\) is equal to zero Ha: the intercept, \\(\\alpha\\) is not equal to zero. So the point where the line intercepts zero can be greater than or less that zero. The second null hypothesis is that the slope is different from zero. This means that the slope does not suggest/support a pattern of increasing and decrease between the two variables. Ho: the slope \\(\\beta\\) is equal to zero Ha: the slope, \\(\\alpha\\) is not equal to zero. So the relationship between the two variables is either positive or negative. Now we evaluate the results of the regression between the number of bars in a neighborhood (the barrios) and mortality in this same sector. HE Note that the coefficients of the line are \\(y=3352+14.3*x\\). Then the intercept at zero starts at 3352 fatalities and for each bar. Additionally there are 14.3 more fatalities. This means that if there are no bars x=0, the expected number of fatalities is 3352. Now to determine if these values are significant, we must evaluate the value of p in each line. The null hypothesis of the intercept has a value of p =0.005, which suggests that the hypothesis should be rejected null, and consequently we accept the alternative hypothesis, that the intercept is not equal to zero. The slope has a value of p=0.015 and the null hypothesis is also rejected, this suggests that as increases the number of bars increases the number of fatalities, for example for each additional bar we expect 14.3 additional fatalities. pubReg &lt;- lm(mortality~pubs, data = pubs) summary(pubReg) # The summary of the regression ## ## Call: ## lm(formula = mortality ~ pubs, data = pubs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2495.3 -996.3 -223.5 1145.2 2644.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3351.955 781.236 4.291 0.00515 ** ## pubs 14.339 4.301 3.334 0.01572 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1864 on 6 degrees of freedom ## Multiple R-squared: 0.6495, Adjusted R-squared: 0.591 ## F-statistic: 11.12 on 1 and 6 DF, p-value: 0.01572 pubReg$residuals # to visualize the rediduals ## 1 2 3 4 5 6 7 ## -2495.3445 -1638.7337 -782.1229 74.4879 931.0987 1787.7095 2644.3203 ## 8 ## -521.4153 cooks.distance(pubReg)# To calculate Cooks indices ## 1 2 3 4 5 6 ## 2.132784e-01 8.530493e-02 1.814286e-02 1.547980e-04 2.293965e-02 8.092291e-02 ## 7 8 ## 1.710655e-01 2.271429e+02 tab_model( pubReg,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) ) ## Registered S3 method overwritten by &#39;parameters&#39;: ## method from ## predict.kmeans statip   mortality Predictors Estimates CI p df (Intercept) 3351.96 1440.34 – 5263.57 0.005 6.00 pubs 14.34 3.82 – 24.86 0.016 6.00 Observations 8 R2 / R2 adjusted 0.649 / 0.591 10.3 Visualization of the regression It is observed that there is an increase in fatalities with an increase in the number of bars. But notice the value on the right that seems to be very atypical compared to the others. ggplot(pubs, aes(x=pubs, y=mortality))+ geom_smooth(method = lm)+ # modelo lineal geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.4 The impact of outliers On certain occasions, values outside the normal range can make great changes in the result, in this case the regression result. What is the effect of the large value? We remove that value from the file data and re-evaluate the model (linear regression). Note that now the model is extremely different \\(y=-163.7+103.2*x\\). Are the two hypotheses? pubsnew &lt;- pubs[ which(pubs$pubs&lt;80), ] # remove the outlier pubsnew=pubsnew %&gt;% add_row(pubs = 4, mortality = 0) # adding a pair of values pubRegNew &lt;- lm(mortality~pubs, data = pubsnew) summary(pubRegNew) ## ## Call: ## lm(formula = mortality ~ pubs, data = pubsnew) ## ## Residuals: ## Min 1Q Median 3Q Max ## -249.11 -36.48 19.57 75.62 131.67 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -163.701 86.000 -1.903 0.106 ## pubs 103.203 2.055 50.229 4.18e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 128.9 on 6 degrees of freedom ## Multiple R-squared: 0.9976, Adjusted R-squared: 0.9972 ## F-statistic: 2523 on 1 and 6 DF, p-value: 4.177e-09 tab_model( pubRegNew,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) )   mortality Predictors Estimates CI p df (Intercept) -163.70 -374.14 – 46.73 0.106 6.00 pubs 103.20 98.18 – 108.23 &lt;0.001 6.00 Observations 8 R2 / R2 adjusted 0.998 / 0.997 ggplot(pubsnew,aes(x=pubs, y=mortality))+ geom_smooth(method=lm)+ geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.5 Selling music records We now evaluate a similar more complex and more realistic data set those one would encounter in a study in medicine, sociology or ecological. The data represents the amount of money dedicated to the promotion of different CD’s from a music company and the number of CD’s (CD/downloads) sold. In the first line we see the amount of pounds sterling, £ (UK) dedicated to the promotion of the album music, in the first line we see that he spent £10,256, and then the number of CDs sold was 330. We have information about 200 albums different. library(readr) Album_Sales_1_new &lt;- read_csv(&quot;Data/Album_Sales_1_new.csv&quot;) ## Rows: 200 Columns: 4 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): adverts, sales, airplay, attract ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(Album_Sales_1_new) ## # A tibble: 6 × 4 ## adverts sales airplay attract ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.3 330 43 10 ## 2 986. 120 28 7 ## 3 1446. 360 35 7 ## 4 1188. 270 33 7 ## 5 575. 220 44 5 ## 6 569. 170 19 5 length(Album_Sales_1_new$adverts) # count how many data points we have. ## [1] 200 shapiro.test(Album_Sales_1_new$adverts) # test of normality using Shapiro-Wilks ## ## Shapiro-Wilk normality test ## ## data: Album_Sales_1_new$adverts ## W = 0.92542, p-value = 1.482e-08 library(nortest) #Anderson-Darling ad.test(Album_Sales_1_new$adverts) # test of normality using Anderson_Darling test ## ## Anderson-Darling normality test ## ## data: Album_Sales_1_new$adverts ## A = 3.8762, p-value = 1.089e-09 We begin by graphing the relationship between the two variables. Note that in the geom_smooth() part, has to include method=lm, this means that the method of constructing the line will use linear regression. It is added to the linear function \\(\\epsilon\\) that represents the errors of the values when comparing with the line that represents the best model. \\[Y_{ i }=\\beta _{ 0 }+\\beta _{ 1 }X_{ i }+\\epsilon _{ i }\\] Remember that \\(\\beta _{ 0 }\\) is the intercept and the \\(\\beta _{ 1 }x_{ i }\\) is the earring. The shaded area is the area of 95% interval of trust. This means that the best line, intercept and slope could vary in this range if we repeat the experiment. Note here all the alternatives, I added the two extreme slopes, with a slope major (red) and a minor (violet). Each point represents the sale of a CD with its corresponding amount dedicated to the promotion. The \\(epsilon\\) would be the difference between the best line and the original value. This is also called the residuals. library(ggplot2) ggplot(Album_Sales_1_new,aes(x=adverts, y=sales))+ geom_smooth(method=lm, se = TRUE)+ geom_point()+ geom_segment(aes(x=0, y=120, xend=2250, yend=380), colour=&quot;red&quot;)+ geom_segment(aes(x=0, y=150, xend=2250, yend=320), colour=&quot;purple&quot;) ## Warning in geom_segment(aes(x = 0, y = 120, xend = 2250, yend = 380), colour = &quot;red&quot;): All aesthetics have length 1, but the data has 200 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing a single row. ## Warning in geom_segment(aes(x = 0, y = 150, xend = 2250, yend = 320), colour = &quot;purple&quot;): All aesthetics have length 1, but the data has 200 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing a single row. ## `geom_smooth()` using formula = &#39;y ~ x&#39; The linear model with the lm() function. How do you interpret the coefficients and if these are significant? Are the null hypotheses rejected? library(sjPlot) model1=lm(sales~adverts, Album_Sales_1_new) #summary(model1) tab_model( model1,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) )   sales Predictors Estimates CI p df (Intercept) 134.14 119.28 – 149.00 &lt;0.001 198.00 adverts 0.10 0.08 – 0.12 &lt;0.001 198.00 Observations 200 R2 / R2 adjusted 0.335 / 0.331 ggplot(Album_Sales_1_new, aes(x=adverts, y=sales))+ geom_smooth(method=lm)+ geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.6 Linear Regression Assumptions Equality of variance: In the first graph you must evaluate whether the data are distributed more or less equally. I mean you must determine that not there is more variation in one area of the graph compared to another area. Note that the data are more or less equally distributed over above and below the zero line through the distribution of the Fitted” values, which are the predicted values. Normality of the data, evaluate the graph #2 with the graph of qqplot we see that the data follows the null model (the line) almost perfectly fits the data, then one can assume that the data complies with a normal distribution. But note that the data in the upper quartile they are not above the line. Evaluate whether there are biased data (outliers) that influence the results, evaluate graph #3. If the standardized values of “Student” are greater than 3, this suggests that there are outliers. In the fourth graph, Cook’s distance evaluate if there are values that have a lot of weight if whether or not they are included in the analysis, evaluate graph #4. These are going to be identified. The values to worry about are those that are above or below the broken line. In this graph there are three values that must be evaluated (1, 42, 169), these values must be ensured to be correct. It is always good to remove the biased values and redo the analysis to observe how different the results are. Typically more data in an analysis, the less weight a biased value will have on the results, unless this value is very different from most of the data. plot(model1) # Evaluate the assumptions, 1. Equality of variance, 2. Normality, 3. Students biased estimator, 4. Biased data (Cook&#39;s Distance) 10.7 Cook’s Distance Continuing with the theme of evaluating whether there are values that could influence the analysis a lot, we can use one of the tools to evaluate the weight of each value on a linear regression based on least squares methods, called Cook’s Distance. This analysis was developed by R. Dennis Cook in 1977 and has as its objective to evaluate each value in the data matrix and the weight it has about the result (when it is included or not in the analysis). Produces an index for each of the values on the result based on the residual values is called Cook’s Distance. Therefore, this analysis evaluates the relative impact of each value about the index. Unfortunately it is not clear what the value is critical; That is, what value can tell us that we are overweight? about the results. The two main suggestions are: Distance of Cook, Di, is greater than 1 (suggested by R. Dennis Cook Cook himself in 1982); and that Di &gt; 4/n, where n is the number of observations (Bollen et al. 1990). To make an illustration, we will continue with the model model1 using the values calculated in the previous model. The graph is will be re-organized using the seq_along option, so that the values in the X axis are based on the sequence of data in the file and the values on the Y axis are based on the values of the Distance of Cook. In this case, we see that all the values are well below of 1, suggesting that none of the individual values would greatly influence the results even if they were excluded. Now let us consider a second alternative of Di &gt; 4/n, then we note 6 \\(D_i\\) values that are greater than 4/200=0.02 should be of concern, where 200 is the amount of data in the file. If you consider this second alternative, it would be necessary to evaluate 6 values in the table of data that could be suspicious (values above the line red). Note that it’s not that they are incorrect; rather, this result is only a tool to evaluate values that appear to have a considerable impact on the results. Below is how to add the values of Cook’s distance to the data file, add a sequence column to the data, create a graph of Cook’s distance, and determine if there are values of Cook’s \\(D_i\\) greater than 1, or 4/n. the “cook.distance” values to your file Add a “sequence” column to the data Create a graph of Cook’s distance. Determine if there are values of Cook’s \\(D_i\\) greater than 1, or 4/n. 4/length(Album_Sales_1_new$adverts) ## [1] 0.02 Album_Sales_1_new$cooks.distance&lt;-cooks.distance(model1) Album_Sales_1_new$sequence=c(1:200) ggplot(Album_Sales_1_new, aes(sequence, cooks.distance))+ geom_point()+ geom_hline(aes(yintercept=4/length(Album_Sales_1_new$adverts), colour=&quot;red&quot;)) 10.8 Steps to do a linear regression analysis 10.8.1 Step 1 First step, build your model and evaluate the coefficients. The result: The coefficient (intercept) and slope of the model 10.8.2 Step 2 Evaluate if the coefficients are different from zero. The first hypothesis: Determine if the intercept is equal to zero. Look at the value of p, Pr(&gt;|t|), determine if the value is less than 0.05, if it is, the Ho is rejected and consequently we have confidence that the intercept does not include zero. The second null hypothesis: Determine if the slope is equal to zero. Look at the value of p, Pr(&gt;|t|), how the value is less than p=0.05, Ho is rejected and consequently we are confident that the slope does not include zero. 10.8.3 Step 3 Evaluate whether the data meet the assumptions of the model: Equality of variance, use the residual graph Normality, qqplot Biased values, Cook’s test advertizingReg &lt;- lm(sales~adverts, data = Album_Sales_1_new) summary(advertizingReg) ## ## Call: ## lm(formula = sales ~ adverts, data = Album_Sales_1_new) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.949 -43.796 -0.393 37.040 211.866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.341e+02 7.537e+00 17.799 &lt;2e-16 *** ## adverts 9.612e-02 9.632e-03 9.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.99 on 198 degrees of freedom ## Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 ## F-statistic: 99.59 on 1 and 198 DF, p-value: &lt; 2.2e-16 tab_model( advertizingReg,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) )   sales Predictors Estimates CI p df (Intercept) 134.14 119.28 – 149.00 &lt;0.001 198.00 adverts 0.10 0.08 – 0.12 &lt;0.001 198.00 Observations 200 R2 / R2 adjusted 0.335 / 0.331 ##Alternative to Graph the Residuals 10.8.4 Plot the residuals If the assumption of equality of variance is met, what we will observe that the distribution of the residuals looks more or less uniform around the average of the residuals (zero). There are approximately equal number of values greater than zero (above the blue line) and less than zero (below the blue line) that are distributed to through the variable on the X axis, or estimated values. In addition that the residuals (negative or positive) are not limited to sub groups of the estimated values (in the X). # &quot;model1&quot;, nota que este no es un data frame pero un modelo # La figura principal ggplot(Album_Sales_1_new, aes(x=adverts, y=sales))+ geom_smooth(method=lm, se = TRUE)+ geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Graficando los residuales con ggplot2 ggplot(model1, aes(x=.fitted, y=.resid))+ # note here we use the model1 with the .fitted and .resid to make the figure geom_point()+ geom_smooth(method=lm) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.8.5 Exercise 1 The data state.x77 comes from the package datasets where represents information about the American states, their level population, income, level of illiteracy, life expectancy, and other variables. Select the illiteracy and life variables expectancy”, and evaluates if there is a relationship between the two variables. Note you have to clean the variable names, because some of the names has spaces. The janitor package and the function are used clean_names(). Use illiteracy data and evaluate it as a predictor of life expectancy (life_exp). Using the following data do a complete regression analysis linear. Determines whether the assumptions of the linear regression are met. library(datasets) state.x77=as.data.frame(state.x77) # convert the data into a data frame state.x77=janitor::clean_names(state.x77) # clean the names of the variables head(state.x77) ## population income illiteracy life_exp murder hs_grad frost area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 10.8.6 EExercise 2 The ToothGrowth data set in the datasets package contains the result of an experiment that studies the effect of vitamin C on the growth of teeth in 60 guinea pigs. each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two methods of administration (orange juice or ascorbic acid (a form of vitamin C and coded as VC). Using the following data do a complete regression analysis linear. library(datasets) head(ToothGrowth) ## len supp dose ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10.0 VC 0.5 10.9 Scatter plots with jitter 10.10 Scatter plots with marginal histograms 10.11 Scatter plots for multiple variables "],["linear-regression-model.html", "Chapter 11 Linear regression model 11.1 Visualize the model with ggplot2 11.2 the linear model 11.3 the summary of the model 11.4 the assumptions of the model", " Chapter 11 Linear regression model 11.1 Visualize the model with ggplot2 11.2 the linear model 11.3 the summary of the model 11.4 the assumptions of the model "],["ring-chart-alternative-to-pie-chart.html", "Chapter 12 Ring chart (alternative to pie chart)", " Chapter 12 Ring chart (alternative to pie chart) "],["histogramsdensity-plots.html", "Chapter 13 Histograms/Density Plots", " Chapter 13 Histograms/Density Plots "],["boxplots.html", "Chapter 14 Boxplots", " Chapter 14 Boxplots "],["other-visualizations.html", "Chapter 15 Other visualizations", " Chapter 15 Other visualizations "],["linear-regression.html", "Chapter 16 Linear Regression 16.1 Model of linear regression 16.2 Assumption of linear regression 16.3 Visualization of the model 16.4 Quantile Regression 16.5 The model 16.6 The assumptions 16.7 Visualization", " Chapter 16 Linear Regression 16.1 Model of linear regression 16.2 Assumption of linear regression 16.3 Visualization of the model 16.4 Quantile Regression 16.5 The model 16.6 The assumptions 16.7 Visualization "],["generalized-linear-models.html", "Chapter 17 Generalized Linear Models 17.1 Linear models 17.2 Necesito cambiar este gráfico, ya que viene del web (del siguiente website “https://towardsdatascience.com”). 17.3 When the dependent variable is not linear 17.4 Link available 17.5 The assumptions 17.6 The advantages of GLM 17.7 ## Binomial or Bernoulli models 17.8 The basic parameters of the gamma. 17.9 Evaluando distribuciones 17.10 Binomial regression 17.11 Poisson regression 17.12 Negative binomial regression 17.13 Multinomial logit analysis", " Chapter 17 Generalized Linear Models if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(tidyverse, huxtable, wakefield, SuppDists,univariateML) library(tidyverse) library(huxtable) library(wakefield) library(SuppDists) library(univariateML) 17.1 Linear models Generalized linear models, GLM, are an extension of linear models where the dependent variable has a normal distribution. We remember that in a linear regression the dependent variable follows the model \\(y_i = \\beta_0 + \\beta_1 x_i\\), where \\(\\beta_0\\) is the intercept and the \\(\\beta_1\\) is the coefficient, that is, the slope and the \\(x_i\\) are the values of x’s. One of the assumptions is that the variation in the values of \\(\\mu_i\\), which are the \\(y_i\\), have a normal distribution in each x’s and that there is homogeneity of variance. \\[\\mu_i=\\beta_0+\\beta_1x_i\\] An important assumption is the assumption that the variation in the \\(y_i\\) are normally distributed and that there is homogeneity of variance. \\[y_i\\sim N\\left(\\mu_i,\\ \\epsilon\\right)\\] We can visualize it with the following figure, where the values of y’s have a normal distribution and that this distribution is homogeneous across the values of x’s. 17.2 Necesito cambiar este gráfico, ya que viene del web (del siguiente website “https://towardsdatascience.com”). 17.3 When the dependent variable is not linear The main problem for a long time has been that the dependent variable does not have a normal distribution and consequently did not meet the assumptions of linear regression. The method of linearizing the response variable was developed by Nelder and Wedderburn 1972. Information about the test and its evolution of GLM can be found at https://encyclopediaofmath.org/wiki/Generalized_linear_models. With the advancement of the use of computers in the 80s, it ended up being one of the most used statistical methods. The term GLM refers to a wide variety of regression models. The assumption in these models is that the response variable \\(y_i\\) follows a distribution within the family of exponential distributions with an average \\(\\mu_i\\), where a function \\(\\mu_i^T\\beta\\) is assumed that is frequently non-linear . To linearize the variable it is necessary to use a “link” to convert the dependent variable, \\(y_i\\). 17.4 Link available Here I show a partial list of the different types of “links” for different types of data (or distributions) of the independent variable, \\(y_i\\). The decision of which transformation or link to use is necessary will depend on the data types and their distribution. This is only a subset of avaliable link functions. Common Link function in GLM. ModeloVariable_DepLinkVariables_Independiente Linear regressionlNormalIdentidyContinuous ANOVANormalIdentityCategorical Logistic RegressionBinomialLogitMix Poisson RegressionPoissonLogMix Although the previous distributions are very common, they are not the only links available to transform the data. Here is supplementary information on some other R links that are available in certain packages. https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family Common link functions in GLM. FamiliaLinks Gaussian/Normalidentity, log, inverse Binomiallogit, probit, cauchit, log, cloglog Gammainverse, identity, log Poissonlog, identity, square root Inverse Gausian1/µ^2, inverse, identity, log 17.5 The assumptions The advantage is that now the dependent variable does not have to have a normal distribution. Furthermore, untransformed data does not have to have a linear relationship. However, the data must come from a distribution of the exponential family, for example, binomial, Poisson, multinomial, normal, inverse normal, etc. GLM does not assume a linear relationship between the dependent and independent variable, but assumes a linear relationship of the transformed dependent variable data and the independent variable. Consequently, a linear relationship is assumed between the binary variable and the explanatory variable after using the transformation with the link, \\(logit\\left(\\pi\\right)=\\beta_0+\\beta_1\\cdot x_i\\). The assumption of homogeneity of variance does not have to be taken into account. The errors have to be independent but it does not matter if they comply with a normal distribution. 17.6 The advantages of GLM The parameters are estimated using likelihood (MLE = Maximum Likelihood estimators), not least squares (OLS = ordinary least square). The models are estimated via likelihood, then they optimize the estimators, \\(\\beta\\). There is no need to transform the data of the dependent variable to normalize them. The selection of the “link” is independent of the dependent variable(s), which makes it easier to create models. Tools to evaluate inferences and models are available such as evaluating residuals, confidence intervals, deviation, likelihood ratio test among others. 17.7 ## Binomial or Bernoulli models If the variable is binomial, there are only two alternatives, 0 and 1, or whether or not, dead or alive, the binomial distribution is used. The logit function is used as a link function and the binomial/Bernoulli distribution as a probability distribution, the model is called Logistic Regression. \\[\\log\\frac{q_i}{1-q_i}=\\beta_0+\\beta_1X_i\\] where the distribution is a binomial with \\(y_i\\sim Binomial\\left(q_i\\right)\\). #library(wakefield) x=r_sample_binary(50, x = 0:1, prob =c(0.3, 0.7), name = &quot;Binary&quot;) #x df=as.data.frame(as.factor(x)) #df ggplot(df, aes(x))+ geom_histogram(fill=&quot;blue&quot;)+ scale_x_continuous(breaks = c(0, 1))+ xlab(&quot;Binary Values&quot;) 17.7.1 Gamma Distribution The gamma distribution is frequently used to take into account variables that have very long and large tails (Heavy-Tailed distributions). The distribution is widely used in the area of econometrics and survival estimates. The gamma distribution can be parameterized with a “shape term \\(\\alpha = k\\) and the inverse of a scale parameter \\(\\beta=1/\\theta\\) which is known as a * parameter *rate**. \\[f\\left(x\\right)=\\frac{\\left(\\beta^{\\alpha}\\cdot x^{\\alpha-1}e^{-\\beta x}\\right)}{\\Gamma\\left(\\alpha\\right)}\\ para\\ x&gt;0,\\ \\ \\alpha,\\ \\beta&gt;0\\] where \\(\\Gamma\\left(\\alpha\\right)\\) is the gamma function. For each integer value \\(\\Gamma\\left(\\alpha\\right)=\\left(\\alpha-1\\right)!\\) In another word, the gamma distribution is for modeling continuous variables that are always positive and have skewed distributions. Examples where the gamma distribution is used the time until the moment of failure of a team the time until the death of individuals the amount of water accumulated in a lake the size of unpaid loans Let’s look at some gamma distributions x = 0:20 curve(dgamma(x, shape=1, scale=2), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 3, lwd = 3, main = &quot;Examples of Gamma Distributions&quot;) curve(dgamma(x, shape=2, scale=2), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 5, lwd = 3, add = TRUE) curve(dgamma(x, shape=2, scale=4), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 6, lwd = 3, add = TRUE) curve(dgamma(x,shape =5, scale=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 7, lwd = 3, add = TRUE) curve(dgamma(x, shape=9, scale=0.5), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 1, lwd = 3, add = TRUE) curve(dgamma(x, shape=7.5, scale=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 2, lwd = 3, add = TRUE) legend(&quot;topright&quot;, c(&quot;k=1,theta=2&quot;, &quot;k=2,theta=2&quot;, &quot;k=2,theta=4&quot;,&quot;k=5,theta=1&quot;,&quot;k=9,theta=0.5&quot;, &quot;k=7.5,theta=1.0&quot;), col = c(3, 5,6, 7,1,2), lwd = 3, inset = 0.05) 17.8 The basic parameters of the gamma. Note: The scale is a dispersion index, and the higher the number, the longer the tail. The average is equal to the multiplication of shape=k by scale = theta, \\[average=k\\cdot\\theta\\]. The variance is equal to the multiplication of shape=k by scale = (theta)^2, \\[variance=k\\cdot\\theta^2\\] You can find the formulas to calculate other parameters on the Wikipedia page. https://en.wikipedia.org/wiki/Gamma_distribution 17.8.1 La distribución gaussiana inversa En la teoría de la probabilidad, la distribución gaussiana inversa es una familia de dos parámetros de distribuciones de probabilidad continuas con apoyo en (0, ∞). La distribución sigue la siguiente forma \\[f\\left(x;µ,\\lambda\\right)=\\sqrt{\\frac{\\lambda}{2\\pi x^3}}\\exp\\left(-\\frac{\\lambda\\left(x-µ\\right)^2}{2µ^2x}\\right)\\] donde \\(x&gt;0\\), \\(µ&gt;0\\), y \\(\\lambda\\) es la forma de distribución y siempre es mayor de cero. Mayor es el \\(\\lambda\\) (el shape parameter) más simétrica es la distribución. Cuando λ tiende al infinito, la distribución gaussiana inversa se parece a una distribución normal (gaussiana). La distribución gaussiana inversa se conoce también como la distribución Wald. #La distribución se usa cuando la distribución de población donde la distribución lognormal tiene una cola derecha demasiado pesada. Cuando se refiere a cola pesadas en estadistica, se refiere a que hay más probabilidades en esta region que una distribución normal. Por consecuencia las probabilidades en esta region es mayor. La distribución es utiliza para modelar datos no negativos que son sesgados positivamente. En otra palabra todos los valores son positivos y la cola tiende a disminiur más lentamente que en una distribución normal. 17.8.1.1 Historia gausiana inversa Información historica sobre la distribución gaussiana inversa es poco limitado. La distribución fue aparentemente derivada por la primera vez por Louis Bachelier en 1900, cuando el trataba de estimar el precio de la bolsa de valores para diferentes companias. Pero el nombre de “Inverse Gaussian” fue sugerido por Maurice Tweedie en 1945. Vea este enlace para más detalles. https://en.wikipedia.org/wiki/Normal-inverse_Gaussian_distribution Ejemplos que pudiese ser de este tipo de distribución el tiempo que toma llegar a un sitio. la distribución de precios de casas la cantidad de hijos en una familia la supervivencia de organismos (survival data) Nota library(SuppDists) #dinvGauss(x, nu, lambda, log=FALSE) x = -1:3 curve(dinvGauss(x, nu=.7, lambda=0.2, log=FALSE), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 3, lwd = 2, main = &quot;Gráfico distribución Gausiana Inversa&quot;) curve(dinvGauss(x, nu=.7, lambda=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 5, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=.7, lambda=3), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 6, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=2, lambda=0.2), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 7, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=2, lambda=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 1, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=2, lambda=4), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 2, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=1, lambda=40), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 4, lwd = 2, add = TRUE) legend(&quot;topright&quot;, c(&quot;nu=.7,lambda=0.2&quot;, &quot;nu=.7,lambda=1&quot;, &quot;nu=.7,lambda=3&quot;,&quot;nu=2,lambda=0.2&quot;,&quot;nu=2,lambda=1&quot;, &quot;nu=2,lambda=4&quot;,&quot;nu=1,lambda=40&quot;), col = c(3, 5,6, 7,1,2,4), lwd = 3, inset = 0.05) 17.8.1.2 Los parámetros de la distribución inversa de Gauss El promedio se calcula de la siguiente forma \\[E\\left[X\\right]=\\frac{1}{\\mu}+\\frac{1}{\\lambda}\\] La varianza se calcula de la siguiente forma \\[Var\\left[X\\right]=\\frac{1}{\\mu\\lambda}+\\frac{2}{\\lambda^2}\\] Para otros parámetros vean este enlace https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution 17.9 Evaluando distribuciones Evaluar un conjunto de datos para visualizar la distribución. Si necesita hacer análisis de regresión con datos que tienen una distribución gaussiana inversa el siguiente paquete esta disponible invGauss y si quiere determinar cual es la mejor distribución para sus datos con el paquete univariateML hay unas funciones para ayudar a determinar. Entre tanto que desarollo este tema más profundo le suguiero este enlace. https://www.cienciadedatos.net/documentos/55_ajuste_distribuciones_con_r.html library(invGauss) ## Loading required package: survival ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## aml library(univariateML) data(d.oropha.rec) #d.oropha.rec ggplot(data = d.oropha.rec) + geom_histogram(aes(x = time, y = after_stat(density)), bins = 40, alpha = 0.3, color = &quot;black&quot;) + geom_rug(aes(x = time)) + stat_function(fun = function(.x){dml(x = .x, obj = mlnorm(d.oropha.rec$time))}, aes(color = &quot;normal&quot;), size = 1) + stat_function(fun = function(.x){dml(x = .x, obj = mlinvgauss(d.oropha.rec$time))}, aes(color = &quot;inverse-normal&quot;), size = 1) + scale_color_manual(breaks = c(&quot;normal&quot;, &quot;inverse-normal&quot;), values = c(&quot;normal&quot; = &quot;red&quot;, &quot;inverse-normal&quot; = &quot;blue&quot;)) + labs(title = &quot;Distribución de tiempo de dessarrolo&quot;, color = &quot;Distribución&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;) ## Warning in stat_function(fun = function(.x) {: All aesthetics have length 1, but the data has 192 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing a single row. ## Warning in stat_function(fun = function(.x) {: All aesthetics have length 1, but the data has 192 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing a single row. # Nota que la cola de tiempo es larga un no tiene una distribución normal Ejemplo de los precios de diamantes del paquete ggplot2 #library(univariateML) ggplot(data = diamonds) + geom_histogram(aes(x = price, y = after_stat(density)), bins = 40, alpha = 0.3, color = &quot;black&quot;) + geom_rug(aes(x = price)) + stat_function(fun = function(.x){dml(x = .x, obj = mllnorm(diamonds$price))}, aes(color = &quot;log-normal&quot;), size = 1) + stat_function(fun = function(.x){dml(x = .x, obj = mlinvgauss(diamonds$price))}, aes(color = &quot;inverse-normal&quot;), size = 1) + scale_color_manual(breaks = c(&quot;log-normal&quot;, &quot;inverse-normal&quot;), values = c(&quot;log-normal&quot; = &quot;red&quot;, &quot;inverse-normal&quot; = &quot;blue&quot;)) + labs(title = &quot;Distribución precio diamantes&quot;, color = &quot;Distribución&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;) ## Warning in stat_function(fun = function(.x) {: All aesthetics have length 1, but the data has 53940 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing a single row. ## Warning in stat_function(fun = function(.x) {: All aesthetics have length 1, but the data has 53940 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing a single row. 17.10 Binomial regression 17.11 Poisson regression 17.12 Negative binomial regression 17.13 Multinomial logit analysis 17.13.1 Model 17.13.2 Assumptions 17.13.3 Post estimation analysis "],["beta-regression.html", "Chapter 18 Beta Regression 18.1 What is a beta regression? 18.2 What is the problem with the data? 18.3 First step, what is a beta distribution? 18.4 Wikipedia 18.5 Proportion of smokers in different countries 18.6 Convert the mean and variance into shape \\(\\alpha\\) and \\(\\beta\\) 18.7 Visualization of the distribution with CI. 18.8 Regression model 18.9 Regresión beta, proporción de frutos por cantidad de flores 18.10 Visualize the beta regression 18.11 Compare the traditional linear model versus a beta model 18.12 Distribution of specific values", " Chapter 18 Beta Regression The method presented here is quite innovative (2010 onwards). Unfortunately there is not much information in the literature or the web about the method. You can find supplementary information in the Vignette of the package betareg. References: Cribari-Neto, F., and Zeileis, A. (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. http://www.jstatsoft.org/v34/i02/. Grün, B., Kosmidis, I., and Zeileis, A. (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. http://www.jstatsoft.org/v48/i11/. Article on the beta regression and other R packages with other functions can be found here, Douma and Weedon 2019 18.1 What is a beta regression? The beta regression is an approach under GLM “Generalized linear models”. The beta regression models dependent variables distributed with the beta distribution. Data with the beta distribution include proportions and ratios, where the values \\(x\\) are between 0 and 1 but not inclusive (i.e. \\(0 &lt; x&lt; 1\\)). In some packahes 0 and 1 can be part of the data set. In addition to producing a regression that maximizes the likelihood (both for the mean and the precision of a response distributed in beta), bias-corrected estimates are provided. The values of the response variable satisfy \\(0 &lt; x &lt; 1\\). Consequently, if the values are 0 or 1, it is necessary to change them to \\(0= 0.001\\) and \\(1 = 0.999\\). The numbers cannot be 0 or 1, they must be greater than 0 and less than 1. Indeed, changing the values to \\(0.001\\) and \\(0.999\\) has no impact on the interpretation of the data, unless all the data are only \\(0\\) or \\(1\\), in which case this tool should not be used but a logistic regression. The betaref package is used to perform the beta regression. The package is quite powerful and provides a lot of information about the data. The package is used to calculate the mean, the variance, the precision, and the quantiles of the data. The package also provides the AIC, BIC, and the log-likelihood of the model. Note that the focus of the GLM model is to develop a regression with the response through a link function and a linear predictor. Just like normal GLM, there are numerous link functions, which can be useful such as “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” to linearize the data. Almosty all of the information presented here comes from Cribari-Neto and Zeileis (Beta Regression in R). Consult the pdf, https://cran.r-project.org/web/packages/betareg/betareg.pdf for a package description and more details. Typical analysis errors with data that are fractions. We look at an example. Here the relationship between per capita public health spending in 156 countries and the percentage of girls who are not in school. Data from the “World Development Agency” 18.2 What is the problem with the data? library(ggversa) #Edu_Salud_Gastos_GDP ggplot(Edu_Salud_Gastos_GDP, aes(Gasto_Salud_percapita, Porc_Ninas_no_escuela))+ geom_point()+ geom_smooth(method = lm)+ xlab(&quot;Per capita public health spending&quot;)+ ylab(&quot;Percentage of girls not going to school&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 46 rows containing non-finite outside the scale range ## (`stat_smooth()`). ## Warning: Removed 46 rows containing missing values or values outside the scale ## range (`geom_point()`). Note: there are negative proportion values the confidence interval is also negative the dispersion of the data in y around the mean is not equal as public health spending per capita changes (in x). Models using the beta distribution solve these issues. 18.3 First step, what is a beta distribution? The most important thing about the beta distribution is that the values NEVER are less than 0 or greater than 1 (i.e. \\(0 &lt; x &lt; 1\\)). In addition, the confidence intervals cannot be less than 0 or greater than 1. Here are some examples of the beta distribution. The beta distribution is calculated with two parameters, shape 1 or \\(\\alpha\\) and shape 2 or \\(\\beta\\). We will not go into these parameters, although you can go to the Wikipedia page for more information. Note that if the parameters are not equal (\\(\\alpha \\neq \\beta\\)), the distribution is not symmetrical. There is always a tail that extends to small or large values. Here a series beta distributions. We will not go into how these these parameters are calculated, although you can go to the Wikipedia page for more information. Note that if the parameters are not equal (\\(\\alpha \\neq \\beta\\)), the distribution is not symmetrical. There is always a tail that extends to small or large values. 18.4 Wikipedia On the wikipedia page, you can see how the distribution changes when the parameters change. 18.5 Proportion of smokers in different countries The data comes from the World Bank at the following link, Smokers. The file contains information on 187 countries and the proportion of the population over 15 years old who smoke. library(readr) Proportion_smokers_world &lt;- read_csv(&quot;Data/Proportion_smokers_world.csv&quot;) ## Rows: 187 Columns: 13 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): Country_Name, Country_Code, Indicator_Name, Indicator_Code ## dbl (9): Y2000, Y2005, Y2010, Y2011, Y2012, Y2013, Y2014, Y2015, Y2016 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Smokers=Proportion_smokers_world gt(head(Smokers)) #fukidcnnil table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #fukidcnnil thead, #fukidcnnil tbody, #fukidcnnil tfoot, #fukidcnnil tr, #fukidcnnil td, #fukidcnnil th { border-style: none; } #fukidcnnil p { margin: 0; padding: 0; } #fukidcnnil .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #fukidcnnil .gt_caption { padding-top: 4px; padding-bottom: 4px; } #fukidcnnil .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #fukidcnnil .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #fukidcnnil .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fukidcnnil .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fukidcnnil .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fukidcnnil .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #fukidcnnil .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #fukidcnnil .gt_column_spanner_outer:first-child { padding-left: 0; } #fukidcnnil .gt_column_spanner_outer:last-child { padding-right: 0; } #fukidcnnil .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #fukidcnnil .gt_spanner_row { border-bottom-style: hidden; } #fukidcnnil .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #fukidcnnil .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #fukidcnnil .gt_from_md > :first-child { margin-top: 0; } #fukidcnnil .gt_from_md > :last-child { margin-bottom: 0; } #fukidcnnil .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #fukidcnnil .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #fukidcnnil .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #fukidcnnil .gt_row_group_first td { border-top-width: 2px; } #fukidcnnil .gt_row_group_first th { border-top-width: 2px; } #fukidcnnil .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fukidcnnil .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #fukidcnnil .gt_first_summary_row.thick { border-top-width: 2px; } #fukidcnnil .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fukidcnnil .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fukidcnnil .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #fukidcnnil .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #fukidcnnil .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #fukidcnnil .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fukidcnnil .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fukidcnnil .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fukidcnnil .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fukidcnnil .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fukidcnnil .gt_left { text-align: left; } #fukidcnnil .gt_center { text-align: center; } #fukidcnnil .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #fukidcnnil .gt_font_normal { font-weight: normal; } #fukidcnnil .gt_font_bold { font-weight: bold; } #fukidcnnil .gt_font_italic { font-style: italic; } #fukidcnnil .gt_super { font-size: 65%; } #fukidcnnil .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #fukidcnnil .gt_asterisk { font-size: 100%; vertical-align: 0; } #fukidcnnil .gt_indent_1 { text-indent: 5px; } #fukidcnnil .gt_indent_2 { text-indent: 10px; } #fukidcnnil .gt_indent_3 { text-indent: 15px; } #fukidcnnil .gt_indent_4 { text-indent: 20px; } #fukidcnnil .gt_indent_5 { text-indent: 25px; } #fukidcnnil .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #fukidcnnil div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Country_Name Country_Code Indicator_Name Indicator_Code Y2000 Y2005 Y2010 Y2011 Y2012 Y2013 Y2014 Y2015 Y2016 Honduras HND Smoking prevalence, total (ages 15+) SH.PRV.SMOK 3.9 3.1 2.5 2.4 2.4 2.3 2.2 2.1 2.0 Ethiopia ETH Smoking prevalence, total (ages 15+) SH.PRV.SMOK 4.8 4.6 4.5 4.4 4.5 4.4 4.4 4.4 4.4 Congo, Rep. COG Smoking prevalence, total (ages 15+) SH.PRV.SMOK 5.7 9.1 14.7 16.1 17.9 19.8 22.0 24.2 26.9 Ghana GHA Smoking prevalence, total (ages 15+) SH.PRV.SMOK 5.9 5.0 4.4 4.3 4.3 4.1 4.1 4.0 3.9 Niger NER Smoking prevalence, total (ages 15+) SH.PRV.SMOK 6.4 6.7 7.1 7.2 7.3 7.4 7.5 7.6 7.7 Nigeria NGA Smoking prevalence, total (ages 15+) SH.PRV.SMOK 7.7 7.0 6.3 6.2 6.1 6.1 5.9 5.8 5.8 First convert the data in proportions since the program has to use data greater than 0 and less than 1. Select the year 2000 and create a histogram of the distribution. Smokers$Y2000P=(Smokers$Y2000)/100 # convertir en proporción Smokers %&gt;% dplyr::select(Country_Name, Y2000P) Country_NameY2000P Honduras0.039 Ethiopia0.048 Congo, Rep.0.057 Ghana0.059 Niger0.064 Nigeria0.077 Oman0.082 Barbados0.083 Eritrea0.087 Benin0.091 Eswatini0.093 Togo0.1&nbsp;&nbsp; Bahamas, The0.106 Senegal0.109 Haiti0.121 Peru0.121 Cabo Verde0.13&nbsp; Mali0.13&nbsp; Sub-Saharan Africa (excluding high income)0.134 Sub-Saharan Africa0.134 Sub-Saharan Africa (IDA &amp; IBRD countries)0.134 Liberia0.138 Ecuador0.139 Saudi Arabia0.145 Panama0.15&nbsp; Uzbekistan0.158 El Salvador0.16&nbsp; Singapore0.163 Sri Lanka0.165 Qatar0.165 Algeria0.166 Kenya0.166 Brunei Darussalam0.17&nbsp; Uganda0.171 Burkina Faso0.172 Iran, Islamic Rep.0.175 IDA blend0.175 Djibouti0.176 Egypt, Arab Rep.0.176 Lesotho0.176 Rwanda0.178 Costa Rica0.18&nbsp; Zambia0.18&nbsp; Zimbabwe0.18&nbsp; Jamaica0.184 Dominican Republic0.187 Morocco0.187 Middle East &amp; North Africa (excluding high income)0.189 Middle East &amp; North Africa (IDA &amp; IBRD countries)0.189 Middle East &amp; North Africa0.19&nbsp; Arab World0.191 Botswana0.195 Gambia, The0.199 Colombia0.201 IDA total0.206 Malawi0.21&nbsp; Comoros0.211 India0.212 Namibia0.223 Tanzania0.223 Bahrain0.224 Lower middle income0.224 IDA only0.226 South Asia0.227 South Asia (IDA &amp; IBRD)0.227 South Africa0.227 Moldova0.233 Yemen, Rep.0.233 Mozambique0.234 Least developed countries: UN classification0.235 Early-demographic dividend0.239 Mexico0.24&nbsp; Australia0.245 Pakistan0.245 Latin America &amp; Caribbean (excluding high income)0.246 Kuwait0.248 Thailand0.249 Mauritius0.25&nbsp; Seychelles0.251 Vietnam0.251 United Arab Emirates0.252 Brazil0.252 Latin America &amp; the Caribbean (IDA &amp; IBRD countries)0.253 Latin America &amp; Caribbean0.258 Portugal0.259 Small states0.26&nbsp; Other small states0.262 Low &amp; middle income0.263 Italy0.265 IDA &amp; IBRD total0.265 Azerbaijan0.267 Middle income0.267 Slovenia0.268 Kyrgyz Republic0.273 IBRD only0.277 World0.277 Canada0.282 Malaysia0.282 New Zealand0.294 Palau0.296 Finland0.297 China0.301 Iceland0.301 Cambodia0.301 Upper middle income0.301 East Asia &amp; Pacific (excluding high income)0.304 East Asia &amp; Pacific (IDA &amp; IBRD countries)0.304 Late-demographic dividend0.304 East Asia &amp; Pacific0.305 Paraguay0.308 North America0.311 Switzerland0.312 Bangladesh0.314 United States0.314 Armenia0.318 Tunisia0.318 Israel0.319 Kazakhstan0.319 Vanuatu0.319 Slovak Republic0.321 Georgia0.322 Mongolia0.322 Sweden0.323 Myanmar0.325 Indonesia0.329 Japan0.33&nbsp; OECD members0.33&nbsp; Croatia0.331 High income0.336 Post-demographic dividend0.338 Czech Republic0.341 Korea, Rep.0.343 Philippines0.344 Luxembourg0.347 Albania0.348 France0.349 Euro area0.35&nbsp; Fiji0.351 Germany0.353 Malta0.353 Lithuania0.359 Ukraine0.36&nbsp; European Union0.361 Tonga0.363 Belarus0.367 Europe &amp; Central Asia0.37&nbsp; Maldives0.371 Andorra0.374 Belgium0.374 Lebanon0.375 Netherlands0.377 Ireland0.378 United Kingdom0.382 Europe &amp; Central Asia (excluding high income)0.383 Pacific island small states0.383 Denmark0.383 Turkey0.384 Europe &amp; Central Asia (IDA &amp; IBRD countries)0.385 Latvia0.388 Nepal0.389 Spain0.395 Central Europe and the Baltics0.395 Estonia0.396 Romania0.396 Poland0.407 Hungary0.411 Argentina0.414 Sierra Leone0.422 Cyprus0.424 Lao PDR0.427 Russian Federation0.428 Norway0.431 Samoa0.451 Cuba0.457 Bosnia and Herzegovina0.477 Serbia0.487 Austria0.491 Suriname0.5&nbsp;&nbsp; Timor-Leste0.518 Bulgaria0.52&nbsp; Montenegro0.527 Uruguay0.527 Greece0.535 Chile0.566 Papua New Guinea0.609 Nauru0.638 Kiribati0.734 18.6 Convert the mean and variance into shape \\(\\alpha\\) and \\(\\beta\\) Conver the mean and variance of the data into the values of the shape \\(\\alpha\\) and \\(\\beta\\). The following equation is used to calculate the shapes. The expected values and the variance behave differently. The mean of a beta distribution is equal to \\[E(X) = \\frac{\\alpha}{\\alpha+\\beta}\\] The variance os beta distribution is \\[V(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta+1)(\\alpha+\\beta)^2}\\] Using the mean and variance, the following equations can be used to convert them into \\(\\alpha\\) and \\(\\beta\\). \\[\\alpha = \\frac{1-mu}{(var-1)/mu}*mu^2\\] \\[\\beta = \\alpha*(\\frac{1}{mu}-1)\\] Here is the script to convert the parameters into shape. estBetaParams &lt;- function(mu, var) { alpha &lt;- ((1 - mu) / var - 1 / mu) * mu ^ 2 beta &lt;- alpha * (1 / mu - 1) return(params = list(alpha = alpha, beta = beta)) } #mean(Smokers$Y2000P) #var(Smokers$Y2000P) estBetaParams((mean(Smokers$Y2000P)), (var(Smokers$Y2000P))) ## $alpha ## [1] 3.592488 ## ## $beta ## [1] 9.181559 Now visualize de distibution of the proportion of smokers Green line is the normal distribution Red line is the beta distribution Smokers$Y2000P=(Smokers$Y2000)/100 # convertir en proporción x &lt;- seq(0, 1, len = 100) #mean(Smokers$Y2000P) #var(Smokers$Y2000P) ggplot(Smokers, aes(Y2000P))+ geom_histogram(aes(y=..density..), colour=&quot;white&quot;, fill=&quot;grey50&quot;)+ stat_function(aes(x = Smokers$Y2000P, y = ..y..), fun = dbeta, colour=&quot;red&quot;, n = 100, args = list(shape1 = 3.593, shape2 = 9.185))+ stat_function(fun = dnorm, args = list(mean = mean(Smokers$Y2000P, na.rm = TRUE), sd = sd(Smokers$Y2000P, na.rm = TRUE)), colour = &quot;green&quot;, size = 1)+ xlab(&quot;Proporción de fumadores de mayor \\n de 15 años por Pais&quot;)+ ylab(&quot;Densidad&quot;)+ annotate(&quot;text&quot;, x = .5, y = 4.2, label = &quot;Verde: dist Normal&quot;, color=&quot;darkgreen&quot;)+ annotate(&quot;text&quot;, x = .5, y = 3.7, label = &quot;Verde: dist Beta&quot;, color=&quot;red&quot;)+ xlim(-0.1, 0.9) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing missing values or values outside the scale ## range (`geom_bar()`). Calculate the confidence interval of the average of a beta distribution. The following packages are needed simpleboot, boot. library(simpleboot) ## Simple Bootstrap Routines (1.1-8) library(boot) # package to calculate the confidence interval of a beta distribution n=187 # The smaple size of the data set. alpha = 3.593 # The alpha parameter beta = 9.185 # the beta parameter x = rbeta(n, alpha, beta) x.boot = one.boot(x, median, R=10^4) # Here we use the median as the central tendency because the *mean* is bias to the right boot.ci(x.boot, type=&quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = x.boot, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% ( 0.2592, 0.3101 ) ## Calculations and Intervals on Original Scale 18.7 Visualization of the distribution with CI. Overlay the confidence interval of the median on the beta distribution. The confidence interval is between 0.2320 and 0.2768. The confidence interval is not symmetric, and the distribution is not symmetric. The distribution is skewed to the right. ggplot(Smokers, aes(Y2000P))+ geom_histogram(aes(y=..density..), bins=20, colour=&quot;white&quot;, fill=&quot;grey50&quot;)+ stat_function(aes(x = Smokers$Y2000P, y = ..y..), fun = dbeta, colour=&quot;red&quot;, n = 100, args = list(shape1 = 3.593, shape2 = 9.185))+ geom_vline(xintercept =0.2320, colour=&quot;blue&quot;)+ # the confidence interval, lower limit geom_vline(xintercept =0.2768, colour=&quot;blue&quot;)+ # the confidence interval, upper limit rlt_theme+ xlab(&quot;Proportion of smokers over 15 years old by country&quot;) 18.8 Regression model Now let us view an example using regression model, where the response is a proportion. library(betareg) # El paquete para hacer regresión beta library(ggversa) # paquete para los datos attach(dipodium) head(dipodium, n=4) Tree NumberTree speciesDBHPlant numberRamet numberDistanceOrientationNumber_of_FlowersHeight_InfloHerbivoryRowPosition_NFNumber_Flowers_positionNumber_of_fruitsPerc_FR_setpardalinum_or_roseumFruit_position_effectFrutos_si_o_noP_or_R_Infl_LenghtNum of fruitsSpecies_NameCardinal orientation 1E.o75112.47401135n12400&nbsp;&nbsp;&nbsp;r10r0r1 1E.o76211.97501947n22300&nbsp;&nbsp;&nbsp;r20r0r2 2E.o76311.953501863n32510.04r30r1r8 3E.o58413.242102447n42050.25r40r5r5 18.9 Regresión beta, proporción de frutos por cantidad de flores Now we will do the first regression analysis where our answer is a proportion. The data comes from an Australian orchid species, Dipodium roseum, collected by RLT in 2004-2005. We are going to evaluate the relationship between the number of flowers and the proportion of fruits per plant. The first step is to ensure that there are no values of 0 and 1. In this case there is not a single plant that has 100% fruits, but there are individuals that have zero fruits. REMEMBER x &gt;0 y &lt;1. 0 or 1 is NOT accepted. So a minimum value such as 0.001 can be added to the values of 0 and 0.001 can be subtracted from the 1 values. In reality, this modification does not impact the interpretation of the results. the NAs are also removed from the file. Note that the model is constructed as a linear model betareg(y~x, data =na.omit(df)). The variables in the file are PropFR, the proportion of fruits (number of fruits/numbers of flowers) for each individual and the number of flowers, Number_of_Flowers. #head(dipodium) library(tidyverse) dipodium$PropFR=dipodium$Perc_FR_set+0.0001 # solucionar para remover los cero #dipodium$PropFR dipodium2=dipodium %&gt;% dplyr::select(PropFR, Number_of_Flowers,Height_Inflo, Distance) %&gt;% filter(complete.cases(PropFR, Number_of_Flowers,Height_Inflo, Distance)) #dipodium2 write.csv(x=dipodium2, file=&quot;dipodium2.csv&quot;) library(readr) dipodium2 &lt;- read_csv(&quot;Data/dipodium2.csv&quot;) ## New names: ## Rows: 62 Columns: 5 ## ── Column specification ## ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; dbl ## (5): ...1, PropFR, Number_of_Flowers, Height_Inflo, Distance ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set `show_col_types = FALSE` to ## quiet this message. ## • `` -&gt; `...1` modelpropFr=betareg(PropFR~Number_of_Flowers+Height_Inflo+Distance, data =na.omit(dipodium2)) summary(modelpropFr) ## ## Call: ## betareg(formula = PropFR ~ Number_of_Flowers + Height_Inflo + Distance, ## data = na.omit(dipodium2)) ## ## Quantile residuals: ## Min 1Q Median 3Q Max ## -2.5667 -0.5097 0.0825 0.9165 1.7887 ## ## Coefficients (mean model with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.796016 0.727139 -5.220 1.78e-07 *** ## Number_of_Flowers 0.076200 0.028247 2.698 0.00698 ** ## Height_Inflo 0.006054 0.017847 0.339 0.73444 ## Distance -0.111010 0.093825 -1.183 0.23674 ## ## Phi coefficients (precision model with identity link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (phi) 4.8382 0.9982 4.847 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Type of estimator: ML (maximum likelihood) ## Log-likelihood: 113.9 on 5 Df ## Pseudo R-squared: 0.2757 ## Number of iterations: 19 (BFGS) + 2 (Fisher scoring) 18.10 Visualize the beta regression dipodiumbeta=dipodium2[,c(&quot;Number_of_Flowers&quot;,&quot;PropFR&quot;)] # Create a df with only the variables of interest dp2=dipodiumbeta[complete.cases(dipodiumbeta),] # remove all &quot;NA&quot; modelpropFr=betareg(PropFR~Number_of_Flowers, data=dp2) # The beta model using **betareg** summary(modelpropFr) ## ## Call: ## betareg(formula = PropFR ~ Number_of_Flowers, data = dp2) ## ## Quantile residuals: ## Min 1Q Median 3Q Max ## -2.3537 -0.4533 0.0723 0.8698 1.9507 ## ## Coefficients (mean model with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.88360 0.44251 -8.776 &lt; 2e-16 *** ## Number_of_Flowers 0.08259 0.01803 4.581 4.62e-06 *** ## ## Phi coefficients (precision model with identity link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (phi) 4.6947 0.9699 4.84 1.3e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Type of estimator: ML (maximum likelihood) ## Log-likelihood: 112.9 on 3 Df ## Pseudo R-squared: 0.2538 ## Number of iterations: 18 (BFGS) + 1 (Fisher scoring) predict(modelpropFr, type = &quot;response&quot;) # calculate the predicted values ## 1 2 3 4 5 6 7 ## 0.02568523 0.02783519 0.03015954 0.04856508 0.05679268 0.05679268 0.05679268 ## 8 9 10 11 12 13 14 ## 0.06138242 0.06138242 0.06138242 0.06138242 0.06138242 0.06138242 0.06631700 ## 15 16 17 18 19 20 21 ## 0.06631700 0.07161801 0.07161801 0.07161801 0.07161801 0.07730766 0.07730766 ## 22 23 24 25 26 27 28 ## 0.07730766 0.08340872 0.08340872 0.08994433 0.08994433 0.08994433 0.08994433 ## 29 30 31 32 33 34 35 ## 0.09693789 0.10441284 0.10441284 0.10441284 0.10441284 0.10441284 0.11239245 ## 36 37 38 39 40 41 42 ## 0.11239245 0.12089957 0.12089957 0.12089957 0.12089957 0.12089957 0.12089957 ## 43 44 45 46 47 48 49 ## 0.12995632 0.12995632 0.12995632 0.12995632 0.12995632 0.13958381 0.14980172 ## 50 51 52 53 54 55 56 ## 0.14980172 0.14980172 0.17207828 0.17207828 0.17207828 0.17207828 0.19690033 ## 57 58 59 60 61 62 ## 0.19690033 0.22433274 0.22433274 0.23903102 0.27035714 0.28695536 dp2$response=predict(modelpropFr, type = &quot;response&quot;) dp2$precision=predict(modelpropFr, type = &quot;precision&quot;) dp2$variance=predict(modelpropFr, type = &quot;variance&quot;) dp2$quantile_.01=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.01)) # calculate the quantiles of 1% dp2$quantile_.05=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.05)) dp2$quantile_.10=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.10)) # calculate the quantiles of 10% dp2$quantile_.15=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.15)) dp2$quantile_.20=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.20)) dp2$quantile_.25=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.25)) dp2$quantile_.30=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.30)) dp2$quantile_.35=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.35)) dp2$quantile_.40=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.40)) dp2$quantile_.45=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.45)) dp2$quantile_.50=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.50)) # calculate the quantiles of 50% (the median) dp2$quantile_.55=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.55)) dp2$quantile_.60=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.60)) dp2$quantile_.65=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.65)) dp2$quantile_.70=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.70)) dp2$quantile_.75=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.75)) dp2$quantile_.80=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.80)) dp2$quantile_.85=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.85)) dp2$quantile_.90=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.90)) # calculate the quantiles of 90% dp2$quantile_.95=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.95)) dp2$quantile_.99=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.99)) # calculate the quantiles of 99% dp2 (#tab:beta regression plot preparation) Number_of_FlowersPropFRresponseprecisionvariancequantile_.01quantile_.05quantile_.10quantile_.15quantile_.20quantile_.25quantile_.30quantile_.35quantile_.40quantile_.45quantile_.50quantile_.55quantile_.60quantile_.65quantile_.70quantile_.75quantile_.80quantile_.85quantile_.90quantile_.95quantile_.99 30.00010.02574.690.004393.86e-182.42e-127.58e-102.19e-082.38e-071.51e-066.86e-062.46e-057.46e-050.0001980.0004750.001050.002170.004240.007920.01430.02530.04440.07920.1530.342 40.00010.02784.690.004757.44e-171.66e-113.34e-097.44e-086.72e-073.71e-061.5e-05&nbsp;4.87e-050.0001350.0003330.0007480.001550.003040.005650.0101&nbsp;0.01750.02990.05060.08720.1630.353 50.00010.03024.690.005141.14e-159.81e-111.31e-082.3e-07&nbsp;1.75e-068.48e-063.07e-059.13e-050.0002350.0005390.00114&nbsp;0.002240.004160.0074&nbsp;0.0127&nbsp;0.02120.03490.05720.09570.1730.364 110.00010.04864.690.008112.75e-103.2e-07&nbsp;6.69e-063.96e-050.00014&nbsp;0.0003720.0008290.00163&nbsp;0.00295&nbsp;0.00497&nbsp;0.00795&nbsp;0.0122&nbsp;0.0182&nbsp;0.0265&nbsp;0.0378&nbsp;0.05340.07510.106&nbsp;0.155&nbsp;0.2420.434 130.03580.05684.690.009415.29e-092.21e-062.98e-050.0001360.0004010.0009280.00184&nbsp;0.0033&nbsp;&nbsp;0.00547&nbsp;0.00859&nbsp;0.0129&nbsp;&nbsp;0.0187&nbsp;0.0265&nbsp;0.0368&nbsp;0.0503&nbsp;0.06830.09260.127&nbsp;0.177&nbsp;0.2670.458 130.09390.05684.690.009415.29e-092.21e-062.98e-050.0001360.0004010.0009280.00184&nbsp;0.0033&nbsp;&nbsp;0.00547&nbsp;0.00859&nbsp;0.0129&nbsp;&nbsp;0.0187&nbsp;0.0265&nbsp;0.0368&nbsp;0.0503&nbsp;0.06830.09260.127&nbsp;0.177&nbsp;0.2670.458 130.00010.05684.690.009415.29e-092.21e-062.98e-050.0001360.0004010.0009280.00184&nbsp;0.0033&nbsp;&nbsp;0.00547&nbsp;0.00859&nbsp;0.0129&nbsp;&nbsp;0.0187&nbsp;0.0265&nbsp;0.0368&nbsp;0.0503&nbsp;0.06830.09260.127&nbsp;0.177&nbsp;0.2670.458 140.00010.06144.690.0101&nbsp;1.96e-085.21e-065.77e-050.0002360.0006410.00139&nbsp;0.00263&nbsp;0.00451&nbsp;0.00723&nbsp;0.011&nbsp;&nbsp;&nbsp;0.016&nbsp;&nbsp;&nbsp;0.0227&nbsp;0.0314&nbsp;0.0427&nbsp;0.0574&nbsp;0.07650.102&nbsp;0.137&nbsp;0.189&nbsp;0.28&nbsp;0.47&nbsp; 140.364&nbsp;0.06144.690.0101&nbsp;1.96e-085.21e-065.77e-050.0002360.0006410.00139&nbsp;0.00263&nbsp;0.00451&nbsp;0.00723&nbsp;0.011&nbsp;&nbsp;&nbsp;0.016&nbsp;&nbsp;&nbsp;0.0227&nbsp;0.0314&nbsp;0.0427&nbsp;0.0574&nbsp;0.07650.102&nbsp;0.137&nbsp;0.189&nbsp;0.28&nbsp;0.47&nbsp; 140.105&nbsp;0.06144.690.0101&nbsp;1.96e-085.21e-065.77e-050.0002360.0006410.00139&nbsp;0.00263&nbsp;0.00451&nbsp;0.00723&nbsp;0.011&nbsp;&nbsp;&nbsp;0.016&nbsp;&nbsp;&nbsp;0.0227&nbsp;0.0314&nbsp;0.0427&nbsp;0.0574&nbsp;0.07650.102&nbsp;0.137&nbsp;0.189&nbsp;0.28&nbsp;0.47&nbsp; 140.131&nbsp;0.06144.690.0101&nbsp;1.96e-085.21e-065.77e-050.0002360.0006410.00139&nbsp;0.00263&nbsp;0.00451&nbsp;0.00723&nbsp;0.011&nbsp;&nbsp;&nbsp;0.016&nbsp;&nbsp;&nbsp;0.0227&nbsp;0.0314&nbsp;0.0427&nbsp;0.0574&nbsp;0.07650.102&nbsp;0.137&nbsp;0.189&nbsp;0.28&nbsp;0.47&nbsp; 140.231&nbsp;0.06144.690.0101&nbsp;1.96e-085.21e-065.77e-050.0002360.0006410.00139&nbsp;0.00263&nbsp;0.00451&nbsp;0.00723&nbsp;0.011&nbsp;&nbsp;&nbsp;0.016&nbsp;&nbsp;&nbsp;0.0227&nbsp;0.0314&nbsp;0.0427&nbsp;0.0574&nbsp;0.07650.102&nbsp;0.137&nbsp;0.189&nbsp;0.28&nbsp;0.47&nbsp; 140.00010.06144.690.0101&nbsp;1.96e-085.21e-065.77e-050.0002360.0006410.00139&nbsp;0.00263&nbsp;0.00451&nbsp;0.00723&nbsp;0.011&nbsp;&nbsp;&nbsp;0.016&nbsp;&nbsp;&nbsp;0.0227&nbsp;0.0314&nbsp;0.0427&nbsp;0.0574&nbsp;0.07650.102&nbsp;0.137&nbsp;0.189&nbsp;0.28&nbsp;0.47&nbsp; 150.00010.06634.690.0109&nbsp;6.54e-081.15e-050.0001070.0003920.0009890.00203&nbsp;0.00366&nbsp;0.00605&nbsp;0.00937&nbsp;0.0138&nbsp;&nbsp;0.0197&nbsp;&nbsp;0.0273&nbsp;0.037&nbsp;&nbsp;0.0493&nbsp;0.0651&nbsp;0.08540.112&nbsp;0.149&nbsp;0.202&nbsp;0.2930.483 150.179&nbsp;0.06634.690.0109&nbsp;6.54e-081.15e-050.0001070.0003920.0009890.00203&nbsp;0.00366&nbsp;0.00605&nbsp;0.00937&nbsp;0.0138&nbsp;&nbsp;0.0197&nbsp;&nbsp;0.0273&nbsp;0.037&nbsp;&nbsp;0.0493&nbsp;0.0651&nbsp;0.08540.112&nbsp;0.149&nbsp;0.202&nbsp;0.2930.483 160.03710.07164.690.0117&nbsp;1.99e-072.39e-050.0001880.0006270.00148&nbsp;0.00288&nbsp;0.00499&nbsp;0.00794&nbsp;0.0119&nbsp;&nbsp;0.0172&nbsp;&nbsp;0.0239&nbsp;&nbsp;0.0324&nbsp;0.0431&nbsp;0.0565&nbsp;0.0733&nbsp;0.09480.123&nbsp;0.16&nbsp;&nbsp;0.215&nbsp;0.3070.496 160.00010.07164.690.0117&nbsp;1.99e-072.39e-050.0001880.0006270.00148&nbsp;0.00288&nbsp;0.00499&nbsp;0.00794&nbsp;0.0119&nbsp;&nbsp;0.0172&nbsp;&nbsp;0.0239&nbsp;&nbsp;0.0324&nbsp;0.0431&nbsp;0.0565&nbsp;0.0733&nbsp;0.09480.123&nbsp;0.16&nbsp;&nbsp;0.215&nbsp;0.3070.496 160.125&nbsp;0.07164.690.0117&nbsp;1.99e-072.39e-050.0001880.0006270.00148&nbsp;0.00288&nbsp;0.00499&nbsp;0.00794&nbsp;0.0119&nbsp;&nbsp;0.0172&nbsp;&nbsp;0.0239&nbsp;&nbsp;0.0324&nbsp;0.0431&nbsp;0.0565&nbsp;0.0733&nbsp;0.09480.123&nbsp;0.16&nbsp;&nbsp;0.215&nbsp;0.3070.496 160.077&nbsp;0.07164.690.0117&nbsp;1.99e-072.39e-050.0001880.0006270.00148&nbsp;0.00288&nbsp;0.00499&nbsp;0.00794&nbsp;0.0119&nbsp;&nbsp;0.0172&nbsp;&nbsp;0.0239&nbsp;&nbsp;0.0324&nbsp;0.0431&nbsp;0.0565&nbsp;0.0733&nbsp;0.09480.123&nbsp;0.16&nbsp;&nbsp;0.215&nbsp;0.3070.496 170.08010.07734.690.0125&nbsp;5.56e-074.69e-050.0003170.00097&nbsp;0.00215&nbsp;0.00399&nbsp;0.00664&nbsp;0.0102&nbsp;&nbsp;0.015&nbsp;&nbsp;&nbsp;0.021&nbsp;&nbsp;&nbsp;0.0286&nbsp;&nbsp;0.0381&nbsp;0.0498&nbsp;0.0642&nbsp;0.0822&nbsp;0.105&nbsp;0.134&nbsp;0.172&nbsp;0.228&nbsp;0.3210.509 170.333&nbsp;0.07734.690.0125&nbsp;5.56e-074.69e-050.0003170.00097&nbsp;0.00215&nbsp;0.00399&nbsp;0.00664&nbsp;0.0102&nbsp;&nbsp;0.015&nbsp;&nbsp;&nbsp;0.021&nbsp;&nbsp;&nbsp;0.0286&nbsp;&nbsp;0.0381&nbsp;0.0498&nbsp;0.0642&nbsp;0.0822&nbsp;0.105&nbsp;0.134&nbsp;0.172&nbsp;0.228&nbsp;0.3210.509 170.00010.07734.690.0125&nbsp;5.56e-074.69e-050.0003170.00097&nbsp;0.00215&nbsp;0.00399&nbsp;0.00664&nbsp;0.0102&nbsp;&nbsp;0.015&nbsp;&nbsp;&nbsp;0.021&nbsp;&nbsp;&nbsp;0.0286&nbsp;&nbsp;0.0381&nbsp;0.0498&nbsp;0.0642&nbsp;0.0822&nbsp;0.105&nbsp;0.134&nbsp;0.172&nbsp;0.228&nbsp;0.3210.509 180.04010.08344.690.0134&nbsp;1.44e-068.76e-050.0005150.00145&nbsp;0.00304&nbsp;0.00541&nbsp;0.00868&nbsp;0.013&nbsp;&nbsp;&nbsp;0.0185&nbsp;&nbsp;0.0254&nbsp;&nbsp;0.034&nbsp;&nbsp;&nbsp;0.0444&nbsp;0.0571&nbsp;0.0726&nbsp;0.0916&nbsp;0.115&nbsp;0.145&nbsp;0.185&nbsp;0.242&nbsp;0.3350.522 180.00010.08344.690.0134&nbsp;1.44e-068.76e-050.0005150.00145&nbsp;0.00304&nbsp;0.00541&nbsp;0.00868&nbsp;0.013&nbsp;&nbsp;&nbsp;0.0185&nbsp;&nbsp;0.0254&nbsp;&nbsp;0.034&nbsp;&nbsp;&nbsp;0.0444&nbsp;0.0571&nbsp;0.0726&nbsp;0.0916&nbsp;0.115&nbsp;0.145&nbsp;0.185&nbsp;0.242&nbsp;0.3350.522 190.00010.08994.690.0144&nbsp;3.45e-060.0001560.0008080.00212&nbsp;0.0042&nbsp;&nbsp;0.00718&nbsp;0.0112&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0226&nbsp;&nbsp;0.0305&nbsp;&nbsp;0.0399&nbsp;&nbsp;0.0514&nbsp;0.0651&nbsp;0.0816&nbsp;0.102&nbsp;&nbsp;0.126&nbsp;0.158&nbsp;0.198&nbsp;0.256&nbsp;0.35&nbsp;0.535 190.00010.08994.690.0144&nbsp;3.45e-060.0001560.0008080.00212&nbsp;0.0042&nbsp;&nbsp;0.00718&nbsp;0.0112&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0226&nbsp;&nbsp;0.0305&nbsp;&nbsp;0.0399&nbsp;&nbsp;0.0514&nbsp;0.0651&nbsp;0.0816&nbsp;0.102&nbsp;&nbsp;0.126&nbsp;0.158&nbsp;0.198&nbsp;0.256&nbsp;0.35&nbsp;0.535 190.238&nbsp;0.08994.690.0144&nbsp;3.45e-060.0001560.0008080.00212&nbsp;0.0042&nbsp;&nbsp;0.00718&nbsp;0.0112&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0226&nbsp;&nbsp;0.0305&nbsp;&nbsp;0.0399&nbsp;&nbsp;0.0514&nbsp;0.0651&nbsp;0.0816&nbsp;0.102&nbsp;&nbsp;0.126&nbsp;0.158&nbsp;0.198&nbsp;0.256&nbsp;0.35&nbsp;0.535 190.182&nbsp;0.08994.690.0144&nbsp;3.45e-060.0001560.0008080.00212&nbsp;0.0042&nbsp;&nbsp;0.00718&nbsp;0.0112&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0226&nbsp;&nbsp;0.0305&nbsp;&nbsp;0.0399&nbsp;&nbsp;0.0514&nbsp;0.0651&nbsp;0.0816&nbsp;0.102&nbsp;&nbsp;0.126&nbsp;0.158&nbsp;0.198&nbsp;0.256&nbsp;0.35&nbsp;0.535 200.00010.09694.690.0154&nbsp;7.76e-060.0002670.00123&nbsp;0.003&nbsp;&nbsp;&nbsp;0.00568&nbsp;0.00935&nbsp;0.0141&nbsp;&nbsp;0.0201&nbsp;&nbsp;0.0273&nbsp;&nbsp;0.0361&nbsp;&nbsp;0.0465&nbsp;&nbsp;0.059&nbsp;&nbsp;0.0737&nbsp;0.0913&nbsp;0.112&nbsp;&nbsp;0.138&nbsp;0.17&nbsp;&nbsp;0.212&nbsp;0.27&nbsp;&nbsp;0.3650.549 210.179&nbsp;0.104&nbsp;4.690.0164&nbsp;1.64e-050.0004380.00181&nbsp;0.00415&nbsp;0.00752&nbsp;0.012&nbsp;&nbsp;&nbsp;0.0176&nbsp;&nbsp;0.0244&nbsp;&nbsp;0.0326&nbsp;&nbsp;0.0424&nbsp;&nbsp;0.0538&nbsp;&nbsp;0.0673&nbsp;0.0831&nbsp;0.102&nbsp;&nbsp;0.124&nbsp;&nbsp;0.151&nbsp;0.184&nbsp;0.226&nbsp;0.285&nbsp;0.38&nbsp;0.563 210.154&nbsp;0.104&nbsp;4.690.0164&nbsp;1.64e-050.0004380.00181&nbsp;0.00415&nbsp;0.00752&nbsp;0.012&nbsp;&nbsp;&nbsp;0.0176&nbsp;&nbsp;0.0244&nbsp;&nbsp;0.0326&nbsp;&nbsp;0.0424&nbsp;&nbsp;0.0538&nbsp;&nbsp;0.0673&nbsp;0.0831&nbsp;0.102&nbsp;&nbsp;0.124&nbsp;&nbsp;0.151&nbsp;0.184&nbsp;0.226&nbsp;0.285&nbsp;0.38&nbsp;0.563 210.273&nbsp;0.104&nbsp;4.690.0164&nbsp;1.64e-050.0004380.00181&nbsp;0.00415&nbsp;0.00752&nbsp;0.012&nbsp;&nbsp;&nbsp;0.0176&nbsp;&nbsp;0.0244&nbsp;&nbsp;0.0326&nbsp;&nbsp;0.0424&nbsp;&nbsp;0.0538&nbsp;&nbsp;0.0673&nbsp;0.0831&nbsp;0.102&nbsp;&nbsp;0.124&nbsp;&nbsp;0.151&nbsp;0.184&nbsp;0.226&nbsp;0.285&nbsp;0.38&nbsp;0.563 210.278&nbsp;0.104&nbsp;4.690.0164&nbsp;1.64e-050.0004380.00181&nbsp;0.00415&nbsp;0.00752&nbsp;0.012&nbsp;&nbsp;&nbsp;0.0176&nbsp;&nbsp;0.0244&nbsp;&nbsp;0.0326&nbsp;&nbsp;0.0424&nbsp;&nbsp;0.0538&nbsp;&nbsp;0.0673&nbsp;0.0831&nbsp;0.102&nbsp;&nbsp;0.124&nbsp;&nbsp;0.151&nbsp;0.184&nbsp;0.226&nbsp;0.285&nbsp;0.38&nbsp;0.563 210.00010.104&nbsp;4.690.0164&nbsp;1.64e-050.0004380.00181&nbsp;0.00415&nbsp;0.00752&nbsp;0.012&nbsp;&nbsp;&nbsp;0.0176&nbsp;&nbsp;0.0244&nbsp;&nbsp;0.0326&nbsp;&nbsp;0.0424&nbsp;&nbsp;0.0538&nbsp;&nbsp;0.0673&nbsp;0.0831&nbsp;0.102&nbsp;&nbsp;0.124&nbsp;&nbsp;0.151&nbsp;0.184&nbsp;0.226&nbsp;0.285&nbsp;0.38&nbsp;0.563 220.296&nbsp;0.112&nbsp;4.690.0175&nbsp;3.28e-050.0006940.00259&nbsp;0.00562&nbsp;0.00978&nbsp;0.0151&nbsp;&nbsp;0.0216&nbsp;&nbsp;0.0294&nbsp;&nbsp;0.0386&nbsp;&nbsp;0.0494&nbsp;&nbsp;0.0618&nbsp;&nbsp;0.0763&nbsp;0.0931&nbsp;0.113&nbsp;&nbsp;0.136&nbsp;&nbsp;0.164&nbsp;0.198&nbsp;0.241&nbsp;0.301&nbsp;0.3960.577 220.04560.112&nbsp;4.690.0175&nbsp;3.28e-050.0006940.00259&nbsp;0.00562&nbsp;0.00978&nbsp;0.0151&nbsp;&nbsp;0.0216&nbsp;&nbsp;0.0294&nbsp;&nbsp;0.0386&nbsp;&nbsp;0.0494&nbsp;&nbsp;0.0618&nbsp;&nbsp;0.0763&nbsp;0.0931&nbsp;0.113&nbsp;&nbsp;0.136&nbsp;&nbsp;0.164&nbsp;0.198&nbsp;0.241&nbsp;0.301&nbsp;0.3960.577 230.333&nbsp;0.121&nbsp;4.690.0187&nbsp;6.23e-050.00106&nbsp;0.00362&nbsp;0.00746&nbsp;0.0125&nbsp;&nbsp;0.0188&nbsp;&nbsp;0.0263&nbsp;&nbsp;0.0351&nbsp;&nbsp;0.0453&nbsp;&nbsp;0.057&nbsp;&nbsp;&nbsp;0.0705&nbsp;&nbsp;0.086&nbsp;&nbsp;0.104&nbsp;&nbsp;0.124&nbsp;&nbsp;0.149&nbsp;&nbsp;0.177&nbsp;0.212&nbsp;0.256&nbsp;0.317&nbsp;0.4120.591 230.26&nbsp;&nbsp;0.121&nbsp;4.690.0187&nbsp;6.23e-050.00106&nbsp;0.00362&nbsp;0.00746&nbsp;0.0125&nbsp;&nbsp;0.0188&nbsp;&nbsp;0.0263&nbsp;&nbsp;0.0351&nbsp;&nbsp;0.0453&nbsp;&nbsp;0.057&nbsp;&nbsp;&nbsp;0.0705&nbsp;&nbsp;0.086&nbsp;&nbsp;0.104&nbsp;&nbsp;0.124&nbsp;&nbsp;0.149&nbsp;&nbsp;0.177&nbsp;0.212&nbsp;0.256&nbsp;0.317&nbsp;0.4120.591 230.105&nbsp;0.121&nbsp;4.690.0187&nbsp;6.23e-050.00106&nbsp;0.00362&nbsp;0.00746&nbsp;0.0125&nbsp;&nbsp;0.0188&nbsp;&nbsp;0.0263&nbsp;&nbsp;0.0351&nbsp;&nbsp;0.0453&nbsp;&nbsp;0.057&nbsp;&nbsp;&nbsp;0.0705&nbsp;&nbsp;0.086&nbsp;&nbsp;0.104&nbsp;&nbsp;0.124&nbsp;&nbsp;0.149&nbsp;&nbsp;0.177&nbsp;0.212&nbsp;0.256&nbsp;0.317&nbsp;0.4120.591 230.05270.121&nbsp;4.690.0187&nbsp;6.23e-050.00106&nbsp;0.00362&nbsp;0.00746&nbsp;0.0125&nbsp;&nbsp;0.0188&nbsp;&nbsp;0.0263&nbsp;&nbsp;0.0351&nbsp;&nbsp;0.0453&nbsp;&nbsp;0.057&nbsp;&nbsp;&nbsp;0.0705&nbsp;&nbsp;0.086&nbsp;&nbsp;0.104&nbsp;&nbsp;0.124&nbsp;&nbsp;0.149&nbsp;&nbsp;0.177&nbsp;0.212&nbsp;0.256&nbsp;0.317&nbsp;0.4120.591 230.231&nbsp;0.121&nbsp;4.690.0187&nbsp;6.23e-050.00106&nbsp;0.00362&nbsp;0.00746&nbsp;0.0125&nbsp;&nbsp;0.0188&nbsp;&nbsp;0.0263&nbsp;&nbsp;0.0351&nbsp;&nbsp;0.0453&nbsp;&nbsp;0.057&nbsp;&nbsp;&nbsp;0.0705&nbsp;&nbsp;0.086&nbsp;&nbsp;0.104&nbsp;&nbsp;0.124&nbsp;&nbsp;0.149&nbsp;&nbsp;0.177&nbsp;0.212&nbsp;0.256&nbsp;0.317&nbsp;0.4120.591 230.05890.121&nbsp;4.690.0187&nbsp;6.23e-050.00106&nbsp;0.00362&nbsp;0.00746&nbsp;0.0125&nbsp;&nbsp;0.0188&nbsp;&nbsp;0.0263&nbsp;&nbsp;0.0351&nbsp;&nbsp;0.0453&nbsp;&nbsp;0.057&nbsp;&nbsp;&nbsp;0.0705&nbsp;&nbsp;0.086&nbsp;&nbsp;0.104&nbsp;&nbsp;0.124&nbsp;&nbsp;0.149&nbsp;&nbsp;0.177&nbsp;0.212&nbsp;0.256&nbsp;0.317&nbsp;0.4120.591 240.25&nbsp;&nbsp;0.13&nbsp;&nbsp;4.690.0199&nbsp;0.0001130.00158&nbsp;0.00496&nbsp;0.00972&nbsp;0.0158&nbsp;&nbsp;0.023&nbsp;&nbsp;&nbsp;0.0316&nbsp;&nbsp;0.0414&nbsp;&nbsp;0.0527&nbsp;&nbsp;0.0655&nbsp;&nbsp;0.08&nbsp;&nbsp;&nbsp;&nbsp;0.0964&nbsp;0.115&nbsp;&nbsp;0.137&nbsp;&nbsp;0.162&nbsp;&nbsp;0.191&nbsp;0.227&nbsp;0.272&nbsp;0.333&nbsp;0.4280.605 240.04560.13&nbsp;&nbsp;4.690.0199&nbsp;0.0001130.00158&nbsp;0.00496&nbsp;0.00972&nbsp;0.0158&nbsp;&nbsp;0.023&nbsp;&nbsp;&nbsp;0.0316&nbsp;&nbsp;0.0414&nbsp;&nbsp;0.0527&nbsp;&nbsp;0.0655&nbsp;&nbsp;0.08&nbsp;&nbsp;&nbsp;&nbsp;0.0964&nbsp;0.115&nbsp;&nbsp;0.137&nbsp;&nbsp;0.162&nbsp;&nbsp;0.191&nbsp;0.227&nbsp;0.272&nbsp;0.333&nbsp;0.4280.605 240.179&nbsp;0.13&nbsp;&nbsp;4.690.0199&nbsp;0.0001130.00158&nbsp;0.00496&nbsp;0.00972&nbsp;0.0158&nbsp;&nbsp;0.023&nbsp;&nbsp;&nbsp;0.0316&nbsp;&nbsp;0.0414&nbsp;&nbsp;0.0527&nbsp;&nbsp;0.0655&nbsp;&nbsp;0.08&nbsp;&nbsp;&nbsp;&nbsp;0.0964&nbsp;0.115&nbsp;&nbsp;0.137&nbsp;&nbsp;0.162&nbsp;&nbsp;0.191&nbsp;0.227&nbsp;0.272&nbsp;0.333&nbsp;0.4280.605 240.00010.13&nbsp;&nbsp;4.690.0199&nbsp;0.0001130.00158&nbsp;0.00496&nbsp;0.00972&nbsp;0.0158&nbsp;&nbsp;0.023&nbsp;&nbsp;&nbsp;0.0316&nbsp;&nbsp;0.0414&nbsp;&nbsp;0.0527&nbsp;&nbsp;0.0655&nbsp;&nbsp;0.08&nbsp;&nbsp;&nbsp;&nbsp;0.0964&nbsp;0.115&nbsp;&nbsp;0.137&nbsp;&nbsp;0.162&nbsp;&nbsp;0.191&nbsp;0.227&nbsp;0.272&nbsp;0.333&nbsp;0.4280.605 240.06260.13&nbsp;&nbsp;4.690.0199&nbsp;0.0001130.00158&nbsp;0.00496&nbsp;0.00972&nbsp;0.0158&nbsp;&nbsp;0.023&nbsp;&nbsp;&nbsp;0.0316&nbsp;&nbsp;0.0414&nbsp;&nbsp;0.0527&nbsp;&nbsp;0.0655&nbsp;&nbsp;0.08&nbsp;&nbsp;&nbsp;&nbsp;0.0964&nbsp;0.115&nbsp;&nbsp;0.137&nbsp;&nbsp;0.162&nbsp;&nbsp;0.191&nbsp;0.227&nbsp;0.272&nbsp;0.333&nbsp;0.4280.605 250.25&nbsp;&nbsp;0.14&nbsp;&nbsp;4.690.0211&nbsp;0.0001950.00229&nbsp;0.00664&nbsp;0.0125&nbsp;&nbsp;0.0196&nbsp;&nbsp;0.0279&nbsp;&nbsp;0.0376&nbsp;&nbsp;0.0485&nbsp;&nbsp;0.0608&nbsp;&nbsp;0.0746&nbsp;&nbsp;0.0902&nbsp;&nbsp;0.108&nbsp;&nbsp;0.127&nbsp;&nbsp;0.15&nbsp;&nbsp;&nbsp;0.176&nbsp;&nbsp;0.206&nbsp;0.243&nbsp;0.289&nbsp;0.35&nbsp;&nbsp;0.4450.62&nbsp; 260.05010.15&nbsp;&nbsp;4.690.0224&nbsp;0.0003250.00322&nbsp;0.00872&nbsp;0.0157&nbsp;&nbsp;0.024&nbsp;&nbsp;&nbsp;0.0335&nbsp;&nbsp;0.0443&nbsp;&nbsp;0.0563&nbsp;&nbsp;0.0697&nbsp;&nbsp;0.0846&nbsp;&nbsp;0.101&nbsp;&nbsp;&nbsp;0.12&nbsp;&nbsp;&nbsp;0.14&nbsp;&nbsp;&nbsp;0.164&nbsp;&nbsp;0.191&nbsp;&nbsp;0.222&nbsp;0.259&nbsp;0.306&nbsp;0.367&nbsp;0.4620.634 260.31&nbsp;&nbsp;0.15&nbsp;&nbsp;4.690.0224&nbsp;0.0003250.00322&nbsp;0.00872&nbsp;0.0157&nbsp;&nbsp;0.024&nbsp;&nbsp;&nbsp;0.0335&nbsp;&nbsp;0.0443&nbsp;&nbsp;0.0563&nbsp;&nbsp;0.0697&nbsp;&nbsp;0.0846&nbsp;&nbsp;0.101&nbsp;&nbsp;&nbsp;0.12&nbsp;&nbsp;&nbsp;0.14&nbsp;&nbsp;&nbsp;0.164&nbsp;&nbsp;0.191&nbsp;&nbsp;0.222&nbsp;0.259&nbsp;0.306&nbsp;0.367&nbsp;0.4620.634 260.107&nbsp;0.15&nbsp;&nbsp;4.690.0224&nbsp;0.0003250.00322&nbsp;0.00872&nbsp;0.0157&nbsp;&nbsp;0.024&nbsp;&nbsp;&nbsp;0.0335&nbsp;&nbsp;0.0443&nbsp;&nbsp;0.0563&nbsp;&nbsp;0.0697&nbsp;&nbsp;0.0846&nbsp;&nbsp;0.101&nbsp;&nbsp;&nbsp;0.12&nbsp;&nbsp;&nbsp;0.14&nbsp;&nbsp;&nbsp;0.164&nbsp;&nbsp;0.191&nbsp;&nbsp;0.222&nbsp;0.259&nbsp;0.306&nbsp;0.367&nbsp;0.4620.634 280.06910.172&nbsp;4.690.025&nbsp;&nbsp;0.00081&nbsp;0.00599&nbsp;0.0143&nbsp;&nbsp;0.024&nbsp;&nbsp;&nbsp;0.0349&nbsp;&nbsp;0.0469&nbsp;&nbsp;0.06&nbsp;&nbsp;&nbsp;&nbsp;0.0743&nbsp;&nbsp;0.0899&nbsp;&nbsp;0.107&nbsp;&nbsp;&nbsp;0.125&nbsp;&nbsp;&nbsp;0.146&nbsp;&nbsp;0.168&nbsp;&nbsp;0.194&nbsp;&nbsp;0.222&nbsp;&nbsp;0.255&nbsp;0.294&nbsp;0.341&nbsp;0.403&nbsp;0.4970.664 280.22&nbsp;&nbsp;0.172&nbsp;4.690.025&nbsp;&nbsp;0.00081&nbsp;0.00599&nbsp;0.0143&nbsp;&nbsp;0.024&nbsp;&nbsp;&nbsp;0.0349&nbsp;&nbsp;0.0469&nbsp;&nbsp;0.06&nbsp;&nbsp;&nbsp;&nbsp;0.0743&nbsp;&nbsp;0.0899&nbsp;&nbsp;0.107&nbsp;&nbsp;&nbsp;0.125&nbsp;&nbsp;&nbsp;0.146&nbsp;&nbsp;0.168&nbsp;&nbsp;0.194&nbsp;&nbsp;0.222&nbsp;&nbsp;0.255&nbsp;0.294&nbsp;0.341&nbsp;0.403&nbsp;0.4970.664 280.22&nbsp;&nbsp;0.172&nbsp;4.690.025&nbsp;&nbsp;0.00081&nbsp;0.00599&nbsp;0.0143&nbsp;&nbsp;0.024&nbsp;&nbsp;&nbsp;0.0349&nbsp;&nbsp;0.0469&nbsp;&nbsp;0.06&nbsp;&nbsp;&nbsp;&nbsp;0.0743&nbsp;&nbsp;0.0899&nbsp;&nbsp;0.107&nbsp;&nbsp;&nbsp;0.125&nbsp;&nbsp;&nbsp;0.146&nbsp;&nbsp;0.168&nbsp;&nbsp;0.194&nbsp;&nbsp;0.222&nbsp;&nbsp;0.255&nbsp;0.294&nbsp;0.341&nbsp;0.403&nbsp;0.4970.664 280.235&nbsp;0.172&nbsp;4.690.025&nbsp;&nbsp;0.00081&nbsp;0.00599&nbsp;0.0143&nbsp;&nbsp;0.024&nbsp;&nbsp;&nbsp;0.0349&nbsp;&nbsp;0.0469&nbsp;&nbsp;0.06&nbsp;&nbsp;&nbsp;&nbsp;0.0743&nbsp;&nbsp;0.0899&nbsp;&nbsp;0.107&nbsp;&nbsp;&nbsp;0.125&nbsp;&nbsp;&nbsp;0.146&nbsp;&nbsp;0.168&nbsp;&nbsp;0.194&nbsp;&nbsp;0.222&nbsp;&nbsp;0.255&nbsp;0.294&nbsp;0.341&nbsp;0.403&nbsp;0.4970.664 300.1&nbsp;&nbsp;&nbsp;0.197&nbsp;4.690.0278&nbsp;0.00178&nbsp;0.0103&nbsp;&nbsp;0.0222&nbsp;&nbsp;0.035&nbsp;&nbsp;&nbsp;0.0488&nbsp;&nbsp;0.0634&nbsp;&nbsp;0.079&nbsp;&nbsp;&nbsp;0.0956&nbsp;&nbsp;0.113&nbsp;&nbsp;&nbsp;0.132&nbsp;&nbsp;&nbsp;0.153&nbsp;&nbsp;&nbsp;0.175&nbsp;&nbsp;0.2&nbsp;&nbsp;&nbsp;&nbsp;0.226&nbsp;&nbsp;0.256&nbsp;&nbsp;0.29&nbsp;&nbsp;0.33&nbsp;&nbsp;0.378&nbsp;0.441&nbsp;0.5330.694 300.154&nbsp;0.197&nbsp;4.690.0278&nbsp;0.00178&nbsp;0.0103&nbsp;&nbsp;0.0222&nbsp;&nbsp;0.035&nbsp;&nbsp;&nbsp;0.0488&nbsp;&nbsp;0.0634&nbsp;&nbsp;0.079&nbsp;&nbsp;&nbsp;0.0956&nbsp;&nbsp;0.113&nbsp;&nbsp;&nbsp;0.132&nbsp;&nbsp;&nbsp;0.153&nbsp;&nbsp;&nbsp;0.175&nbsp;&nbsp;0.2&nbsp;&nbsp;&nbsp;&nbsp;0.226&nbsp;&nbsp;0.256&nbsp;&nbsp;0.29&nbsp;&nbsp;0.33&nbsp;&nbsp;0.378&nbsp;0.441&nbsp;0.5330.694 320.111&nbsp;0.224&nbsp;4.690.0306&nbsp;0.00353&nbsp;0.0166&nbsp;&nbsp;0.0327&nbsp;&nbsp;0.049&nbsp;&nbsp;&nbsp;0.0659&nbsp;&nbsp;0.0833&nbsp;&nbsp;0.101&nbsp;&nbsp;&nbsp;0.12&nbsp;&nbsp;&nbsp;&nbsp;0.14&nbsp;&nbsp;&nbsp;&nbsp;0.161&nbsp;&nbsp;&nbsp;0.184&nbsp;&nbsp;&nbsp;0.208&nbsp;&nbsp;0.234&nbsp;&nbsp;0.262&nbsp;&nbsp;0.293&nbsp;&nbsp;0.328&nbsp;0.369&nbsp;0.418&nbsp;0.48&nbsp;&nbsp;0.5710.725 320.08580.224&nbsp;4.690.0306&nbsp;0.00353&nbsp;0.0166&nbsp;&nbsp;0.0327&nbsp;&nbsp;0.049&nbsp;&nbsp;&nbsp;0.0659&nbsp;&nbsp;0.0833&nbsp;&nbsp;0.101&nbsp;&nbsp;&nbsp;0.12&nbsp;&nbsp;&nbsp;&nbsp;0.14&nbsp;&nbsp;&nbsp;&nbsp;0.161&nbsp;&nbsp;&nbsp;0.184&nbsp;&nbsp;&nbsp;0.208&nbsp;&nbsp;0.234&nbsp;&nbsp;0.262&nbsp;&nbsp;0.293&nbsp;&nbsp;0.328&nbsp;0.369&nbsp;0.418&nbsp;0.48&nbsp;&nbsp;0.5710.725 330.154&nbsp;0.239&nbsp;4.690.0319&nbsp;0.00481&nbsp;0.0206&nbsp;&nbsp;0.039&nbsp;&nbsp;&nbsp;0.0573&nbsp;&nbsp;0.0757&nbsp;&nbsp;0.0946&nbsp;&nbsp;0.114&nbsp;&nbsp;&nbsp;0.134&nbsp;&nbsp;&nbsp;0.155&nbsp;&nbsp;&nbsp;0.177&nbsp;&nbsp;&nbsp;0.2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.225&nbsp;&nbsp;0.252&nbsp;&nbsp;0.281&nbsp;&nbsp;0.313&nbsp;&nbsp;0.348&nbsp;0.389&nbsp;0.438&nbsp;0.5&nbsp;&nbsp;&nbsp;0.5890.74&nbsp; 350.25&nbsp;&nbsp;0.27&nbsp;&nbsp;4.690.0346&nbsp;0.0084&nbsp;&nbsp;0.0306&nbsp;&nbsp;0.0541&nbsp;&nbsp;0.0763&nbsp;&nbsp;0.0981&nbsp;&nbsp;0.12&nbsp;&nbsp;&nbsp;&nbsp;0.142&nbsp;&nbsp;&nbsp;0.164&nbsp;&nbsp;&nbsp;0.187&nbsp;&nbsp;&nbsp;0.211&nbsp;&nbsp;&nbsp;0.236&nbsp;&nbsp;&nbsp;0.262&nbsp;&nbsp;0.29&nbsp;&nbsp;&nbsp;0.32&nbsp;&nbsp;&nbsp;0.353&nbsp;&nbsp;0.39&nbsp;&nbsp;0.431&nbsp;0.479&nbsp;0.54&nbsp;&nbsp;0.6280.77&nbsp; 360.1&nbsp;&nbsp;&nbsp;0.287&nbsp;4.690.0359&nbsp;0.0108&nbsp;&nbsp;0.0366&nbsp;&nbsp;0.0629&nbsp;&nbsp;0.0872&nbsp;&nbsp;0.111&nbsp;&nbsp;&nbsp;0.134&nbsp;&nbsp;&nbsp;0.157&nbsp;&nbsp;&nbsp;0.18&nbsp;&nbsp;&nbsp;&nbsp;0.204&nbsp;&nbsp;&nbsp;0.229&nbsp;&nbsp;&nbsp;0.255&nbsp;&nbsp;&nbsp;0.282&nbsp;&nbsp;0.311&nbsp;&nbsp;0.341&nbsp;&nbsp;0.374&nbsp;&nbsp;0.411&nbsp;0.452&nbsp;0.501&nbsp;0.561&nbsp;0.6470.785 When constructing the figure for beta regression, one of the main advantages of using this approach is that the quartiles are calculated with a beta distribution. Therefore, the margin of error does NOT fall below 0 and does NOT exceed 1. Evaluate the following figure at each through regression. library(ggplot2) ggplot(dp2, aes(x=Number_of_Flowers, y=PropFR))+ geom_point()+ geom_line(aes(y=quantile_.05), linetype=&quot;twodash&quot;, colour=&quot;blue&quot;)+ geom_line(aes(y=quantile_.25),linetype=2, colour=&quot;green&quot;)+ geom_line(aes(y=quantile_.50), colour=&quot;red&quot;)+ geom_line(aes(y=quantile_.75), linetype=2, colour=&quot;green&quot;)+ geom_line(aes(y=quantile_.95), linetype=&quot;twodash&quot;, colour=&quot;blue&quot;)+ ylab(&quot;Predicción de la proporción de frutos&quot;)+ xlab(&quot;Números de Flores&quot;)+ annotate(&quot;text&quot;, x=25, y=0.50, label=&quot;95th quartile&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=32, y=0.39, label=&quot;75th quartile&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=33, y=0.14, label=&quot;25th quartile&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=33, y=0.27, label=&quot;Median&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=35, y=-0.02, label=&quot;5th quartile&quot;, fontface=&quot;italic&quot;)+ theme(axis.title.y = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;), axis.text.x = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;), axis.text.y = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;), axis.title.x = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;))+ theme(legend.position=&quot;none&quot;)+ rlt_theme ggsave(&quot;Figures/Beta_number_flowers.png&quot;) ## Saving 7 x 5 in image This is a representation of the beta distributions in the x, number of flowers. In red we simulate the distribution of the expected proportion of fruits in plants that have 15 flowers and in the blue line we simulate the expected distribution of the proportion of fruits in plants with 30 flowers. 18.11 Compare the traditional linear model versus a beta model To compare the effectiveness of the models we use the Akaike Information Criterion (AIC). In a model selection approach, the smallest AIC index, which represents the most parsimonious model, is accepted as the best model. An AIC difference of 4 is significant. Note that the beta model is much better (AIC = -219) than the linear regression model (AIC = -102) #AIC(modelpropFr) # modelo beta modelpropFr_lm=lm(PropFR~Number_of_Flowers, data=dp2) AIC(modelpropFr,modelpropFr_lm) dfAIC 3-220 3-102 18.12 Distribution of specific values Evaluating the beta distribution for specific values of x = Ratio of fruits per plant based on the number of flowers on the plant. Select the different values of x and calculate the mean and variance and convert these to \\(\\alpha\\) and \\(\\beta\\). With these parameters, the density of the distribution can be constructed for each value of x. Specific values are selected to display the distribution, plants that have 25, 30 and 35 flowers. The data has to be rearranged to calculate the mean and variance. dpQ=dp2 %&gt;% dplyr::select(c(1, 6:26)) dpQ2=dpQ%&gt;% filter(Number_of_Flowers== 35) %&gt;% dplyr::select(c(2:22))%&gt;% t %&gt;% as.data.frame dpQ2$Quartiles=c(.01, 0.05, .1, .15, .2, .25, .30, .35, .4, .45, .5, .55, .6, .65, .7,.75, .8, .85, .9, .95, .99 ) mean(dpQ2$V1, na.rm=FALSE) ## [1] 0.2757771 var(dpQ2$V1, na.rm=FALSE) ## [1] 0.04226223 #dpQ2 ggplot(dpQ2, aes(Quartiles, V1))+ geom_line() Using the variance calculated in the previous chunk, the \\(\\alpha\\) and \\(\\beta\\) can be calculated. estBetaParams &lt;- function(mu, var) { alpha &lt;- ((1 - mu) / var - 1 / mu) * mu ^ 2 beta &lt;- alpha * (1 / mu - 1) return(params = list(alpha = alpha, beta = beta)) } #mean(Smokers$Y2000P) #var(Smokers$Y2000P) estBetaParams(0.2757771,0.04226223) ## $alpha ## [1] 1.027498 ## ## $beta ## [1] 2.698331 Graphics production. It is observed that for plants that have 25 and 30 flowers the density is skewed to the left, in other words the probability of having few fruits dominates the distributions. library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine a=ggplot(dipodium2, aes(PropFR))+ stat_function(aes(x = dipodium2$PropFR, y = ..y..), fun = dbeta, colour=&quot;red&quot;, n = 62, args = list(shape1 =0.5418068, shape2 =3.135593))+ ylab(&quot;Beta \\nDensity&quot;)+ xlab(&quot;Probabilidad de tener frutos&quot;)+ coord_cartesian(xlim=c(0,0.05))+ ggtitle(&quot;Densidad beta para plantas con 25 flores&quot;) b=ggplot(dipodium2, aes(PropFR))+ stat_function(aes(x = dipodium2$PropFR, y = ..y..), fun = dbeta, colour=&quot;blue&quot;, n = 62, args = list(shape1 = 0.7549048, shape2 =2.949404))+ ylab(&quot;Beta \\nDensity&quot;)+ xlab(&quot;Probabilidad de tener frutos&quot;)+ coord_cartesian(xlim=c(0,0.1))+ ggtitle(&quot;Densidad beta para plantas con 30 flores&quot;) c=ggplot(dipodium2, aes(PropFR))+ stat_function(aes(x = dipodium2$PropFR, y = ..y..), fun = dbeta, colour=&quot;black&quot;, n = 62, args = list(shape1 = 1.027498, shape2 =2.698331))+ ylab(&quot;Beta \\nDensity&quot;)+ xlab(&quot;Probabilidad de tener frutos&quot;)+ coord_cartesian(xlim=c(0,0.10))+ ggtitle(&quot;Densidad beta para plantas con 35 flores&quot;) tresDensidad=grid.arrange(a,b,c, ncol=1) tresDensidad ## TableGrob (3 x 1) &quot;arrange&quot;: 3 grobs ## z cells name grob ## 1 1 (1-1,1-1) arrange gtable[layout] ## 2 2 (2-2,1-1) arrange gtable[layout] ## 3 3 (3-3,1-1) arrange gtable[layout] ggsave(&quot;Figures/tresDensidad.png&quot;) ## Saving 7 x 5 in image For an excellent new step by step use of the beta regression and why it is usefull see this website. https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/ “Activities reported in this website was supported by the National Institute of General Medical Sciences of the National Institutes of Health under Award Number R25GM121270. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.” "],["auto-regressive-vectorial-analysis.html", "Chapter 19 Auto Regressive Vectorial Analysis", " Chapter 19 Auto Regressive Vectorial Analysis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
