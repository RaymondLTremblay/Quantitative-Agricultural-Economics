[["index.html", "Quantitative Agricultural Economics Chapter 1 Daily Activities", " Quantitative Agricultural Economics Raymond L. Tremblay 2025-02-13 Chapter 1 Daily Activities Day 1: Introduction to RStudio and available packages •⁠ ⁠Interaction with the Platform (Chapter 2) •⁠ ⁠How to search and download statistical packages? (Chapter3) •⁠ ⁠How to understand the default programming of statistical packages? •⁠ ⁠How to find help? •⁠ ⁠How to upload data, rename variables, delete data, find missing values, view descriptive statistics, view variable types, etc.) •⁠ ⁠Shapito-Wilk test •⁠ ⁠T test •⁠ ⁠Test •⁠ ⁠Kruskal Wallis tests Day 2: Data visualization - graphs •⁠ ⁠Scatter plot (simple and with multiple variables) •⁠ ⁠Scatter plot matrix •⁠ ⁠Linear graph (simple and with two variables) •⁠ ⁠Pie chart •⁠ ⁠Histograms (simple and with multiple variables ) •⁠ ⁠Kernel density •⁠ ⁠Box plot •⁠ ⁠Other gsph Day 3: Econometric models and assumptions •⁠ ⁠Ordinary least squares regression •⁠ ⁠Assumption checking •⁠ ⁠Quantile regression •⁠ ⁠Logit regression and post-estimation tests/checks (% correct predictions, marginal effects, goodness of fit, description of data used, etc.) Day 4: Econometric models and assumptions •⁠ ⁠Multinomial logit regression and post-estimation tests/checks •⁠ ⁠Beta regression •⁠ ⁠VAR models (Vector autoregressive models) "],["introduction-to-r-and-posit-rstudio.html", "Chapter 2 Introduction to R and POSIT (RStudio) 2.1 What is R? 2.2 What is RStudio? 2.3 Installing R and RStudio 2.4 Getting started with RStudio 2.5 What is RMarkdown? 2.6 Creating an RStudio project 2.7 Getting help with R 2.8 Conclusion", " Chapter 2 Introduction to R and POSIT (RStudio) R is a powerful tool for data analysis and visualization. It is a free software environment for statistical computing and graphics. R is widely used in academia and industry for data analysis, statistical modeling, and visualization. In this tutorial, we will introduce you to R and RStudio, a popular integrated development environment (IDE) for R. POSIT is the new name for RStudio. The objective of POSIT is to provide a user-friendly interface for working with R, including tools for writing and running R code, managing R packages, and visualizing data for multiple coding languages. Posit is available in both free and commercial versions, and it is widely used by data scientists, statisticians, and researchers for data analysis and visualization. Posit can use not only R but also Python, SQL, Bash, Julia, Stan, Rcpp, GraphViz and Mermaid. Other languages will be added in the future. Consequently, POSIT is a powerful tool for data analysis and visualization with the objective of having a an IDE which can be used for multiple coding languages, even in the same environment. 2.1 What is R? R is a programming language and software environment for statistical computing and graphics. It is an open-source project that is maintained by the R Foundation. R is widely used in academia and industry for data analysis, statistical modeling, and visualization. R has a large and active community of users and developers who contribute to the development of R packages, which are collections of functions and data sets that extend the capabilities of R. Presently R is the preferred language for statisticians and data scientists in part because of the large library of packages available in github and other repositories. 2.2 What is RStudio? RStudio is an integrated development environment (IDE) for R. It provides a user-friendly interface for working with R, including tools for writing and running R code, managing R packages, and visualizing data. RStudio is available in both free and commercial versions, and it is widely used by data scientists, statisticians, and researchers for data analysis and visualization. 2.3 Installing R and RStudio To get started with R and RStudio, you will need to install both software packages on your computer. Here are the steps to install R and RStudio: Download and install R from the Comprehensive R Archive Network (CRAN) website: https://cran.r-project.org/. Download and install RStudio from the RStudio website: https://www.rstudio.com/products/rstudio/download/. Once you have installed R and RStudio, you can open RStudio and start working with R, Python or other languages. 2.4 Getting started with RStudio When you open RStudio, you will see four panes: The Console pane, where you can type and run R code. The Environment pane, which shows information about the objects in your R workspace. The Files pane, where you can navigate your file system and open files. The Plots pane, where plots and visualizations will appear. You can start typing R code in the Console pane and press Enter to run the code. For example, you can type 2 + 2 and press Enter to see the result. Alternatively, you can install RMarkdown library. 2.5 What is RMarkdown? RMarkdown is a file format that allows you to create dynamic documents that combine R code with text, images, and other content. RMarkdown files are written in Markdown, a lightweight markup language, and can be rendered to various output formats, including HTML, PDF, and Word documents. RMarkdown is widely used for creating reports, presentations, and interactive documents that include data analysis and visualizations. To create a new RMarkdown file in RStudio, follow these steps: Click on the File menu and select New File. Click on R Markdown… to create a new RMarkdown file. The first time you do this you will need to install the package rmarkdown. Choose a template for your RMarkdown file, such as HTML, PDF, or Word. Click OK to create the new RMarkdown file. You can start typing R code and text in the RMarkdown file, and use the Knit button to render the document to the selected output format. 2.6 Creating an RStudio project To organize your R code and data files, you can create an RStudio project. An RStudio project is a directory that contains your R code, data files, and other project-related files. To create an RStudio project, follow these steps: Click on the File menu and select New Project. Choose a directory for your project and click Create Project. RStudio will create a new project in the selected directory and open a new RStudio session with the project loaded. You can save your R code, data files, and other project-related files in the project directory, and use the project to organize and manage your work. 2.7 Getting help with R If you need help with R, there are many resources available to you. Here are a few ways to get help with R: The R help system: You can access the R help system by typing help() or ? followed by the name of a function or package. For example, you can type help(plot) or ?plot to get help on the plot function. The RStudio help pane: In RStudio, you can use the help pane to search for help on R functions and packages. You can open the help pane by clicking on the Help tab in the bottom right corner of the RStudio window. Online resources: There are many online resources available for learning R, including tutorials, books, and forums. Some popular resources include RStudio Community, Stack Overflow, and DataCamp. R packages: R packages are collections of functions and data sets that extend the capabilities of R. You can search for R packages on the CRAN website or on [GitHub]( 2.8 Conclusion In this tutorial, we introduced you to R and RStudio, two powerful tools for data analysis and visualization. We showed you how to install R and RStudio, and how to get started with RStudio. We also introduced you to RMarkdown, a file format for creating dynamic documents that combine R code with text and other content. We hope this tutorial has inspired you to explore R and RStudio further and use them in your data analysis projects. Happy coding! 2.8.0.0.1 The above text was partially copied from the following source: https://www.datacamp.com/community/tutorials/r-tutorial-learn-r and suggested from the following AI using Copilot. "],["packages-and-functions-in-r.html", "Chapter 3 Packages and Functions in R 3.1 What is an R package? 3.2 Understanding packages in R 3.3 Installing and loading packages 3.4 Surfing the web for packages and functions 3.5 Installing packages from github 3.6 Installing packages from Bioconductor", " Chapter 3 Packages and Functions in R 3.1 What is an R package? An R package is a collection of functions, data, and compiled code in a well-defined format. The directory where packages are stored is called the library. R comes with a standard set of packages. Others are available for download and installation. Once installed, they have to be loaded into the session to be used using the function library(name of package). Packages are created by the R community and can be found in the Comprehensive R Archive Network (CRAN) or in other repositories, such as Github and Bioconductor. All R packages are free and open source. 3.2 Understanding packages in R There are two basic types of R packages, the simpliest one are does that have collections of data sets such as ggversa and datasets. The other type of packages which are the most common are collections of functions such as dplyr and ggplot2. dplyr : A fast, consistent tool for working with data frame like objects, both in memory and out of memory. ggplot2 : A system for ‘declaratively’ creating graphics, based on “The Grammar of Graphics”. -gt : An R package “that makes it easy to extract, combine and arrange data to prepare it for analysis.”Grammar of Tables” to create nice tables normtest : A collection of normality tests for univariate and multivariate data. tidyverse : this a special type of package which installs a collection of packages that work together to make data science faster, easier, and more fun! There are &gt; 21 000 packages on CRAN as of January 2025, and many more on GitHub and Bioconductor. If someone wrote a statistical test, a data manipulation function, or a visualization tool, there is a good chance that it is already available in an R package. 3.3 Installing and loading packages In the console: To install a package, you can use the install.packages() function. For example, to install the dplyr package, you would run: or use the following code: install.packages(&quot;dplyr&quot;, dependencies = TRUE) # remove the hashtag to install the package, dependencies = TRUE will install all the dependencies of the package In RStudio To download and install the package from Rstudio use the “Packages” tab and click on “Install” and type the name of the package you want to install. You much be connected to the internet to install the packages. 3.4 Surfing the web for packages and functions The best way to find packages is to use the CRAN Task Views. These are curated lists of packages grouped by topic. You can find them at https://cran.r-project.org/web/views/ Another alternative is in your browser, you can use the search engine of your choice and type “R package for [insert what you want to do]”. For example, if you want to do a quantile regression, you can type “R package for quantile regression” and you will find the package quantreg. and subsequently intall it from the CRAN repository. 3.5 Installing packages from github To install a package from github you can use the devtools package. This package is not included in the base R installation, so you will need to install it first. You can do this by running the following code: Here is an example of to install the package “raretrans” which is NOT available on CRAN but on github. this package has functions to calculate rare events from a multinomial distribution using the dirichlet function. This was developped as a tool to maximize population dynamics studies of rare species. The site of the package is: https://github.com/atyre2/raretrans #install.packages(&quot;devtools&quot;) library(devtools) ## Loading required package: usethis #devtools::install_github(&quot;atyre2/raretrans&quot;) # remove the hashtag to install the package 3.6 Installing packages from Bioconductor To install from bioconductor you can use the following code: Here is a package from Martin Morgan called “DirichletMultinomial” which is available on Bioconductor. This package has funciton to fit Dirichlet-multinomial distribution to metagenomic data. The site for the package is: https://www.bioconductor.org/packages/release/bioc/html/DirichletMultinomial.html if (!require(&quot;BiocManager&quot;, quietly = TRUE)) install.packages(&quot;BiocManager&quot;) ## Bioconductor version &#39;3.19&#39; is out-of-date; the current release version &#39;3.20&#39; ## is available with R version &#39;4.4&#39;; see https://bioconductor.org/install ## ## Attaching package: &#39;BiocManager&#39; ## The following object is masked from &#39;package:devtools&#39;: ## ## install # BiocManager::install(&quot;DirichletMultinomial&quot;) # remove the hashtag to install the package "],["data-upload.html", "Chapter 4 Data Upload 4.1 Adding data directly to the document 4.2 Loading data from a file 4.3 Loading data from the web", " Chapter 4 Data Upload In this chapter we will learn how to load data into R from the web, and how to add data directly to the document. We will also learn how to load data from a file, such as a .csv, .txt, .excel, or .spss file. 4.1 Adding data directly to the document If you have a small dataset, you can add it directly to the document. This is useful for reproducibility, as it allows you to share the data along with the code. The easiest method is to use the data.frame or tibble function. For example, to create a data frame with the columns name, age, and height, you would run: 4.1.1 Using the function tibble in the package tibble characters are in quotes numbers are not in quotes the columns are separated by commas library(tibble) My_data_frame &lt;- tibble( name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;), age = c(25, 30, 35), height = c(1.70, 1.80, 1.90) ) My_data_frame ## # A tibble: 3 × 3 ## name age height ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alice 25 1.7 ## 2 Bob 30 1.8 ## 3 Charlie 35 1.9 Another alternatives is using tribble which is a special case of tibble that makes it a bit easier to create small data frames by hand. The columns are specified by formulas, and rows by concatenation: note in this case the first row are the names of the columns the columns are separated by commas the rows are separated by commas characters are in quotes numbers are not in quotes library(tibble) My_data_frame1=tribble(~name, ~age, ~height, &quot;Alice&quot;, 25, 1.70, &quot;Bob&quot;, 30, 1.80, &quot;Charlie&quot;, 35, 1.90) 4.2 Loading data from a file If the data is static, in other words it does not change, you can load it from a file and download this to your computer. There are many file formats that R can read, including .csv, .excel, .txt, and .spss files. To load a file, you can use the read.csv, read_excel, read.table, or read.spss functions, respectively. For example, to load a .csv file, you would run. Downloading to your computer will work if the file is not enormous. However, if it has less than 10^6 lines it should be okay. If the file is larger than that, you may need to use a different method to load the data. Note: the file name is in quotes you can set the header to TRUE or FALSE (depending if the file has a header or not) you can set the column names using the colnames function 4.2.1 .csv file uplaod Here is a file for the population size of the Dominican Republic from 1960 to 2020. The file is in the Data folder and is called DominicanRepublic.csv. The file has the following columns: year, population, and value= population size. To load the file, you would run: note the .csv file is in the Data folder on my computer in the RStudio project the package required is library(readr) the file is read using the function read_csv library(readr) DominicanRepublic &lt;- read_csv(&quot;Data/DominicanRepublic.csv&quot;) ## Rows: 5022 Columns: 10 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (6): Country or Area, Area, Sex, Age, Record Type, Reliability ## dbl (3): Year, Source Year, Value ## lgl (1): Value Footnotes ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(DominicanRepublic) ## # A tibble: 6 × 10 ## `Country or Area` Year Area Sex Age `Record Type` Reliability ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Dominican Republic 1950 Total Both Sexes 0 Census - de jure … Final figu… ## 2 Dominican Republic 1950 Urban Both Sexes 0 Census - de jure … Final figu… ## 3 Dominican Republic 1950 Rural Both Sexes 0 Census - de jure … Final figu… ## 4 Dominican Republic 1950 Total Both Sexes 1 Census - de jure … Final figu… ## 5 Dominican Republic 1950 Total Both Sexes 2 Census - de jure … Final figu… ## 6 Dominican Republic 1950 Total Both Sexes 3 Census - de jure … Final figu… ## # ℹ 3 more variables: `Source Year` &lt;dbl&gt;, Value &lt;dbl&gt;, `Value Footnotes` &lt;lgl&gt; 4.2.2 Excel file upload To read a Excel file you will need the package library(readxl) the file is read using the function read_excel the sheet is specified using the argument sheet, You need to specify which tab to read. The first tab is 1, the second tab is 2, etc or the names of the tab. library(readxl) melocactus &lt;- read_excel(&quot;Data/melocactus.xlsx&quot;, sheet = &quot;datos&quot;) head(melocactus) # data from the melocactus in Puerto Rico, note the data is in Spanish and that the sheet called &quot;datos&quot; is read, this is the 3rd sheet in the excel file. ## # A tibble: 6 × 5 ## azimuto distancia alturatotal longinflo estado ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0 12.0 20 5 S ## 2 0 11.6 47 10 S ## 3 0 17.8 27 16 X ## 4 2 2.75 48 29 S ## 5 3 2.71 23 0 S ## 6 3 2.71 16 0 S 4.2.3 .txt file To read data that are tab delinited you need to use the function read_table. The file is read using the function read_table. the package required is library(data.table) the file is read using the function read_table the file is in the same folder as the Rmd file (IMPORTANT: not in the DATA folder) library(data.table) Titanic=read_table(list.files(&quot;.&quot;, &quot;Titanic.txt&quot;)[1]) # Note that .txt files CANNOT be read from within a folder (thus you cannot have it in the DATA folder) .txt file is a really old format and it is not recommended to use it, you should convert it to a .csv file. ## ## ── Column specification ─────────────────────────────────────────────────────── ## cols( ## PassengerId = col_double(), ## Survived = col_double(), ## Pclass = col_double(), ## Name = col_character(), ## Sex = col_character(), ## Age = col_character(), ## SibSp = col_character(), ## Parch = col_character(), ## Ticket = col_character(), ## Fare = col_character(), ## Cabin = col_character(), ## Embarked = col_character() ## ) ## Warning: 1197 parsing failures. ## row col expected actual file ## 1 -- 12 columns 15 columns &#39; Titanic.txt&#39; ## 2 -- 12 columns 19 columns &#39; Titanic.txt&#39; ## 3 -- 12 columns 14 columns &#39; Titanic.txt&#39; ## 4 -- 12 columns 18 columns &#39; Titanic.txt&#39; ## 5 -- 12 columns 14 columns &#39; Titanic.txt&#39; ## ... ... .......... .......... .............. ## See problems(...) for more details. head(Titanic) ## # A tibble: 6 × 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 0 3 &quot;\\&quot;Bra… Mr. &quot;Owe… &quot;Har… male 22 &quot;1&quot; 0 ## 2 2 1 1 &quot;\\&quot;Cum… Mrs. &quot;Joh… &quot;Bra… (Flo… Briggs &quot;Tha… fema… ## 3 3 1 3 &quot;\\&quot;Hei… Miss. &quot;Lai… &quot;fem… 26 0 &quot;0&quot; STON… ## 4 4 1 1 &quot;\\&quot;Fut… Mrs. &quot;Jac… &quot;Hea… (Lily May &quot;Pee… fema… ## 5 5 0 3 &quot;\\&quot;All… Mr. &quot;Wil… &quot;Hen… male 35 &quot;0&quot; 0 ## 6 6 0 3 &quot;\\&quot;Mor… Mr. &quot;Jam… &quot;mal… 0 0 &quot;330… 8.45… ## # ℹ 1 more variable: Embarked &lt;chr&gt; list.files() # this will list the files in the folder ## [1] &quot; Titanic.txt&quot; ## [2] &quot;_bookdown_files&quot; ## [3] &quot;_bookdown.yml&quot; ## [4] &quot;_output.yml&quot; ## [5] &quot;02-Intro_R_RStudio.Rmd&quot; ## [6] &quot;03-Packages_Functions.Rmd&quot; ## [7] &quot;04-Data.Rmd&quot; ## [8] &quot;05-Data_Wrangling.Rmd&quot; ## [9] &quot;06-Undestanding_data.Rmd&quot; ## [10] &quot;07_Normality_Equality_var_tests.Rmd&quot; ## [11] &quot;08-Non-Parametric_tests.Rmd&quot; ## [12] &quot;09_Scatterplots.Rmd&quot; ## [13] &quot;10-Ring_chart.Rmd&quot; ## [14] &quot;11-Histograms_Density.Rmd&quot; ## [15] &quot;12-Boxplots.Rmd&quot; ## [16] &quot;13-Other_Visualization.Rmd&quot; ## [17] &quot;14-Linear_Regression.Rmd&quot; ## [18] &quot;15-Quantile_Regression.Rmd&quot; ## [19] &quot;16_GLM.Rmd&quot; ## [20] &quot;17-Multinomial_logit_analysis.Rmd&quot; ## [21] &quot;18-Beta_Regression.Rmd&quot; ## [22] &quot;19-Auto_Regressive_Vectorial_Analysis.Rmd&quot; ## [23] &quot;bookdownproj_files&quot; ## [24] &quot;bookdownproj.Rmd&quot; ## [25] &quot;Data&quot; ## [26] &quot;dipodium2.csv&quot; ## [27] &quot;docs&quot; ## [28] &quot;Figures&quot; ## [29] &quot;histogram_festival_hygiene (Raymonds-MacBook-Pro.local&#39;s conflicted copy 2025-02-04).pdf&quot; ## [30] &quot;histogram_festival_hygiene.pdf&quot; ## [31] &quot;index.Rmd&quot; ## [32] &quot;LICENSE&quot; ## [33] &quot;qqplot.png&quot; ## [34] &quot;Quantitative_Agricultural_Economics.Rproj&quot; ## [35] &quot;README.md&quot; ## [36] &quot;style.css&quot; ## [37] &quot;tutorial.css&quot; 4.2.4 .spss file To loas an SPSS file you will need the install and load package haven. The function to read the file is read_sav. the package required is library(haven) the file is read using the function read_sav library(haven) library(haven) Lelto_George &lt;- read_sav(&quot;Data/Lelto-George .sav&quot;) head(Lelto_George) ## # A tibble: 6 × 94 ## POPNUM OCCUPIED REMOVED IND.NUM1 NUM.LEA1 NUM.INF1 NUM.FL1 NUM.FR1 NUM.PR1 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 209 Ocupado No removido 67 2 0 0 0 0 ## 2 209 Ocupado No removido 68 3 1 1 0 0 ## 3 209 Ocupado No removido 69 2 1 0 0 0 ## 4 209 Ocupado No removido 70 2 0 0 0 0 ## 5 209 Ocupado No removido 71 5 0 0 0 0 ## 6 209 Ocupado No removido 72 2 1 1 0 0 ## # ℹ 85 more variables: STAGE1 &lt;chr+lbl&gt;, POPNUM2 &lt;dbl&gt;, IND.NUM2 &lt;dbl&gt;, ## # NUM.LEA2 &lt;dbl&gt;, NUM.INF2 &lt;dbl&gt;, NUM.FL2 &lt;dbl&gt;, NUM.FR2 &lt;dbl&gt;, ## # NUM.PR2 &lt;dbl&gt;, STAGE2 &lt;chr+lbl&gt;, POPNUM3 &lt;dbl&gt;, IND.NUM3 &lt;dbl&gt;, ## # NUM.LEA3 &lt;dbl&gt;, NUM.INF3 &lt;dbl&gt;, NUM.FL3 &lt;dbl&gt;, NUM.FR3 &lt;dbl&gt;, ## # NUM.PR3 &lt;dbl&gt;, STAGE3 &lt;chr&gt;, POPNUM4 &lt;dbl&gt;, IND.NUM4 &lt;dbl&gt;, NUM.LEA4 &lt;dbl&gt;, ## # NUM.INF4 &lt;dbl&gt;, NUM.FL4 &lt;dbl&gt;, NUM.FR4 &lt;dbl&gt;, NUM.PR4 &lt;dbl&gt;, STAGE4 &lt;chr&gt;, ## # NUM.LEA5 &lt;dbl&gt;, NUM.INF5 &lt;dbl&gt;, NUM.FL5 &lt;dbl&gt;, NUM.FR5 &lt;dbl&gt;, … 4.2.5 read a Stata file To read data that are saved in Stata software format, you can use the read_dta function from the haven package. For example, to read the auto.dta file, you would run: library(haven) auto &lt;- read_dta(&quot;auto.dta&quot;) # name of file is auto.dta 4.2.6 Read a SAS file _ To read data that are saved in SAS format, read_sas function from the haven package. For example, to read the auto.sas7bdat file, you would run: library(haven) auto &lt;- read_sas(&quot;auto.sas7bdat&quot;) # name of file is auto.sas7bdat In the following file you will find other examples of how to read files in different formats. https://www.geeksforgeeks.org/reading-files-in-r-programming/ 4.3 Loading data from the web To load data from the web, yu will need to have the URL of the data. For example, to load the iris dataset from the UCI Machine Learning Repository, you would run: Note, the html address is in quotes yuo need to add the information in which format the data is (csv, txt, etc) read.csv the header is set to FALSE because the data does not have a header you can set the column names using the colnames function iris &lt;- read.csv(&quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;, header = FALSE) colnames(iris) &lt;- c(&quot;sepal_length&quot;, &quot;sepal_width&quot;, &quot;petal_length&quot;, &quot;petal_width&quot;, &quot;species&quot;) head(iris) ## sepal_length sepal_width petal_length petal_width species ## 1 5.1 3.5 1.4 0.2 Iris-setosa ## 2 4.9 3.0 1.4 0.2 Iris-setosa ## 3 4.7 3.2 1.3 0.2 Iris-setosa ## 4 4.6 3.1 1.5 0.2 Iris-setosa ## 5 5.0 3.6 1.4 0.2 Iris-setosa ## 6 5.4 3.9 1.7 0.4 Iris-setosa An example of dowloading data from the web is the following code with header: !!!! Find a page with data and copy the URL. For example, the following URL contains data on the number of COVID-19 cases in the US: # Load the data from the web data2= read.csv(&quot;https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv&quot;, header = TRUE) names(data2) # names of the columns/variables ## [1] &quot;RT&quot; &quot;SERIALNO&quot; &quot;DIVISION&quot; &quot;PUMA&quot; &quot;REGION&quot; &quot;ST&quot; ## [7] &quot;ADJUST&quot; &quot;WGTP&quot; &quot;NP&quot; &quot;TYPE&quot; &quot;ACR&quot; &quot;AGS&quot; ## [13] &quot;BDS&quot; &quot;BLD&quot; &quot;BUS&quot; &quot;CONP&quot; &quot;ELEP&quot; &quot;FS&quot; ## [19] &quot;FULP&quot; &quot;GASP&quot; &quot;HFL&quot; &quot;INSP&quot; &quot;KIT&quot; &quot;MHP&quot; ## [25] &quot;MRGI&quot; &quot;MRGP&quot; &quot;MRGT&quot; &quot;MRGX&quot; &quot;PLM&quot; &quot;RMS&quot; ## [31] &quot;RNTM&quot; &quot;RNTP&quot; &quot;SMP&quot; &quot;TEL&quot; &quot;TEN&quot; &quot;VACS&quot; ## [37] &quot;VAL&quot; &quot;VEH&quot; &quot;WATP&quot; &quot;YBL&quot; &quot;FES&quot; &quot;FINCP&quot; ## [43] &quot;FPARC&quot; &quot;GRNTP&quot; &quot;GRPIP&quot; &quot;HHL&quot; &quot;HHT&quot; &quot;HINCP&quot; ## [49] &quot;HUGCL&quot; &quot;HUPAC&quot; &quot;HUPAOC&quot; &quot;HUPARC&quot; &quot;LNGI&quot; &quot;MV&quot; ## [55] &quot;NOC&quot; &quot;NPF&quot; &quot;NPP&quot; &quot;NR&quot; &quot;NRC&quot; &quot;OCPIP&quot; ## [61] &quot;PARTNER&quot; &quot;PSF&quot; &quot;R18&quot; &quot;R60&quot; &quot;R65&quot; &quot;RESMODE&quot; ## [67] &quot;SMOCP&quot; &quot;SMX&quot; &quot;SRNT&quot; &quot;SVAL&quot; &quot;TAXP&quot; &quot;WIF&quot; ## [73] &quot;WKEXREL&quot; &quot;WORKSTAT&quot; &quot;FACRP&quot; &quot;FAGSP&quot; &quot;FBDSP&quot; &quot;FBLDP&quot; ## [79] &quot;FBUSP&quot; &quot;FCONP&quot; &quot;FELEP&quot; &quot;FFSP&quot; &quot;FFULP&quot; &quot;FGASP&quot; ## [85] &quot;FHFLP&quot; &quot;FINSP&quot; &quot;FKITP&quot; &quot;FMHP&quot; &quot;FMRGIP&quot; &quot;FMRGP&quot; ## [91] &quot;FMRGTP&quot; &quot;FMRGXP&quot; &quot;FMVYP&quot; &quot;FPLMP&quot; &quot;FRMSP&quot; &quot;FRNTMP&quot; ## [97] &quot;FRNTP&quot; &quot;FSMP&quot; &quot;FSMXHP&quot; &quot;FSMXSP&quot; &quot;FTAXP&quot; &quot;FTELP&quot; ## [103] &quot;FTENP&quot; &quot;FVACSP&quot; &quot;FVALP&quot; &quot;FVEHP&quot; &quot;FWATP&quot; &quot;FYBLP&quot; ## [109] &quot;wgtp1&quot; &quot;wgtp2&quot; &quot;wgtp3&quot; &quot;wgtp4&quot; &quot;wgtp5&quot; &quot;wgtp6&quot; ## [115] &quot;wgtp7&quot; &quot;wgtp8&quot; &quot;wgtp9&quot; &quot;wgtp10&quot; &quot;wgtp11&quot; &quot;wgtp12&quot; ## [121] &quot;wgtp13&quot; &quot;wgtp14&quot; &quot;wgtp15&quot; &quot;wgtp16&quot; &quot;wgtp17&quot; &quot;wgtp18&quot; ## [127] &quot;wgtp19&quot; &quot;wgtp20&quot; &quot;wgtp21&quot; &quot;wgtp22&quot; &quot;wgtp23&quot; &quot;wgtp24&quot; ## [133] &quot;wgtp25&quot; &quot;wgtp26&quot; &quot;wgtp27&quot; &quot;wgtp28&quot; &quot;wgtp29&quot; &quot;wgtp30&quot; ## [139] &quot;wgtp31&quot; &quot;wgtp32&quot; &quot;wgtp33&quot; &quot;wgtp34&quot; &quot;wgtp35&quot; &quot;wgtp36&quot; ## [145] &quot;wgtp37&quot; &quot;wgtp38&quot; &quot;wgtp39&quot; &quot;wgtp40&quot; &quot;wgtp41&quot; &quot;wgtp42&quot; ## [151] &quot;wgtp43&quot; &quot;wgtp44&quot; &quot;wgtp45&quot; &quot;wgtp46&quot; &quot;wgtp47&quot; &quot;wgtp48&quot; ## [157] &quot;wgtp49&quot; &quot;wgtp50&quot; &quot;wgtp51&quot; &quot;wgtp52&quot; &quot;wgtp53&quot; &quot;wgtp54&quot; ## [163] &quot;wgtp55&quot; &quot;wgtp56&quot; &quot;wgtp57&quot; &quot;wgtp58&quot; &quot;wgtp59&quot; &quot;wgtp60&quot; ## [169] &quot;wgtp61&quot; &quot;wgtp62&quot; &quot;wgtp63&quot; &quot;wgtp64&quot; &quot;wgtp65&quot; &quot;wgtp66&quot; ## [175] &quot;wgtp67&quot; &quot;wgtp68&quot; &quot;wgtp69&quot; &quot;wgtp70&quot; &quot;wgtp71&quot; &quot;wgtp72&quot; ## [181] &quot;wgtp73&quot; &quot;wgtp74&quot; &quot;wgtp75&quot; &quot;wgtp76&quot; &quot;wgtp77&quot; &quot;wgtp78&quot; ## [187] &quot;wgtp79&quot; &quot;wgtp80&quot; "],["data-wrangling.html", "Chapter 5 Data wrangling 5.1 Cleaning column names 5.2 Changing the names to variables 5.3 Deleting variables 5.4 Changing names of levels/factors 5.5 Finding missing values 5.6 5.7 Subsetting data set 5.8 Titanic data set", " Chapter 5 Data wrangling A major component of data analysis is data wrangling. This is the process of cleaning and transforming raw data into a usable format. This is a critical step in the data analysis process, as the quality of the data will directly impact the quality of the analysis. In this section, we will cover some common data wrangling tasks in R, including cleaning data, transforming data, and handling missing values. In general this will be the most time consuming period of your analysis, cleaning the names of the variables, deleting variables, changing names of levels/factors, finding missing values, etc. A major issue will always be the missing values, and how to deal with them in addition to resolving the issues of the data such as outliers and non sensical data. In almost all project data wrangling will incolve more than 80% of your time and that data anlysis usually less than 20% of your time. 5.1 Cleaning column names 5.1.1 Cleaning the names of the variables with janitor The easiest part is likely to clean the names of the variables. This can be done with the janitor package. This package has a function called clean_names that will organize the names of the variables in a data frame with specific charatceristics. This function will convert the names to lowercase, remove special characters, and replace spaces with underscores. This will make the names of the variables more consistent and easier to work with. Sometime older functions in R will not work with variables that have spaces or special characters, such as (é, ü, &amp;, %, ect). Let us start with a dirty data frame with the following problems: the names of the variables are in uppercase the names of the variables have spaces the names of the variables have special characters the names of the variables are too long the names of the variables are not consistent the names of the variables are not informative library(tibble) Dirty_data=tribble(~&quot;Name&quot;, ~&quot;Age ^2&quot;, ~&quot;Height (m)&quot;, ~&quot;% of value&quot;, ~&quot;Date of Birth&quot;, ~&quot;Place of Birth&quot;, &quot;Alice&quot;, 25, 1.70, .70, &quot;1997-01-01&quot;, &quot;New York City Manhathan&quot;, &quot;Bob&quot;, 30, 1.80, .80, &quot;1992-01-01&quot;, &quot;Los Angeles&quot;, &quot;Charlie&quot;, 35, NA, .90, &quot;1987-01-01&quot;, NA) Dirty_data ## # A tibble: 3 × 6 ## Name `Age ^2` `Height (m)` `% of value` `Date of Birth` `Place of Birth` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Alice 25 1.7 0.7 1997-01-01 New York City Manh… ## 2 Bob 30 1.8 0.8 1992-01-01 Los Angeles ## 3 Charlie 35 NA 0.9 1987-01-01 &lt;NA&gt; 5.1.2 Now let us clean the names of the variables Note the following changes all are in lowercase the spaces are replaced by underscores the special characters are removed and even given new names the names are more informative the names are more consistent library(janitor) Clean_df_clean=clean_names(Dirty_data) Clean_df= Clean_df_clean 5.2 Changing the names to variables To change the names of the variables in a data frame, you can use the rename function from the dplyr package. This function allows you to specify the new names of the variables using the new_name = old_name syntax. For example, to change the name of the name variable to first_name in a data frame called df, you would run: Changing the names of the variables is a common data wrangling task that can help make your data more readable and easier to work with. By using the rename function, you can quickly and easily change the names of the variables in a data frame to better reflect the data they contain. We will replace the name of the variable name to full_name in the data frame Clean_df. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## between, first, last ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Clean_df &lt;- Clean_df %&gt;% rename(first_name = name) Clean_df # observe the change in the name of the variable ## # A tibble: 3 × 6 ## first_name age_2 height_m percent_of_value date_of_birth place_of_birth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Alice 25 1.7 0.7 1997-01-01 New York City Manhat… ## 2 Bob 30 1.8 0.8 1992-01-01 Los Angeles ## 3 Charlie 35 NA 0.9 1987-01-01 &lt;NA&gt; 5.3 Deleting variables We can delete variables from a data frame using the select function from the dplyr package. This function allows you to select a subset of variables from a data frame by specifying the variables you want to keep. To delete a variable from a data frame, you can simply exclude it from the list of variables you want to keep. For example, to delete the age_2 variable from a data frame called df, you would run: library(dplyr) Clean_df &lt;- Clean_df %&gt;% select(-age_2) Clean_df # observe the change in the data frame, now the column is removed ## # A tibble: 3 × 5 ## first_name height_m percent_of_value date_of_birth place_of_birth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Alice 1.7 0.7 1997-01-01 New York City Manhathan ## 2 Bob 1.8 0.8 1992-01-01 Los Angeles ## 3 Charlie NA 0.9 1987-01-01 &lt;NA&gt; 5.4 Changing names of levels/factors We can change the names of the levels or factors in variable using the fct_recode function from the forcats package. Let us recode the name of the cities. library(forcats) Clean_df$place_of_birth &lt;- fct_recode(Clean_df$place_of_birth, &quot;NY&quot; = &quot;New York City Manhathan&quot;, &quot;LA&quot; = &quot;Los Angeles&quot;) Clean_df # observe the change in the data frame, now the names of the cities are recoded ## # A tibble: 3 × 5 ## first_name height_m percent_of_value date_of_birth place_of_birth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 Alice 1.7 0.7 1997-01-01 NY ## 2 Bob 1.8 0.8 1992-01-01 LA ## 3 Charlie NA 0.9 1987-01-01 &lt;NA&gt; 5.5 Finding missing values One of the challenges is detecting missing value in large data sets. The is.na function can be used to detect missing values in a data frame. This function returns a logical vector that indicates whether each element in the data frame is missing or not. For example, to find missing values in a data frame called df, you would run: Note that when there is missing data, the function will return a logical vector with TRUE for missing values and FALSE for non-missing values. is.na(Clean_df) # this will return a logical vector indicating whether each element in the data frame is missing or not ## first_name height_m percent_of_value date_of_birth place_of_birth ## [1,] FALSE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE ## [3,] FALSE TRUE FALSE FALSE TRUE The following code is acceptible for small data set, but for large data sets it is better to use the summary function. This function will return a summary of the data frame, including the number of missing values in each variable. For example, to get a summary of the missing values in a data frame called df, you would run: Note here that the place of birth for Charlie is missing, it has an NA value in addition to the height_m of an individual. summary(Clean_df) # this will return a summary of the data frame, including the number of missing values in each variable ## first_name height_m percent_of_value date_of_birth ## Length:3 Min. :1.700 Min. :0.70 Length:3 ## Class :character 1st Qu.:1.725 1st Qu.:0.75 Class :character ## Mode :character Median :1.750 Median :0.80 Mode :character ## Mean :1.750 Mean :0.80 ## 3rd Qu.:1.775 3rd Qu.:0.85 ## Max. :1.800 Max. :0.90 ## NA&#39;s :1 ## place_of_birth ## LA :1 ## NY :1 ## NA&#39;s:1 ## ## ## ## 5.6 Subset all rows with missing values in the variables. This will result in a rwo that will include only Charlie. Subset all rows with missing values in the variables. This will result in a rwo that will include only Charlie. NOTE the the command is the ! symbol before the complete.cases function. This will return all rows with missing values in the data frame. Clean_df[!complete.cases(Clean_df), ] # this will return all rows with missing values in the data frame ## # A tibble: 1 × 5 ## first_name height_m percent_of_value date_of_birth place_of_birth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 Charlie NA 0.9 1987-01-01 &lt;NA&gt; Note if you want subset all rows with NO missing values in the variables. This will result in a two rows that will include only Alice and Bob. Clean_df[complete.cases(Clean_df), ] # this will return all rows with NO missing values in the data frame ## # A tibble: 2 × 5 ## first_name height_m percent_of_value date_of_birth place_of_birth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;fct&gt; ## 1 Alice 1.7 0.7 1997-01-01 NY ## 2 Bob 1.8 0.8 1992-01-01 LA 5.7 Subsetting data set The function to subset a data frame is filter. This function is used to extract rows from a data frame that meet a certain condition. The syntax for the filter function is as follows: library(dplyr) Clean_df_clean %&gt;% filter(age_2 &gt; 29) # this will return all rows where the age is greater than 29 ## # A tibble: 2 × 6 ## name age_2 height_m percent_of_value date_of_birth place_of_birth ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Bob 30 1.8 0.8 1992-01-01 Los Angeles ## 2 Charlie 35 NA 0.9 1987-01-01 &lt;NA&gt; 5.8 Titanic data set library(readr) Titanic &lt;- read_csv(&quot;Data/ Titanic.csv&quot;) # The data is in the &quot;DATA&quot; folder ## Rows: 1309 Columns: 12 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Name, Sex, Ticket, Cabin, Embarked ## dbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Titanic = clean_names(Titanic) # this will clean the names of the variables in the data frame names(Titanic) # this will return the names of the variables in the data frame ## [1] &quot;passenger_id&quot; &quot;survived&quot; &quot;pclass&quot; &quot;name&quot; &quot;sex&quot; ## [6] &quot;age&quot; &quot;sib_sp&quot; &quot;parch&quot; &quot;ticket&quot; &quot;fare&quot; ## [11] &quot;cabin&quot; &quot;embarked&quot; Let us now select all rows with females, and that are in the first class and age less than 20. From 1,309 passengers we 16 females with the above characteristics. Titanic %&gt;% filter(sex==&quot;female&quot; &amp; pclass==1 &amp; age&lt;20) # this will return ## # A tibble: 16 × 12 ## passenger_id survived pclass name sex age sib_sp parch ticket fare ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 137 1 1 Newsom, M… fema… 19 0 2 11752 26.3 ## 2 292 1 1 Bishop, M… fema… 19 1 0 11967 91.1 ## 3 298 0 1 Allison, … fema… 2 1 2 113781 152. ## 4 308 1 1 Penasco y… fema… 17 1 0 PC 17… 109. ## 5 312 1 1 Ryerson, … fema… 18 2 2 PC 17… 262. ## 6 330 1 1 Hippach, … fema… 16 0 1 111361 58.0 ## 7 436 1 1 Carter, M… fema… 14 1 2 113760 120 ## 8 505 1 1 Maioni, M… fema… 16 0 0 110152 86.5 ## 9 586 1 1 Taussig, … fema… 18 0 2 110413 79.6 ## 10 690 1 1 Madill, M… fema… 15 0 1 24160 211. ## 11 701 1 1 Astor, Mr… fema… 18 1 0 PC 17… 228. ## 12 782 1 1 Dick, Mrs… fema… 17 1 0 17474 57 ## 13 854 1 1 Lines, Mi… fema… 16 0 1 PC 17… 39.4 ## 14 888 1 1 Graham, M… fema… 19 0 0 112053 30 ## 15 1074 1 1 Marvin, M… fema… 18 1 0 113773 53.1 ## 16 1287 1 1 Smith, Mr… fema… 18 1 0 13695 60 ## # ℹ 2 more variables: cabin &lt;chr&gt;, embarked &lt;chr&gt; "],["understanding-your-file-data-strutcture.html", "Chapter 6 Understanding your file data strutcture 6.1 Introduction 6.2 Functions to evaluate your data 6.3 Example", " Chapter 6 Understanding your file data strutcture 6.1 Introduction Every data analysis project starts with understanding the data. This includes understanding the dimensions of the data, the structure of the data, the levels of the variables, and the relationships between the variables. In this chapter, we will learn how to evaluate the data and understand the data. 6.2 Functions to evaluate your data The following functions can be used to evaluate the data: dim(): This function returns the dimensions of the data. str(): This function returns the structure of the data. levels(): This function returns the levels of the variables. summary(): This function returns the summary of the data. head(): This function returns the first 6 rows of the data. tail(): This function returns the last f6 rows of the data. 6.3 Example Let us look at an example to understand the data. https://catalog.data.gov/dataset/data-from-correlations-among-agronomic-traits-obtained-from-sorghum-accessions-planted-in- Data based on a total of 179 sorghum accessions from Ethiopia, Gambia, and Senegal maintained by the USDA-ARS, Plant Genetic Resources Conservation Unit, Griffin, Georgia, USA, and planted in replicated plots in Isabela, Puerto Rico. Data for anthracnose, grain mold and rust infections with various agronomic traits such as anthracnose, grain mold, rust, seed weight, percent germination rate, and grain yield, seed weight, panicle height, panicle length, and flowering time were collected in 2017 and 2018 by L. K. Prom, H. Cuevas, E. Ahn, T. S. Isakeit, and C. W. Magill. The disease rating was based on previously published scale of 1 to 5. Seed weight in grams was based on the weight of 100 randomly selected seeds from each replication. Percent germination based on the number of seeds that germinated out of 100 seeds placed on germination paper. At maturity, plant height was measured from the soil to the top of the plant in centimeters, and panicle length was measured from the first branch with racemes to the top of the panicle. Grain yield in grams was based on three harvested panicles per accession, and each panicle was threshed and weighed in grams. The flowering date was based on Julian days. (this section was copied from DATA.gov, the above link). Load the data library(readr) Sorghum_2017_2018_Diseaase_traits &lt;- read_csv(&quot;Data/Sorghum_2017_2018_Diseaase_traits.csv&quot;) ## Rows: 2412 Columns: 10 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Cultivar ## dbl (9): Anthrac, Rust, GrainM, Seedwt, %Germ, Pani Hght, Pani Length, Grain... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Sorghum=Sorghum_2017_2018_Diseaase_traits 6.3.1 Dimensions of the data dim(Sorghum) # number of rows (2412) and number of columns (10) ## [1] 2412 10 6.3.2 Structure of the data Note here we are using the str() function to understand the structure of the data. This function provides the structure of the data, including the variables and their types. int: Integer dbl: Double chr: Character fct: Factor dttm: Date time lgl: Logical date: Date time: Time hms: Hours, minutes, seconds and others Using str() function to understand the structure of the data, you will see the first data points for each variable, and the type of the variable. str(Sorghum) ## spc_tbl_ [2,412 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Cultivar : chr [1:2412] &quot;PI665166&quot; &quot;PI665165&quot; &quot;PI661148&quot; &quot;PI660638&quot; ... ## $ Anthrac : num [1:2412] 2 4 5 2 5 2 4 2 2 3 ... ## $ Rust : num [1:2412] 1 1 3 1 1 5 3 3 1 1 ... ## $ GrainM : num [1:2412] 2 2 3 NA 2 3 4 2 4 2 ... ## $ Seedwt : num [1:2412] 3.17 4.02 2.78 NA 3.22 2.17 1.52 1.76 2.4 3.33 ... ## $ %Germ : num [1:2412] 70 88 70 NA 81 5 49 64 64 28 ... ## $ Pani Hght : num [1:2412] 185 270 145 220 218 220 200 125 60 72 ... ## $ Pani Length: num [1:2412] 17 20 15 20 18 15 30 25 10 12 ... ## $ Grain Yld : num [1:2412] 28.9 54.2 30.4 12.2 26.5 ... ## $ Julian days: num [1:2412] 127 127 109 118 128 NA 127 109 109 109 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Cultivar = col_character(), ## .. Anthrac = col_double(), ## .. Rust = col_double(), ## .. GrainM = col_double(), ## .. Seedwt = col_double(), ## .. `%Germ` = col_double(), ## .. `Pani Hght` = col_double(), ## .. `Pani Length` = col_double(), ## .. `Grain Yld` = col_double(), ## .. `Julian days` = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 6.3.3 Recoding variables The variable is mis coded as a numeric variable where Rust represent different types of Rust, thus this variable should be recoded as a factor variable. Sorghum$Rust=as.factor(Sorghum$Rust) str(Sorghum) # note now the variable **Rust** is a factor variable. ## spc_tbl_ [2,412 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Cultivar : chr [1:2412] &quot;PI665166&quot; &quot;PI665165&quot; &quot;PI661148&quot; &quot;PI660638&quot; ... ## $ Anthrac : num [1:2412] 2 4 5 2 5 2 4 2 2 3 ... ## $ Rust : Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 3 1 1 5 3 3 1 1 ... ## $ GrainM : num [1:2412] 2 2 3 NA 2 3 4 2 4 2 ... ## $ Seedwt : num [1:2412] 3.17 4.02 2.78 NA 3.22 2.17 1.52 1.76 2.4 3.33 ... ## $ %Germ : num [1:2412] 70 88 70 NA 81 5 49 64 64 28 ... ## $ Pani Hght : num [1:2412] 185 270 145 220 218 220 200 125 60 72 ... ## $ Pani Length: num [1:2412] 17 20 15 20 18 15 30 25 10 12 ... ## $ Grain Yld : num [1:2412] 28.9 54.2 30.4 12.2 26.5 ... ## $ Julian days: num [1:2412] 127 127 109 118 128 NA 127 109 109 109 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Cultivar = col_character(), ## .. Anthrac = col_double(), ## .. Rust = col_double(), ## .. GrainM = col_double(), ## .. Seedwt = col_double(), ## .. `%Germ` = col_double(), ## .. `Pani Hght` = col_double(), ## .. `Pani Length` = col_double(), ## .. `Grain Yld` = col_double(), ## .. `Julian days` = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 6.3.4 Levels of the variables The levels of variables refer to the unique values of the variables. The levels() function can be used to get the levels of the variables. For example the levels of the variable Cultivar can be obtained as follows: levels(Sorghum$Rust) # since the variable Rust is a factor variable, we can get the levels of the variable Rust. ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; # However if the variable is not a factor variable, we can convert it to a factor variable and then get the levels of the variable. Sorghum$Cultivar=as.factor(Sorghum$Cultivar) head(levels(Sorghum$Cultivar)) # levels of the variable Cultivar, showing only the first six levels ## [1] &quot;Brandeis (PI576436)&quot; &quot;BTx378&quot; &quot;BTx398&quot; ## [4] &quot;BTx623&quot; &quot;IS18760&quot; &quot;PI147833&quot; Another miss coded variable is the variable is the Julain days, however, but the information is not the Julian days which would be the number of days that have passed since the first day since January 1, 4713 BC. What the authors have in the number of the consecutive days since the 1st January of that year. Thus we need to do the follwoing two steps: change the name to Consecutive_days change the Consecutive_days to be more informative by converting it to a date variable. Sorghum$Consecutive_days=Sorghum$&quot;Julian days&quot; Sorghum$Consecutive_days=as.Date(&quot;2017-01-01&quot;)+ Sorghum$Consecutive_days-1 str(Sorghum) # note now the variable **Consecutive_days** is a date variable. ## spc_tbl_ [2,412 × 11] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Cultivar : Factor w/ 201 levels &quot;Brandeis (PI576436)&quot;,..: 171 170 167 166 165 159 158 155 154 153 ... ## $ Anthrac : num [1:2412] 2 4 5 2 5 2 4 2 2 3 ... ## $ Rust : Factor w/ 6 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 1 3 1 1 5 3 3 1 1 ... ## $ GrainM : num [1:2412] 2 2 3 NA 2 3 4 2 4 2 ... ## $ Seedwt : num [1:2412] 3.17 4.02 2.78 NA 3.22 2.17 1.52 1.76 2.4 3.33 ... ## $ %Germ : num [1:2412] 70 88 70 NA 81 5 49 64 64 28 ... ## $ Pani Hght : num [1:2412] 185 270 145 220 218 220 200 125 60 72 ... ## $ Pani Length : num [1:2412] 17 20 15 20 18 15 30 25 10 12 ... ## $ Grain Yld : num [1:2412] 28.9 54.2 30.4 12.2 26.5 ... ## $ Julian days : num [1:2412] 127 127 109 118 128 NA 127 109 109 109 ... ## $ Consecutive_days: Date[1:2412], format: &quot;2017-05-07&quot; &quot;2017-05-07&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Cultivar = col_character(), ## .. Anthrac = col_double(), ## .. Rust = col_double(), ## .. GrainM = col_double(), ## .. Seedwt = col_double(), ## .. `%Germ` = col_double(), ## .. `Pani Hght` = col_double(), ## .. `Pani Length` = col_double(), ## .. `Grain Yld` = col_double(), ## .. `Julian days` = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 6.3.5 Summary of the data The function summary() can be used to get a synopsis of the description of each variable. The summary function includes the following output Min: Minimum value 1st Qu: First quartile Median: Median value Mean: Mean value 3rd Qu: Third quartile Max: Maximum value NA's: Number of missing values summary(Sorghum) ## Cultivar Anthrac Rust GrainM ## Brandeis (PI576436): 12 Min. :2.000 1 : 369 Min. :1.00 ## BTx378 : 12 1st Qu.:2.000 2 : 156 1st Qu.:3.00 ## BTx398 : 12 Median :2.000 3 : 169 Median :3.00 ## BTx623 : 12 Mean :2.221 4 : 156 Mean :3.41 ## IS18760 : 12 3rd Qu.:2.000 5 : 124 3rd Qu.:4.00 ## PI147833 : 12 Max. :5.000 6 : 87 Max. :5.00 ## (Other) :2340 NA&#39;s :1351 NA&#39;s:1351 NA&#39;s :1791 ## Seedwt %Germ Pani Hght Pani Length ## Min. :0.300 Min. : 0.00 Min. : 50.0 Min. :10.00 ## 1st Qu.:1.600 1st Qu.: 43.52 1st Qu.:125.0 1st Qu.:20.00 ## Median :2.070 Median : 75.50 Median :165.0 Median :20.00 ## Mean :2.114 Mean : 65.97 Mean :174.9 Mean :24.86 ## 3rd Qu.:2.500 3rd Qu.: 88.40 3rd Qu.:219.5 3rd Qu.:30.00 ## Max. :4.600 Max. :100.00 Max. :390.0 Max. :80.00 ## NA&#39;s :1782 NA&#39;s :1784 NA&#39;s :1018 NA&#39;s :1018 ## Grain Yld Julian days Consecutive_days ## Min. : 0.372 Min. :109.0 Min. :2017-04-19 ## 1st Qu.: 7.228 1st Qu.:109.0 1st Qu.:2017-04-19 ## Median :12.342 Median :112.0 Median :2017-04-22 ## Mean :16.308 Mean :117.5 Mean :2017-04-27 ## 3rd Qu.:22.110 3rd Qu.:127.0 3rd Qu.:2017-05-07 ## Max. :97.748 Max. :148.0 Max. :2017-05-28 ## NA&#39;s :1997 NA&#39;s :1959 NA&#39;s :1959 6.3.6 First and lat rows few rows of the data head(Sorghum) # first few rows of the data ## # A tibble: 6 × 11 ## Cultivar Anthrac Rust GrainM Seedwt `%Germ` `Pani Hght` `Pani Length` ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PI665166 2 1 2 3.17 70 185 17 ## 2 PI665165 4 1 2 4.02 88 270 20 ## 3 PI661148 5 3 3 2.78 70 145 15 ## 4 PI660638 2 1 NA NA NA 220 20 ## 5 PI660637 5 1 2 3.22 81 218 18 ## 6 PI644717 2 5 3 2.17 5 220 15 ## # ℹ 3 more variables: `Grain Yld` &lt;dbl&gt;, `Julian days` &lt;dbl&gt;, ## # Consecutive_days &lt;date&gt; tail(Sorghum) # last few rows of the data ## # A tibble: 6 × 11 ## Cultivar Anthrac Rust GrainM Seedwt `%Germ` `Pani Hght` `Pani Length` ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 SC283 NA &lt;NA&gt; NA NA NA 145 30 ## 2 SC112-14 NA &lt;NA&gt; NA NA NA 115 15 ## 3 IS18760 NA &lt;NA&gt; NA NA NA 240 35 ## 4 PI570726 NA &lt;NA&gt; NA NA NA NA NA ## 5 PI569979 NA &lt;NA&gt; NA NA NA NA NA ## 6 PI570841 NA &lt;NA&gt; NA NA NA NA NA ## # ℹ 3 more variables: `Grain Yld` &lt;dbl&gt;, `Julian days` &lt;dbl&gt;, ## # Consecutive_days &lt;date&gt; head(Sorghum, 10) # first 10 rows of the data, you change the output to any number of rows you want to see. ## # A tibble: 10 × 11 ## Cultivar Anthrac Rust GrainM Seedwt `%Germ` `Pani Hght` `Pani Length` ## &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PI665166 2 1 2 3.17 70 185 17 ## 2 PI665165 4 1 2 4.02 88 270 20 ## 3 PI661148 5 3 3 2.78 70 145 15 ## 4 PI660638 2 1 NA NA NA 220 20 ## 5 PI660637 5 1 2 3.22 81 218 18 ## 6 PI644717 2 5 3 2.17 5 220 15 ## 7 PI644502 4 3 4 1.52 49 200 30 ## 8 PI576431 2 3 2 1.76 64 125 25 ## 9 PI576381 2 1 4 2.4 64 60 10 ## 10 PI576380 3 1 2 3.33 28 72 12 ## # ℹ 3 more variables: `Grain Yld` &lt;dbl&gt;, `Julian days` &lt;dbl&gt;, ## # Consecutive_days &lt;date&gt; "],["tests-of-normality-and-equality-of-variance.html", "Chapter 7 Tests of Normality and Equality of Variance 7.1 Shapiro-Wilk normality test 7.2 The shapiro-Francia normality test 7.3 Anderson-Darling normality test 7.4 Cramer-von Mises test of normality 7.5 Lilliefors test of normality 7.6 Visualizing the data 7.7 Remove the outlier from the day1 hygiene score", " Chapter 7 Tests of Normality and Equality of Variance Add and activate packages. NOTE THE NEW FUNCTION to install all packages ONLY IF Needed…..pacman::p_load(car, pastecs, psych, ggplot2) library(readr) DownloadFestival &lt;- read_csv(&quot;Data/DownloadFestival.csv&quot;) ## Rows: 810 Columns: 5 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (4): ticknumb, day1, day2, day3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(DownloadFestival) ## # A tibble: 6 × 5 ## ticknumb gender day1 day2 day3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2111 Male 2.64 1.35 1.61 ## 2 2229 Female 0.97 1.41 0.29 ## 3 2338 Male 0.84 NA NA ## 4 2384 Female 3.03 NA NA ## 5 2401 Female 0.88 0.08 NA ## 6 2405 Male 0.85 NA NA dlf &lt;- DownloadFestival 7.0.0.1 Understanding the data METADATA The data is from a festival in UK. The data is about the hygiene score of the participants. The hygiene score is a score from 0 to 4. The scale is as follows: 0 = You smell like a rotting corpse 4 = You smell like of sweet roses Multiple variables #Two alternative ways to describe multiple variables. psych::describe(cbind(dlf$day1, dlf$day2, dlf$day3)) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 810 1.79 0.94 1.79 1.77 0.70 0.02 20.02 20.00 8.83 168.97 0.03 ## X2 2 264 0.96 0.72 0.79 0.87 0.61 0.00 3.44 3.44 1.08 0.76 0.04 ## X3 3 123 0.98 0.71 0.76 0.90 0.61 0.02 3.41 3.39 1.01 0.59 0.06 psych::describe(dlf[,c(&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;)]) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## day1 1 810 1.79 0.94 1.79 1.77 0.70 0.02 20.02 20.00 8.83 168.97 0.03 ## day2 2 264 0.96 0.72 0.79 0.87 0.61 0.00 3.44 3.44 1.08 0.76 0.04 ## day3 3 123 0.98 0.71 0.76 0.90 0.61 0.02 3.41 3.39 1.01 0.59 0.06 7.1 Shapiro-Wilk normality test Test of normality, Shapiro Wilks Test is a test of normality in frequentist statistics. It tests the null hypothesis that the data was drawn from a normal distribution. The test is based on the correlation between the data and the corresponding normal scores. The test is considered to be the most powerful test for normality, particularly for small sample sizes. However it is sensitive to the presence of outliers. and tends to be conservative when the sample size is large. in other words it tends to reject the null model when sample size are large and the data is sufficient normal. library(pastecs) stat.desc(dlf$day3, basic = FALSE, norm = TRUE) # &quot;norm=TRUE&quot;&quot; is to calculate the Shapiro Wilk Test ## median mean SE.mean CI.mean.0.95 var std.dev ## 7.600000e-01 9.765041e-01 6.404352e-02 1.267805e-01 5.044934e-01 7.102770e-01 ## coef.var skewness skew.2SE kurtosis kurt.2SE normtest.W ## 7.273672e-01 1.007813e+00 2.309035e+00 5.945454e-01 6.862946e-01 9.077516e-01 ## normtest.p ## 3.804486e-07 stat.desc(cbind(dlf$day1, dlf$day2, dlf$day3), basic = FALSE, norm = TRUE) ## V1 V2 V3 ## median 1.790000e+00 7.900000e-01 7.600000e-01 ## mean 1.793358e+00 9.609091e-01 9.765041e-01 ## SE.mean 3.318617e-02 4.436095e-02 6.404352e-02 ## CI.mean.0.95 6.514115e-02 8.734781e-02 1.267805e-01 ## var 8.920705e-01 5.195239e-01 5.044934e-01 ## std.dev 9.444949e-01 7.207801e-01 7.102770e-01 ## coef.var 5.266627e-01 7.501022e-01 7.273672e-01 ## skewness 8.832504e+00 1.082811e+00 1.007813e+00 ## skew.2SE 5.140707e+01 3.611574e+00 2.309035e+00 ## kurtosis 1.689671e+02 7.554615e-01 5.945454e-01 ## kurt.2SE 4.923139e+02 1.264508e+00 6.862946e-01 ## normtest.W 6.539142e-01 9.083191e-01 9.077516e-01 ## normtest.p 1.545986e-37 1.281630e-11 3.804486e-07 7.2 The shapiro-Francia normality test The Shapiro-Francia test is known to perform well, see also the comments by Royston (1993). The expected ordered quantiles from the standard normal distribution are approximated by qnorm(ppoints(x, a = 3/8)), being slightly different from the approximation qnorm(ppoints(x, a = 1/2)) used for the normal quantile-quantile plot by qqnorm for sample sizes greater than 10. Royston, P. (1993): A pocket-calculator algorithm for the Shapiro-Francia test for non-normality: an application to medicine. Statistics in Medicine, 12, 181–184. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. sf &lt;- sf.test(dlf$day3) sf ## ## Shapiro-Francia normality test ## ## data: dlf$day3 ## W = 0.90903, p-value = 2.348e-06 7.3 Anderson-Darling normality test The Anderson-Darling test is a modification of the Kolmogorov-Smirnov test that gives more weight to the tails of the distribution. It is a more powerful test than the Kolmogorov-Smirnov test, but it is also more sensitive to departures from normality in the center of the distribution. Stephens, M.A. (1986): Tests based on EDF statistics. In: D’Agostino, R.B. and Stephens, M.A., eds.: Goodness-of-Fit Techniques. Marcel Dekker, New York. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. ad.test(dlf$day1) ## ## Anderson-Darling normality test ## ## data: dlf$day1 ## A = 15.406, p-value &lt; 2.2e-16 7.4 Cramer-von Mises test of normality The Cramer-von Mises test is a modification of the Anderson-Darling test that gives more weight to the center of the distribution. It is a more powerful test than the Anderson-Darling test, but it is also more sensitive to departures from normality in the tails of the distribution. Stephens, M.A. (1986): Tests based on EDF statistics. In: D’Agostino, R.B. and Stephens, M.A., eds.: Goodness-of-Fit Techniques. Marcel Dekker, New York. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. cvm.test(dlf$day1) ## Warning in cvm.test(dlf$day1): p-value is smaller than 7.37e-10, cannot be ## computed more accurately ## ## Cramer-von Mises normality test ## ## data: dlf$day1 ## W = 1.9628, p-value = 7.37e-10 7.5 Lilliefors test of normality The Lilliefors (Kolomorov-Smirnov) test is the most famous EDF omnibus test for normality. Compared to the Anderson-Darling test and the Cramer-von Mises test it is known to perform worse. Although the test statistic obtained from lillie.test(x) is the same as that obtained from ks.test(x, “pnorm”, mean(x), sd(x)), it is not correct to use the p-value from the latter for the composite hypothesis of normality (mean and variance unknown), since the distribution of the test statistic is different when the parameters are estimated. Dallal, G.E. and Wilkinson, L. (1986): An analytic approximation to the distribution of Lilliefors’ test for normality. The American Statistician, 40, 294–296. Stephens, M.A. (1974): EDF statistics for goodness of fit and some comparisons. Journal of the American Statistical Association, 69, 730–737. Thode Jr., H.C. (2002): Testing for Normality. Marcel Dekker, New York. lillie.test(dlf$day1) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: dlf$day1 ## D = 0.082706, p-value = 2.539e-14 7.6 Visualizing the data 7.6.1 qq-plot Add a straight line on the qqplot # This function is to add a straight line through the qqplot qqplot.data &lt;- function (vec) # argument: vector of numbers { # following four lines from base R&#39;s qqline() y &lt;- quantile(vec[!is.na(vec)], c(0.25, 0.75)) x &lt;- qnorm(c(0.25, 0.75)) slope &lt;- diff(y)/diff(x) int &lt;- y[1L] - slope * x[1L] d &lt;- data.frame(resids = vec) ggplot(d, aes(sample = resids)) + stat_qq() + geom_abline(slope = slope, intercept = int, color=&quot;red&quot;) } qqplot.data(dlf$day3) ## Warning: Removed 687 rows containing non-finite outside the scale range ## (`stat_qq()`). ggsave(&quot;qqplot.png&quot;) ## Saving 7 x 5 in image ## Warning: Removed 687 rows containing non-finite outside the scale range ## (`stat_qq()`). 7.6.2 Histogram dlf=subset(DownloadFestival) head(dlf) ## # A tibble: 6 × 5 ## ticknumb gender day1 day2 day3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2111 Male 2.64 1.35 1.61 ## 2 2229 Female 0.97 1.41 0.29 ## 3 2338 Male 0.84 NA NA ## 4 2384 Female 3.03 NA NA ## 5 2401 Female 0.88 0.08 NA ## 6 2405 Male 0.85 NA NA tail(dlf) ## # A tibble: 6 × 5 ## ticknumb gender day1 day2 day3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4749 Female 0.52 NA NA ## 2 4756 Female 2.91 0.94 NA ## 3 4758 Female 2.61 1.44 NA ## 4 4759 Female 1.47 NA NA ## 5 4760 Male 1.28 NA NA ## 6 4765 Female 1.26 NA NA hist.day1 &lt;- ggplot(dlf, aes(day1)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;white&quot;) hist.day1+ labs(x=&quot;Hygiene score on day 1&quot;, y = &quot;Density&quot;) ## Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. hist.day1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggsave(&quot;histogram_festival_hygiene.pdf&quot;) # Can be either be a device function (e.g. png()), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex), &quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only) ## Saving 7 x 5 in image ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 7.6.3 Density plot The Normal Distribution https://en.wikipedia.org/wiki/Normal_distribution visualization if the observed distribution follows a theoretical normal distribution Test to determine if the observed distribution follows a theoretical distribution \\[P(x)=\\frac{1}{{\\sigma\\sqrt{ 2\\pi}}}{e}^{\\frac{{(x-µ)}^{2}}{{2\\sigma}^{2}}}\\] # Add a line to represent the normal distrubution hist.day1 + stat_function(fun = dnorm, args = list(mean = mean(dlf$day1,na.rm = TRUE), sd = sd(dlf$day1 , na.rm = TRUE)), colour = &quot;blue&quot;, size = 1) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. shapiro.test(dlf$day1) # don&#39;t use with more than 40 a 200 data point ## ## Shapiro-Wilk normality test ## ## data: dlf$day1 ## W = 0.65391, p-value &lt; 2.2e-16 length(dlf$day1) ## [1] 810 Visualize the distribution of the data “Histogram” 7.7 Remove the outlier from the day1 hygiene score dlf\\(day1 &lt;- ifelse(dlf\\)day1 &gt; 5, NA, dlf\\(day1) df\\)Column = ifelse(df$column_to_be evaluated, replace_with_NA, otherwise_leave_as_before) dlf$day1 &lt;- ifelse(dlf$day1 &gt; 5, NA, dlf$day1) # Note here that we use ..density.. # What is the difference between ..density.. and frequency? hist.day1 &lt;- ggplot(dlf, aes(day1)) + theme(legend.position = &quot;none&quot;) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;white&quot;) + labs(x=&quot;Hygiene score on day 1&quot;, y = &quot;Density&quot;) hist.day1 ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_bin()`). dlf=DownloadFestival #Quantifying normality with numbers library(psych) #load the &quot;psych&quot; library, if you haven&#39;t already, for the describe() function. #Using the describe() function for a single variable. psych::describe(dlf$day2) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 264 0.96 0.72 0.79 0.87 0.61 0 3.44 3.44 1.08 0.76 0.04 7.7.1 What is the variance? The variance is a measure of dispersion \\[s^{ 2 }=\\frac { \\sum _{ i=1 }^{ n }{ (x_{ i }-\\bar { x } ) } ^{ 2 } }{ n-1 } \\] 7.7.2 What is the standard deviation? The standard deviation of the mean is a measure of the dispersion of the mean of the data. It is the square root of the variance. \\[s=\\sqrt { s^{ 2 } } \\] "],["what-is-the-mode.html", "Chapter 8 What is the mode? 8.1 Test of normality, Shapiro Wilks Test 8.2 Tests of Equality of variances", " Chapter 8 What is the mode? The mode is the value that appears most frequently in a data set. A set of data may have one mode, more than one mode, or no mode at all. The mode is one of the measures of CENTRAL TENDENCIES. The package modeest has many functions to evaluate modes. Note that in two of the data sets there are no SINGLE most common values, then it result that is return is NA. # the mode library(modeest) ## Registered S3 method overwritten by &#39;rmutil&#39;: ## method from ## plot.residuals psych mfv(dlf$day1, method=&quot;mfv&quot;) ## [1] 2 mfv(dlf$day2, method=&quot;mfv&quot;) ## [1] NA mfv(dlf$day3, method=&quot;mfv&quot;) ## [1] NA 8.1 Test of normality, Shapiro Wilks Test library(pastecs) stat.desc(dlf$day3, basic = FALSE, norm = TRUE) # &quot;norm=TRUE&quot;&quot; is to calculate the Shapiro Wilk Test ## median mean SE.mean CI.mean.0.95 var std.dev ## 7.600000e-01 9.765041e-01 6.404352e-02 1.267805e-01 5.044934e-01 7.102770e-01 ## coef.var skewness skew.2SE kurtosis kurt.2SE normtest.W ## 7.273672e-01 1.007813e+00 2.309035e+00 5.945454e-01 6.862946e-01 9.077516e-01 ## normtest.p ## 3.804486e-07 We can evaluate the normality of multiple variables at the same time using the following code: Note here we use [,], where the inside the square brackets we specify the variables we want to evaluate. Before the comma, we specify the rows we want to evaluate and after the comma, we specify the columns we want to evaluate. round(stat.desc(dlf[, c(&quot;day1&quot;, &quot;day2&quot;, &quot;day3&quot;)], basic = FALSE, norm = TRUE), digits = 4) ## day1 day2 day3 ## median 1.7900 0.7900 0.7600 ## mean 1.7934 0.9609 0.9765 ## SE.mean 0.0332 0.0444 0.0640 ## CI.mean.0.95 0.0651 0.0873 0.1268 ## var 0.8921 0.5195 0.5045 ## std.dev 0.9445 0.7208 0.7103 ## coef.var 0.5267 0.7501 0.7274 ## skewness 8.8325 1.0828 1.0078 ## skew.2SE 51.4071 3.6116 2.3090 ## kurtosis 168.9671 0.7555 0.5945 ## kurt.2SE 492.3139 1.2645 0.6863 ## normtest.W 0.6539 0.9083 0.9078 ## normtest.p 0.0000 0.0000 0.0000 8.2 Tests of Equality of variances Most parametric test require that when testing differences among groups, these have more or less the same variance around the mean. The Levene’s test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group means. The test is considered to be robust against departures from normality, but it is sensitive to the presence of outliers. 8.2.1 The F-test of equality of variance The F-test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the ratio of the variances in the different groups. This test is EXTREMELY sensitive to non-normality and outliers. Consequently the F-test is considered to be less robust than the Levene’s test, Bartlett’s test or the Brown-Forsythe test. It is NOT recommended to use the F-test for testing the equality of variances. Sokal, R. R., Rohlf, F. J. (1995). Biometry: The Principles and Practice of Statistics in Biological Research. W. H. Freeman and Company \\[F=s_x^2/s_y^2\\], where \\(s_x^2\\) and \\(s_y^2\\) are the variances of the two groups, and x is the variance of the group with the larger variance. var.test(dlf$day1, dlf$day2) ## ## F test to compare two variances ## ## data: dlf$day1 and dlf$day2 ## F = 1.7171, num df = 809, denom df = 263, p-value = 3.23e-07 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 1.402898 2.080705 ## sample estimates: ## ratio of variances ## 1.717092 8.2.2 Bartlett’s test of equality of variance Bartlett’s test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group means. The test is considered to be robust against departures from normality, but it is sensitive to the presence of outliers. Bartlett, M. S. (1937). Properties of sufficiency and statistical tests. Proceedings of the Royal Society of London Series A 160, 268–282. doi:10.1098/rspa.1937.0109. Note: that the data needs to be manipulated so it appear in the long form subset(dlf, select = c(&quot;day1&quot;, &quot;day2&quot;)) -&gt; dlfsub # create a subset of the data dlfsub1 &lt;- stack(dlfsub) # reorganize the data in the long form dlfsub1 # the data is now in the long form ## values ind ## 1 2.64 day1 ## 2 0.97 day1 ## 3 0.84 day1 ## 4 3.03 day1 ## 5 0.88 day1 ## 6 0.85 day1 ## 7 1.56 day1 ## 8 3.02 day1 ## 9 2.29 day1 ## 10 1.11 day1 ## 11 2.17 day1 ## 12 0.82 day1 ## 13 1.41 day1 ## 14 1.76 day1 ## 15 1.38 day1 ## 16 2.79 day1 ## 17 1.50 day1 ## 18 1.91 day1 ## 19 2.32 day1 ## 20 2.05 day1 ## 21 2.17 day1 ## 22 2.05 day1 ## 23 1.61 day1 ## 24 1.66 day1 ## 25 2.30 day1 ## 26 2.76 day1 ## 27 1.44 day1 ## 28 1.06 day1 ## 29 3.23 day1 ## 30 0.97 day1 ## 31 2.57 day1 ## 32 0.26 day1 ## 33 0.47 day1 ## 34 1.73 day1 ## 35 1.94 day1 ## 36 1.91 day1 ## 37 2.08 day1 ## 38 1.91 day1 ## 39 1.42 day1 ## 40 1.50 day1 ## 41 0.11 day1 ## 42 1.67 day1 ## 43 2.08 day1 ## 44 2.05 day1 ## 45 2.00 day1 ## 46 1.52 day1 ## 47 1.58 day1 ## 48 1.28 day1 ## 49 1.88 day1 ## 50 1.32 day1 ## 51 2.09 day1 ## 52 2.00 day1 ## 53 2.64 day1 ## 54 0.85 day1 ## 55 2.47 day1 ## 56 1.79 day1 ## 57 1.64 day1 ## 58 1.32 day1 ## 59 2.97 day1 ## 60 1.44 day1 ## 61 2.02 day1 ## 62 1.79 day1 ## 63 1.34 day1 ## 64 2.29 day1 ## 65 1.66 day1 ## 66 0.60 day1 ## 67 1.76 day1 ## 68 1.50 day1 ## 69 2.08 day1 ## 70 1.00 day1 ## 71 1.73 day1 ## 72 1.05 day1 ## 73 2.81 day1 ## 74 1.52 day1 ## 75 1.47 day1 ## 76 2.64 day1 ## 77 2.20 day1 ## 78 0.55 day1 ## 79 2.29 day1 ## 80 2.00 day1 ## 81 2.23 day1 ## 82 2.45 day1 ## 83 1.20 day1 ## 84 2.91 day1 ## 85 1.14 day1 ## 86 1.88 day1 ## 87 0.94 day1 ## 88 1.85 day1 ## 89 2.58 day1 ## 90 0.61 day1 ## 91 0.70 day1 ## 92 1.38 day1 ## 93 1.94 day1 ## 94 2.29 day1 ## 95 1.59 day1 ## 96 2.46 day1 ## 97 1.67 day1 ## 98 2.02 day1 ## 99 1.50 day1 ## 100 2.70 day1 ## 101 1.61 day1 ## 102 2.29 day1 ## 103 0.97 day1 ## 104 1.85 day1 ## 105 2.76 day1 ## 106 1.64 day1 ## 107 1.17 day1 ## 108 1.57 day1 ## 109 2.23 day1 ## 110 2.05 day1 ## 111 2.05 day1 ## 112 2.94 day1 ## 113 2.39 day1 ## 114 1.94 day1 ## 115 2.12 day1 ## 116 1.11 day1 ## 117 0.97 day1 ## 118 1.35 day1 ## 119 2.81 day1 ## 120 2.50 day1 ## 121 1.87 day1 ## 122 1.33 day1 ## 123 1.26 day1 ## 124 1.44 day1 ## 125 0.55 day1 ## 126 1.75 day1 ## 127 2.08 day1 ## 128 0.85 day1 ## 129 2.52 day1 ## 130 3.00 day1 ## 131 1.41 day1 ## 132 1.08 day1 ## 133 1.20 day1 ## 134 1.94 day1 ## 135 2.26 day1 ## 136 1.41 day1 ## 137 2.50 day1 ## 138 2.17 day1 ## 139 1.82 day1 ## 140 1.44 day1 ## 141 1.66 day1 ## 142 1.82 day1 ## 143 1.26 day1 ## 144 2.67 day1 ## 145 1.47 day1 ## 146 1.84 day1 ## 147 2.58 day1 ## 148 1.73 day1 ## 149 1.23 day1 ## 150 2.32 day1 ## 151 2.67 day1 ## 152 1.02 day1 ## 153 1.66 day1 ## 154 1.88 day1 ## 155 1.91 day1 ## 156 1.64 day1 ## 157 1.34 day1 ## 158 1.85 day1 ## 159 2.08 day1 ## 160 1.02 day1 ## 161 1.79 day1 ## 162 1.94 day1 ## 163 3.26 day1 ## 164 1.14 day1 ## 165 1.50 day1 ## 166 2.03 day1 ## 167 2.24 day1 ## 168 1.11 day1 ## 169 2.21 day1 ## 170 1.94 day1 ## 171 2.41 day1 ## 172 0.88 day1 ## 173 1.17 day1 ## 174 2.23 day1 ## 175 1.64 day1 ## 176 2.14 day1 ## 177 0.11 day1 ## 178 2.17 day1 ## 179 1.67 day1 ## 180 1.00 day1 ## 181 0.88 day1 ## 182 2.20 day1 ## 183 2.17 day1 ## 184 2.32 day1 ## 185 1.64 day1 ## 186 3.00 day1 ## 187 2.38 day1 ## 188 1.60 day1 ## 189 1.58 day1 ## 190 2.61 day1 ## 191 1.44 day1 ## 192 1.57 day1 ## 193 2.32 day1 ## 194 1.14 day1 ## 195 1.93 day1 ## 196 2.47 day1 ## 197 2.29 day1 ## 198 1.00 day1 ## 199 1.58 day1 ## 200 2.44 day1 ## 201 0.83 day1 ## 202 2.71 day1 ## 203 1.73 day1 ## 204 1.58 day1 ## 205 1.50 day1 ## 206 1.05 day1 ## 207 2.05 day1 ## 208 2.63 day1 ## 209 2.55 day1 ## 210 2.00 day1 ## 211 2.00 day1 ## 212 1.32 day1 ## 213 3.14 day1 ## 214 1.44 day1 ## 215 1.85 day1 ## 216 1.41 day1 ## 217 1.94 day1 ## 218 2.91 day1 ## 219 1.85 day1 ## 220 1.70 day1 ## 221 2.23 day1 ## 222 1.11 day1 ## 223 1.47 day1 ## 224 2.20 day1 ## 225 1.82 day1 ## 226 1.42 day1 ## 227 2.44 day1 ## 228 2.66 day1 ## 229 1.52 day1 ## 230 1.35 day1 ## 231 1.29 day1 ## 232 2.32 day1 ## 233 0.78 day1 ## 234 2.84 day1 ## 235 0.97 day1 ## 236 1.52 day1 ## 237 1.70 day1 ## 238 0.94 day1 ## 239 1.41 day1 ## 240 1.79 day1 ## 241 1.08 day1 ## 242 1.47 day1 ## 243 1.79 day1 ## 244 2.00 day1 ## 245 0.76 day1 ## 246 2.20 day1 ## 247 0.94 day1 ## 248 1.38 day1 ## 249 1.38 day1 ## 250 0.32 day1 ## 251 2.58 day1 ## 252 0.51 day1 ## 253 0.32 day1 ## 254 0.91 day1 ## 255 1.51 day1 ## 256 1.47 day1 ## 257 2.50 day1 ## 258 2.26 day1 ## 259 2.81 day1 ## 260 1.87 day1 ## 261 2.00 day1 ## 262 2.23 day1 ## 263 2.00 day1 ## 264 1.41 day1 ## 265 1.64 day1 ## 266 1.64 day1 ## 267 1.26 day1 ## 268 1.52 day1 ## 269 2.44 day1 ## 270 2.18 day1 ## 271 3.02 day1 ## 272 1.02 day1 ## 273 2.88 day1 ## 274 1.54 day1 ## 275 1.64 day1 ## 276 2.44 day1 ## 277 1.29 day1 ## 278 1.61 day1 ## 279 1.77 day1 ## 280 0.91 day1 ## 281 0.85 day1 ## 282 0.85 day1 ## 283 1.50 day1 ## 284 1.05 day1 ## 285 3.38 day1 ## 286 1.42 day1 ## 287 1.85 day1 ## 288 1.91 day1 ## 289 0.82 day1 ## 290 1.32 day1 ## 291 2.23 day1 ## 292 1.47 day1 ## 293 2.70 day1 ## 294 1.58 day1 ## 295 1.00 day1 ## 296 1.44 day1 ## 297 2.00 day1 ## 298 1.60 day1 ## 299 2.32 day1 ## 300 3.41 day1 ## 301 2.02 day1 ## 302 0.64 day1 ## 303 3.58 day1 ## 304 1.50 day1 ## 305 1.08 day1 ## 306 1.52 day1 ## 307 1.26 day1 ## 308 1.68 day1 ## 309 1.47 day1 ## 310 1.47 day1 ## 311 1.67 day1 ## 312 2.47 day1 ## 313 1.82 day1 ## 314 2.17 day1 ## 315 3.21 day1 ## 316 1.60 day1 ## 317 0.32 day1 ## 318 0.55 day1 ## 319 1.42 day1 ## 320 1.14 day1 ## 321 2.64 day1 ## 322 2.58 day1 ## 323 2.02 day1 ## 324 2.00 day1 ## 325 2.90 day1 ## 326 1.82 day1 ## 327 0.50 day1 ## 328 1.53 day1 ## 329 2.48 day1 ## 330 2.05 day1 ## 331 2.52 day1 ## 332 1.88 day1 ## 333 2.73 day1 ## 334 2.88 day1 ## 335 1.67 day1 ## 336 1.93 day1 ## 337 1.67 day1 ## 338 1.20 day1 ## 339 2.75 day1 ## 340 1.94 day1 ## 341 0.59 day1 ## 342 1.50 day1 ## 343 1.58 day1 ## 344 2.23 day1 ## 345 2.35 day1 ## 346 2.55 day1 ## 347 1.55 day1 ## 348 2.31 day1 ## 349 2.23 day1 ## 350 0.67 day1 ## 351 2.51 day1 ## 352 1.08 day1 ## 353 2.44 day1 ## 354 0.23 day1 ## 355 2.17 day1 ## 356 1.90 day1 ## 357 1.67 day1 ## 358 2.00 day1 ## 359 2.44 day1 ## 360 1.44 day1 ## 361 0.82 day1 ## 362 2.50 day1 ## 363 1.82 day1 ## 364 1.97 day1 ## 365 2.52 day1 ## 366 0.05 day1 ## 367 2.08 day1 ## 368 2.39 day1 ## 369 1.45 day1 ## 370 2.58 day1 ## 371 2.12 day1 ## 372 2.02 day1 ## 373 1.78 day1 ## 374 0.73 day1 ## 375 2.26 day1 ## 376 2.79 day1 ## 377 0.43 day1 ## 378 0.52 day1 ## 379 2.32 day1 ## 380 2.22 day1 ## 381 0.58 day1 ## 382 2.00 day1 ## 383 0.70 day1 ## 384 1.00 day1 ## 385 0.30 day1 ## 386 1.52 day1 ## 387 1.58 day1 ## 388 2.34 day1 ## 389 0.79 day1 ## 390 2.26 day1 ## 391 2.35 day1 ## 392 1.70 day1 ## 393 3.09 day1 ## 394 1.52 day1 ## 395 0.35 day1 ## 396 2.70 day1 ## 397 1.64 day1 ## 398 0.82 day1 ## 399 2.73 day1 ## 400 2.23 day1 ## 401 1.06 day1 ## 402 2.05 day1 ## 403 1.73 day1 ## 404 0.93 day1 ## 405 2.50 day1 ## 406 1.44 day1 ## 407 2.88 day1 ## 408 0.67 day1 ## 409 1.85 day1 ## 410 1.21 day1 ## 411 1.06 day1 ## 412 0.61 day1 ## 413 2.00 day1 ## 414 1.17 day1 ## 415 1.48 day1 ## 416 1.55 day1 ## 417 3.29 day1 ## 418 1.47 day1 ## 419 0.96 day1 ## 420 1.00 day1 ## 421 1.47 day1 ## 422 2.55 day1 ## 423 0.44 day1 ## 424 2.35 day1 ## 425 1.71 day1 ## 426 1.84 day1 ## 427 1.11 day1 ## 428 1.38 day1 ## 429 0.88 day1 ## 430 0.94 day1 ## 431 1.91 day1 ## 432 2.76 day1 ## 433 1.55 day1 ## 434 2.67 day1 ## 435 1.03 day1 ## 436 2.50 day1 ## 437 1.64 day1 ## 438 2.26 day1 ## 439 2.14 day1 ## 440 0.52 day1 ## 441 1.08 day1 ## 442 1.69 day1 ## 443 2.73 day1 ## 444 1.91 day1 ## 445 1.73 day1 ## 446 3.21 day1 ## 447 2.11 day1 ## 448 2.05 day1 ## 449 2.17 day1 ## 450 2.17 day1 ## 451 2.30 day1 ## 452 2.56 day1 ## 453 2.11 day1 ## 454 1.70 day1 ## 455 1.23 day1 ## 456 3.20 day1 ## 457 2.02 day1 ## 458 2.64 day1 ## 459 2.52 day1 ## 460 1.61 day1 ## 461 1.50 day1 ## 462 1.15 day1 ## 463 1.82 day1 ## 464 1.50 day1 ## 465 2.32 day1 ## 466 2.92 day1 ## 467 1.41 day1 ## 468 1.35 day1 ## 469 0.61 day1 ## 470 0.73 day1 ## 471 2.23 day1 ## 472 1.32 day1 ## 473 2.94 day1 ## 474 1.61 day1 ## 475 1.00 day1 ## 476 3.15 day1 ## 477 2.88 day1 ## 478 2.09 day1 ## 479 1.32 day1 ## 480 1.47 day1 ## 481 1.61 day1 ## 482 2.20 day1 ## 483 2.78 day1 ## 484 2.06 day1 ## 485 0.47 day1 ## 486 2.87 day1 ## 487 1.14 day1 ## 488 3.32 day1 ## 489 2.08 day1 ## 490 2.38 day1 ## 491 2.08 day1 ## 492 1.85 day1 ## 493 1.38 day1 ## 494 1.14 day1 ## 495 1.58 day1 ## 496 1.23 day1 ## 497 2.53 day1 ## 498 0.67 day1 ## 499 0.73 day1 ## 500 1.34 day1 ## 501 2.14 day1 ## 502 1.00 day1 ## 503 1.35 day1 ## 504 1.94 day1 ## 505 0.50 day1 ## 506 3.08 day1 ## 507 2.88 day1 ## 508 1.91 day1 ## 509 1.41 day1 ## 510 2.02 day1 ## 511 0.76 day1 ## 512 1.94 day1 ## 513 0.67 day1 ## 514 2.41 day1 ## 515 2.17 day1 ## 516 2.67 day1 ## 517 1.94 day1 ## 518 2.05 day1 ## 519 2.17 day1 ## 520 0.47 day1 ## 521 0.62 day1 ## 522 2.00 day1 ## 523 0.45 day1 ## 524 2.29 day1 ## 525 2.55 day1 ## 526 0.82 day1 ## 527 3.12 day1 ## 528 2.50 day1 ## 529 1.79 day1 ## 530 2.28 day1 ## 531 0.58 day1 ## 532 2.50 day1 ## 533 1.41 day1 ## 534 2.14 day1 ## 535 0.76 day1 ## 536 1.79 day1 ## 537 1.02 day1 ## 538 2.62 day1 ## 539 0.88 day1 ## 540 1.58 day1 ## 541 2.20 day1 ## 542 1.14 day1 ## 543 1.47 day1 ## 544 1.41 day1 ## 545 1.44 day1 ## 546 1.23 day1 ## 547 1.82 day1 ## 548 2.44 day1 ## 549 1.94 day1 ## 550 2.41 day1 ## 551 2.27 day1 ## 552 1.79 day1 ## 553 1.88 day1 ## 554 1.85 day1 ## 555 2.21 day1 ## 556 1.97 day1 ## 557 2.51 day1 ## 558 2.05 day1 ## 559 1.29 day1 ## 560 2.05 day1 ## 561 2.23 day1 ## 562 1.76 day1 ## 563 1.05 day1 ## 564 1.79 day1 ## 565 1.02 day1 ## 566 2.76 day1 ## 567 1.67 day1 ## 568 2.85 day1 ## 569 0.23 day1 ## 570 1.90 day1 ## 571 1.23 day1 ## 572 1.97 day1 ## 573 1.50 day1 ## 574 3.69 day1 ## 575 0.50 day1 ## 576 2.18 day1 ## 577 2.17 day1 ## 578 1.58 day1 ## 579 2.88 day1 ## 580 2.52 day1 ## 581 2.20 day1 ## 582 1.73 day1 ## 583 2.23 day1 ## 584 1.97 day1 ## 585 1.20 day1 ## 586 2.00 day1 ## 587 1.91 day1 ## 588 0.81 day1 ## 589 1.31 day1 ## 590 0.38 day1 ## 591 1.97 day1 ## 592 0.38 day1 ## 593 2.11 day1 ## 594 3.20 day1 ## 595 0.02 day1 ## 596 2.56 day1 ## 597 2.02 day1 ## 598 2.30 day1 ## 599 2.02 day1 ## 600 2.05 day1 ## 601 1.70 day1 ## 602 1.61 day1 ## 603 0.73 day1 ## 604 2.50 day1 ## 605 2.18 day1 ## 606 2.46 day1 ## 607 1.50 day1 ## 608 1.73 day1 ## 609 1.44 day1 ## 610 1.64 day1 ## 611 20.02 day1 ## 612 1.20 day1 ## 613 0.38 day1 ## 614 1.58 day1 ## 615 1.67 day1 ## 616 1.00 day1 ## 617 2.58 day1 ## 618 2.82 day1 ## 619 2.29 day1 ## 620 1.14 day1 ## 621 1.64 day1 ## 622 1.82 day1 ## 623 3.32 day1 ## 624 3.32 day1 ## 625 1.85 day1 ## 626 2.29 day1 ## 627 1.47 day1 ## 628 2.08 day1 ## 629 2.20 day1 ## 630 1.06 day1 ## 631 0.97 day1 ## 632 2.00 day1 ## 633 1.67 day1 ## 634 2.94 day1 ## 635 1.55 day1 ## 636 0.88 day1 ## 637 1.35 day1 ## 638 0.61 day1 ## 639 1.00 day1 ## 640 1.52 day1 ## 641 1.00 day1 ## 642 1.76 day1 ## 643 2.52 day1 ## 644 2.00 day1 ## 645 2.63 day1 ## 646 0.73 day1 ## 647 1.58 day1 ## 648 0.58 day1 ## 649 1.67 day1 ## 650 1.47 day1 ## 651 1.81 day1 ## 652 1.91 day1 ## 653 1.06 day1 ## 654 1.47 day1 ## 655 2.52 day1 ## 656 1.85 day1 ## 657 3.44 day1 ## 658 1.55 day1 ## 659 2.29 day1 ## 660 1.76 day1 ## 661 1.90 day1 ## 662 2.52 day1 ## 663 2.52 day1 ## 664 2.82 day1 ## 665 2.02 day1 ## 666 1.29 day1 ## 667 1.26 day1 ## 668 0.94 day1 ## 669 2.00 day1 ## 670 0.73 day1 ## 671 2.26 day1 ## 672 2.23 day1 ## 673 2.35 day1 ## 674 0.55 day1 ## 675 1.85 day1 ## 676 0.67 day1 ## 677 1.85 day1 ## 678 1.23 day1 ## 679 2.35 day1 ## 680 1.35 day1 ## 681 1.94 day1 ## 682 1.55 day1 ## 683 1.29 day1 ## 684 2.17 day1 ## 685 1.91 day1 ## 686 2.88 day1 ## 687 2.36 day1 ## 688 2.36 day1 ## 689 2.20 day1 ## 690 2.17 day1 ## 691 0.52 day1 ## 692 0.32 day1 ## 693 1.52 day1 ## 694 2.00 day1 ## 695 1.32 day1 ## 696 2.05 day1 ## 697 1.73 day1 ## 698 1.94 day1 ## 699 1.81 day1 ## 700 0.90 day1 ## 701 1.58 day1 ## 702 2.29 day1 ## 703 2.57 day1 ## 704 1.58 day1 ## 705 2.33 day1 ## 706 3.15 day1 ## 707 2.29 day1 ## 708 0.82 day1 ## 709 1.93 day1 ## 710 1.82 day1 ## 711 1.96 day1 ## 712 1.32 day1 ## 713 1.02 day1 ## 714 1.14 day1 ## 715 2.32 day1 ## 716 2.16 day1 ## 717 2.42 day1 ## 718 1.14 day1 ## 719 1.55 day1 ## 720 1.17 day1 ## 721 1.00 day1 ## 722 1.05 day1 ## 723 1.38 day1 ## 724 1.93 day1 ## 725 2.73 day1 ## 726 2.02 day1 ## 727 2.81 day1 ## 728 2.47 day1 ## 729 1.35 day1 ## 730 2.08 day1 ## 731 2.50 day1 ## 732 2.45 day1 ## 733 2.17 day1 ## 734 1.70 day1 ## 735 0.70 day1 ## 736 1.51 day1 ## 737 1.23 day1 ## 738 2.14 day1 ## 739 1.14 day1 ## 740 0.96 day1 ## 741 1.52 day1 ## 742 0.52 day1 ## 743 1.56 day1 ## 744 3.29 day1 ## 745 0.45 day1 ## 746 2.63 day1 ## 747 1.70 day1 ## 748 3.11 day1 ## 749 1.82 day1 ## 750 1.58 day1 ## 751 2.73 day1 ## 752 1.50 day1 ## 753 1.78 day1 ## 754 2.02 day1 ## 755 0.67 day1 ## 756 1.41 day1 ## 757 0.90 day1 ## 758 1.23 day1 ## 759 2.70 day1 ## 760 1.97 day1 ## 761 0.84 day1 ## 762 1.79 day1 ## 763 2.84 day1 ## 764 2.02 day1 ## 765 1.64 day1 ## 766 1.08 day1 ## 767 2.97 day1 ## 768 0.94 day1 ## 769 2.97 day1 ## 770 0.97 day1 ## 771 1.47 day1 ## 772 2.61 day1 ## 773 1.73 day1 ## 774 3.38 day1 ## 775 3.17 day1 ## 776 2.20 day1 ## 777 2.14 day1 ## 778 1.29 day1 ## 779 3.21 day1 ## 780 2.67 day1 ## 781 1.85 day1 ## 782 1.35 day1 ## 783 2.14 day1 ## 784 1.24 day1 ## 785 2.02 day1 ## 786 2.32 day1 ## 787 1.08 day1 ## 788 1.14 day1 ## 789 2.14 day1 ## 790 2.88 day1 ## 791 1.35 day1 ## 792 1.00 day1 ## 793 2.02 day1 ## 794 0.64 day1 ## 795 0.29 day1 ## 796 1.73 day1 ## 797 1.82 day1 ## 798 2.11 day1 ## 799 1.23 day1 ## 800 0.64 day1 ## 801 2.23 day1 ## 802 2.44 day1 ## 803 1.17 day1 ## 804 0.61 day1 ## 805 0.52 day1 ## 806 2.91 day1 ## 807 2.61 day1 ## 808 1.47 day1 ## 809 1.28 day1 ## 810 1.26 day1 ## 811 1.35 day2 ## 812 1.41 day2 ## 813 NA day2 ## 814 NA day2 ## 815 0.08 day2 ## 816 NA day2 ## 817 NA day2 ## 818 NA day2 ## 819 NA day2 ## 820 0.44 day2 ## 821 NA day2 ## 822 0.20 day2 ## 823 NA day2 ## 824 1.64 day2 ## 825 0.02 day2 ## 826 NA day2 ## 827 NA day2 ## 828 2.05 day2 ## 829 NA day2 ## 830 NA day2 ## 831 0.70 day2 ## 832 NA day2 ## 833 NA day2 ## 834 0.85 day2 ## 835 NA day2 ## 836 NA day2 ## 837 NA day2 ## 838 NA day2 ## 839 NA day2 ## 840 0.38 day2 ## 841 0.11 day2 ## 842 NA day2 ## 843 NA day2 ## 844 NA day2 ## 845 0.82 day2 ## 846 NA day2 ## 847 0.91 day2 ## 848 NA day2 ## 849 NA day2 ## 850 NA day2 ## 851 NA day2 ## 852 NA day2 ## 853 NA day2 ## 854 NA day2 ## 855 NA day2 ## 856 NA day2 ## 857 NA day2 ## 858 0.38 day2 ## 859 NA day2 ## 860 NA day2 ## 861 NA day2 ## 862 NA day2 ## 863 NA day2 ## 864 0.32 day2 ## 865 0.23 day2 ## 866 NA day2 ## 867 NA day2 ## 868 NA day2 ## 869 NA day2 ## 870 0.14 day2 ## 871 NA day2 ## 872 NA day2 ## 873 NA day2 ## 874 1.90 day2 ## 875 NA day2 ## 876 NA day2 ## 877 0.76 day2 ## 878 NA day2 ## 879 0.70 day2 ## 880 0.55 day2 ## 881 NA day2 ## 882 0.38 day2 ## 883 NA day2 ## 884 NA day2 ## 885 NA day2 ## 886 NA day2 ## 887 1.18 day2 ## 888 0.79 day2 ## 889 NA day2 ## 890 NA day2 ## 891 NA day2 ## 892 NA day2 ## 893 NA day2 ## 894 2.08 day2 ## 895 1.00 day2 ## 896 NA day2 ## 897 NA day2 ## 898 NA day2 ## 899 NA day2 ## 900 NA day2 ## 901 NA day2 ## 902 NA day2 ## 903 NA day2 ## 904 NA day2 ## 905 NA day2 ## 906 NA day2 ## 907 0.14 day2 ## 908 0.58 day2 ## 909 NA day2 ## 910 1.70 day2 ## 911 NA day2 ## 912 NA day2 ## 913 1.06 day2 ## 914 NA day2 ## 915 NA day2 ## 916 NA day2 ## 917 NA day2 ## 918 NA day2 ## 919 NA day2 ## 920 1.58 day2 ## 921 NA day2 ## 922 NA day2 ## 923 NA day2 ## 924 NA day2 ## 925 NA day2 ## 926 NA day2 ## 927 NA day2 ## 928 NA day2 ## 929 2.08 day2 ## 930 NA day2 ## 931 NA day2 ## 932 NA day2 ## 933 NA day2 ## 934 NA day2 ## 935 NA day2 ## 936 NA day2 ## 937 NA day2 ## 938 NA day2 ## 939 NA day2 ## 940 NA day2 ## 941 NA day2 ## 942 NA day2 ## 943 1.38 day2 ## 944 1.44 day2 ## 945 1.73 day2 ## 946 NA day2 ## 947 NA day2 ## 948 NA day2 ## 949 1.11 day2 ## 950 1.14 day2 ## 951 NA day2 ## 952 NA day2 ## 953 NA day2 ## 954 NA day2 ## 955 NA day2 ## 956 NA day2 ## 957 NA day2 ## 958 NA day2 ## 959 NA day2 ## 960 NA day2 ## 961 NA day2 ## 962 NA day2 ## 963 2.12 day2 ## 964 NA day2 ## 965 NA day2 ## 966 NA day2 ## 967 NA day2 ## 968 NA day2 ## 969 NA day2 ## 970 NA day2 ## 971 NA day2 ## 972 NA day2 ## 973 1.97 day2 ## 974 0.58 day2 ## 975 0.70 day2 ## 976 NA day2 ## 977 NA day2 ## 978 NA day2 ## 979 NA day2 ## 980 NA day2 ## 981 NA day2 ## 982 NA day2 ## 983 1.35 day2 ## 984 NA day2 ## 985 NA day2 ## 986 NA day2 ## 987 0.29 day2 ## 988 NA day2 ## 989 NA day2 ## 990 NA day2 ## 991 NA day2 ## 992 NA day2 ## 993 NA day2 ## 994 NA day2 ## 995 NA day2 ## 996 NA day2 ## 997 0.85 day2 ## 998 1.02 day2 ## 999 NA day2 ## 1000 NA day2 ## 1001 0.05 day2 ## 1002 NA day2 ## 1003 NA day2 ## 1004 NA day2 ## 1005 NA day2 ## 1006 NA day2 ## 1007 NA day2 ## 1008 NA day2 ## 1009 NA day2 ## 1010 NA day2 ## 1011 NA day2 ## 1012 0.78 day2 ## 1013 NA day2 ## 1014 NA day2 ## 1015 NA day2 ## 1016 NA day2 ## 1017 NA day2 ## 1018 NA day2 ## 1019 2.29 day2 ## 1020 NA day2 ## 1021 NA day2 ## 1022 NA day2 ## 1023 NA day2 ## 1024 NA day2 ## 1025 0.23 day2 ## 1026 0.44 day2 ## 1027 NA day2 ## 1028 NA day2 ## 1029 NA day2 ## 1030 NA day2 ## 1031 NA day2 ## 1032 NA day2 ## 1033 NA day2 ## 1034 NA day2 ## 1035 NA day2 ## 1036 NA day2 ## 1037 NA day2 ## 1038 NA day2 ## 1039 NA day2 ## 1040 0.47 day2 ## 1041 NA day2 ## 1042 NA day2 ## 1043 NA day2 ## 1044 NA day2 ## 1045 NA day2 ## 1046 NA day2 ## 1047 NA day2 ## 1048 1.17 day2 ## 1049 NA day2 ## 1050 NA day2 ## 1051 0.44 day2 ## 1052 NA day2 ## 1053 0.47 day2 ## 1054 NA day2 ## 1055 NA day2 ## 1056 NA day2 ## 1057 0.17 day2 ## 1058 NA day2 ## 1059 0.85 day2 ## 1060 NA day2 ## 1061 NA day2 ## 1062 NA day2 ## 1063 NA day2 ## 1064 1.11 day2 ## 1065 NA day2 ## 1066 NA day2 ## 1067 NA day2 ## 1068 NA day2 ## 1069 NA day2 ## 1070 NA day2 ## 1071 NA day2 ## 1072 0.41 day2 ## 1073 0.76 day2 ## 1074 NA day2 ## 1075 NA day2 ## 1076 NA day2 ## 1077 NA day2 ## 1078 0.55 day2 ## 1079 1.02 day2 ## 1080 NA day2 ## 1081 NA day2 ## 1082 NA day2 ## 1083 NA day2 ## 1084 NA day2 ## 1085 NA day2 ## 1086 2.50 day2 ## 1087 NA day2 ## 1088 0.32 day2 ## 1089 NA day2 ## 1090 0.17 day2 ## 1091 0.20 day2 ## 1092 0.52 day2 ## 1093 NA day2 ## 1094 0.23 day2 ## 1095 NA day2 ## 1096 0.52 day2 ## 1097 NA day2 ## 1098 0.84 day2 ## 1099 0.26 day2 ## 1100 0.76 day2 ## 1101 0.85 day2 ## 1102 1.52 day2 ## 1103 NA day2 ## 1104 NA day2 ## 1105 NA day2 ## 1106 NA day2 ## 1107 NA day2 ## 1108 NA day2 ## 1109 2.53 day2 ## 1110 NA day2 ## 1111 NA day2 ## 1112 0.52 day2 ## 1113 3.35 day2 ## 1114 NA day2 ## 1115 NA day2 ## 1116 NA day2 ## 1117 NA day2 ## 1118 NA day2 ## 1119 1.08 day2 ## 1120 NA day2 ## 1121 1.55 day2 ## 1122 1.97 day2 ## 1123 NA day2 ## 1124 NA day2 ## 1125 NA day2 ## 1126 1.38 day2 ## 1127 NA day2 ## 1128 NA day2 ## 1129 NA day2 ## 1130 NA day2 ## 1131 NA day2 ## 1132 NA day2 ## 1133 NA day2 ## 1134 NA day2 ## 1135 NA day2 ## 1136 NA day2 ## 1137 NA day2 ## 1138 NA day2 ## 1139 NA day2 ## 1140 NA day2 ## 1141 NA day2 ## 1142 NA day2 ## 1143 NA day2 ## 1144 NA day2 ## 1145 NA day2 ## 1146 NA day2 ## 1147 NA day2 ## 1148 NA day2 ## 1149 NA day2 ## 1150 0.97 day2 ## 1151 NA day2 ## 1152 NA day2 ## 1153 0.94 day2 ## 1154 0.11 day2 ## 1155 NA day2 ## 1156 0.82 day2 ## 1157 NA day2 ## 1158 NA day2 ## 1159 NA day2 ## 1160 0.50 day2 ## 1161 NA day2 ## 1162 0.58 day2 ## 1163 NA day2 ## 1164 0.14 day2 ## 1165 NA day2 ## 1166 1.17 day2 ## 1167 0.44 day2 ## 1168 0.58 day2 ## 1169 NA day2 ## 1170 NA day2 ## 1171 NA day2 ## 1172 NA day2 ## 1173 NA day2 ## 1174 NA day2 ## 1175 NA day2 ## 1176 NA day2 ## 1177 NA day2 ## 1178 NA day2 ## 1179 0.82 day2 ## 1180 NA day2 ## 1181 NA day2 ## 1182 0.76 day2 ## 1183 1.14 day2 ## 1184 0.17 day2 ## 1185 0.90 day2 ## 1186 NA day2 ## 1187 0.67 day2 ## 1188 0.38 day2 ## 1189 NA day2 ## 1190 NA day2 ## 1191 NA day2 ## 1192 NA day2 ## 1193 NA day2 ## 1194 NA day2 ## 1195 NA day2 ## 1196 NA day2 ## 1197 0.35 day2 ## 1198 NA day2 ## 1199 NA day2 ## 1200 NA day2 ## 1201 NA day2 ## 1202 NA day2 ## 1203 NA day2 ## 1204 NA day2 ## 1205 NA day2 ## 1206 NA day2 ## 1207 NA day2 ## 1208 NA day2 ## 1209 NA day2 ## 1210 NA day2 ## 1211 NA day2 ## 1212 0.20 day2 ## 1213 1.44 day2 ## 1214 0.91 day2 ## 1215 2.44 day2 ## 1216 NA day2 ## 1217 NA day2 ## 1218 0.23 day2 ## 1219 0.35 day2 ## 1220 0.79 day2 ## 1221 0.76 day2 ## 1222 0.26 day2 ## 1223 NA day2 ## 1224 0.73 day2 ## 1225 0.79 day2 ## 1226 NA day2 ## 1227 NA day2 ## 1228 NA day2 ## 1229 NA day2 ## 1230 1.11 day2 ## 1231 NA day2 ## 1232 2.38 day2 ## 1233 0.06 day2 ## 1234 2.41 day2 ## 1235 0.85 day2 ## 1236 0.58 day2 ## 1237 0.23 day2 ## 1238 NA day2 ## 1239 NA day2 ## 1240 NA day2 ## 1241 NA day2 ## 1242 NA day2 ## 1243 0.32 day2 ## 1244 NA day2 ## 1245 0.29 day2 ## 1246 NA day2 ## 1247 NA day2 ## 1248 NA day2 ## 1249 NA day2 ## 1250 NA day2 ## 1251 NA day2 ## 1252 NA day2 ## 1253 NA day2 ## 1254 NA day2 ## 1255 NA day2 ## 1256 NA day2 ## 1257 NA day2 ## 1258 NA day2 ## 1259 NA day2 ## 1260 NA day2 ## 1261 NA day2 ## 1262 NA day2 ## 1263 0.41 day2 ## 1264 NA day2 ## 1265 NA day2 ## 1266 NA day2 ## 1267 NA day2 ## 1268 NA day2 ## 1269 0.14 day2 ## 1270 NA day2 ## 1271 1.20 day2 ## 1272 0.45 day2 ## 1273 NA day2 ## 1274 NA day2 ## 1275 NA day2 ## 1276 NA day2 ## 1277 NA day2 ## 1278 NA day2 ## 1279 0.14 day2 ## 1280 NA day2 ## 1281 1.88 day2 ## 1282 0.91 day2 ## 1283 1.79 day2 ## 1284 NA day2 ## 1285 NA day2 ## 1286 3.00 day2 ## 1287 NA day2 ## 1288 1.21 day2 ## 1289 1.70 day2 ## 1290 0.35 day2 ## 1291 NA day2 ## 1292 1.50 day2 ## 1293 NA day2 ## 1294 NA day2 ## 1295 NA day2 ## 1296 NA day2 ## 1297 NA day2 ## 1298 3.21 day2 ## 1299 1.38 day2 ## 1300 2.50 day2 ## 1301 NA day2 ## 1302 NA day2 ## 1303 NA day2 ## 1304 NA day2 ## 1305 NA day2 ## 1306 0.70 day2 ## 1307 NA day2 ## 1308 NA day2 ## 1309 NA day2 ## 1310 NA day2 ## 1311 0.70 day2 ## 1312 NA day2 ## 1313 NA day2 ## 1314 0.79 day2 ## 1315 NA day2 ## 1316 NA day2 ## 1317 NA day2 ## 1318 NA day2 ## 1319 NA day2 ## 1320 NA day2 ## 1321 NA day2 ## 1322 NA day2 ## 1323 0.28 day2 ## 1324 NA day2 ## 1325 NA day2 ## 1326 0.41 day2 ## 1327 0.64 day2 ## 1328 0.85 day2 ## 1329 NA day2 ## 1330 NA day2 ## 1331 0.76 day2 ## 1332 NA day2 ## 1333 NA day2 ## 1334 0.91 day2 ## 1335 NA day2 ## 1336 NA day2 ## 1337 2.20 day2 ## 1338 2.23 day2 ## 1339 NA day2 ## 1340 NA day2 ## 1341 1.05 day2 ## 1342 1.29 day2 ## 1343 NA day2 ## 1344 NA day2 ## 1345 0.26 day2 ## 1346 1.11 day2 ## 1347 0.35 day2 ## 1348 NA day2 ## 1349 NA day2 ## 1350 0.20 day2 ## 1351 NA day2 ## 1352 NA day2 ## 1353 0.52 day2 ## 1354 0.23 day2 ## 1355 1.76 day2 ## 1356 1.17 day2 ## 1357 NA day2 ## 1358 NA day2 ## 1359 1.20 day2 ## 1360 NA day2 ## 1361 NA day2 ## 1362 0.23 day2 ## 1363 NA day2 ## 1364 0.64 day2 ## 1365 NA day2 ## 1366 1.94 day2 ## 1367 NA day2 ## 1368 NA day2 ## 1369 1.00 day2 ## 1370 NA day2 ## 1371 NA day2 ## 1372 NA day2 ## 1373 NA day2 ## 1374 NA day2 ## 1375 0.73 day2 ## 1376 1.58 day2 ## 1377 0.55 day2 ## 1378 NA day2 ## 1379 0.84 day2 ## 1380 NA day2 ## 1381 0.52 day2 ## 1382 NA day2 ## 1383 NA day2 ## 1384 NA day2 ## 1385 NA day2 ## 1386 NA day2 ## 1387 NA day2 ## 1388 NA day2 ## 1389 NA day2 ## 1390 NA day2 ## 1391 NA day2 ## 1392 NA day2 ## 1393 NA day2 ## 1394 NA day2 ## 1395 0.67 day2 ## 1396 NA day2 ## 1397 NA day2 ## 1398 NA day2 ## 1399 NA day2 ## 1400 NA day2 ## 1401 NA day2 ## 1402 NA day2 ## 1403 0.76 day2 ## 1404 NA day2 ## 1405 NA day2 ## 1406 NA day2 ## 1407 NA day2 ## 1408 NA day2 ## 1409 NA day2 ## 1410 NA day2 ## 1411 NA day2 ## 1412 NA day2 ## 1413 NA day2 ## 1414 1.64 day2 ## 1415 1.75 day2 ## 1416 1.08 day2 ## 1417 0.91 day2 ## 1418 0.94 day2 ## 1419 NA day2 ## 1420 0.32 day2 ## 1421 2.44 day2 ## 1422 0.17 day2 ## 1423 0.02 day2 ## 1424 1.54 day2 ## 1425 0.50 day2 ## 1426 0.48 day2 ## 1427 1.35 day2 ## 1428 2.61 day2 ## 1429 2.05 day2 ## 1430 NA day2 ## 1431 0.76 day2 ## 1432 0.08 day2 ## 1433 2.91 day2 ## 1434 NA day2 ## 1435 1.00 day2 ## 1436 NA day2 ## 1437 0.47 day2 ## 1438 0.70 day2 ## 1439 NA day2 ## 1440 1.45 day2 ## 1441 0.14 day2 ## 1442 NA day2 ## 1443 0.38 day2 ## 1444 NA day2 ## 1445 NA day2 ## 1446 0.26 day2 ## 1447 2.32 day2 ## 1448 0.20 day2 ## 1449 NA day2 ## 1450 2.72 day2 ## 1451 NA day2 ## 1452 0.41 day2 ## 1453 NA day2 ## 1454 0.88 day2 ## 1455 NA day2 ## 1456 0.85 day2 ## 1457 0.23 day2 ## 1458 NA day2 ## 1459 NA day2 ## 1460 NA day2 ## 1461 NA day2 ## 1462 NA day2 ## 1463 NA day2 ## 1464 1.23 day2 ## 1465 NA day2 ## 1466 0.20 day2 ## 1467 NA day2 ## 1468 1.32 day2 ## 1469 2.70 day2 ## 1470 NA day2 ## 1471 NA day2 ## 1472 2.55 day2 ## 1473 NA day2 ## 1474 0.17 day2 ## 1475 NA day2 ## 1476 NA day2 ## 1477 NA day2 ## 1478 NA day2 ## 1479 1.13 day2 ## 1480 NA day2 ## 1481 0.79 day2 ## 1482 NA day2 ## 1483 NA day2 ## 1484 0.38 day2 ## 1485 NA day2 ## 1486 NA day2 ## 1487 1.00 day2 ## 1488 0.20 day2 ## 1489 NA day2 ## 1490 NA day2 ## 1491 NA day2 ## 1492 NA day2 ## 1493 NA day2 ## 1494 NA day2 ## 1495 NA day2 ## 1496 NA day2 ## 1497 NA day2 ## 1498 NA day2 ## 1499 NA day2 ## 1500 0.47 day2 ## 1501 NA day2 ## 1502 NA day2 ## 1503 0.55 day2 ## 1504 0.94 day2 ## 1505 1.02 day2 ## 1506 NA day2 ## 1507 NA day2 ## 1508 NA day2 ## 1509 NA day2 ## 1510 0.64 day2 ## 1511 0.67 day2 ## 1512 1.87 day2 ## 1513 NA day2 ## 1514 NA day2 ## 1515 0.82 day2 ## 1516 NA day2 ## 1517 NA day2 ## 1518 NA day2 ## 1519 NA day2 ## 1520 NA day2 ## 1521 NA day2 ## 1522 0.64 day2 ## 1523 NA day2 ## 1524 NA day2 ## 1525 NA day2 ## 1526 NA day2 ## 1527 1.70 day2 ## 1528 NA day2 ## 1529 0.79 day2 ## 1530 NA day2 ## 1531 0.58 day2 ## 1532 0.11 day2 ## 1533 NA day2 ## 1534 2.42 day2 ## 1535 NA day2 ## 1536 NA day2 ## 1537 NA day2 ## 1538 NA day2 ## 1539 NA day2 ## 1540 NA day2 ## 1541 NA day2 ## 1542 NA day2 ## 1543 NA day2 ## 1544 0.00 day2 ## 1545 0.23 day2 ## 1546 NA day2 ## 1547 NA day2 ## 1548 0.85 day2 ## 1549 1.14 day2 ## 1550 NA day2 ## 1551 1.14 day2 ## 1552 NA day2 ## 1553 NA day2 ## 1554 0.26 day2 ## 1555 NA day2 ## 1556 NA day2 ## 1557 NA day2 ## 1558 NA day2 ## 1559 NA day2 ## 1560 0.14 day2 ## 1561 NA day2 ## 1562 1.14 day2 ## 1563 1.02 day2 ## 1564 NA day2 ## 1565 0.94 day2 ## 1566 0.55 day2 ## 1567 NA day2 ## 1568 1.11 day2 ## 1569 NA day2 ## 1570 NA day2 ## 1571 NA day2 ## 1572 NA day2 ## 1573 NA day2 ## 1574 NA day2 ## 1575 0.70 day2 ## 1576 NA day2 ## 1577 NA day2 ## 1578 NA day2 ## 1579 1.94 day2 ## 1580 NA day2 ## 1581 NA day2 ## 1582 NA day2 ## 1583 0.20 day2 ## 1584 3.44 day2 ## 1585 1.00 day2 ## 1586 0.91 day2 ## 1587 NA day2 ## 1588 1.58 day2 ## 1589 2.85 day2 ## 1590 NA day2 ## 1591 0.79 day2 ## 1592 NA day2 ## 1593 0.76 day2 ## 1594 0.56 day2 ## 1595 1.78 day2 ## 1596 NA day2 ## 1597 0.23 day2 ## 1598 1.35 day2 ## 1599 1.82 day2 ## 1600 NA day2 ## 1601 0.17 day2 ## 1602 1.70 day2 ## 1603 NA day2 ## 1604 1.32 day2 ## 1605 0.14 day2 ## 1606 0.94 day2 ## 1607 1.52 day2 ## 1608 NA day2 ## 1609 NA day2 ## 1610 NA day2 ## 1611 1.41 day2 ## 1612 0.32 day2 ## 1613 0.58 day2 ## 1614 0.44 day2 ## 1615 NA day2 ## 1616 0.94 day2 ## 1617 1.44 day2 ## 1618 NA day2 ## 1619 NA day2 ## 1620 NA day2 bartlett.test(values~ind, data=dlfsub1) ## ## Bartlett test of homogeneity of variances ## ## data: values by ind ## Bartlett&#39;s K-squared = 26.273, df = 1, p-value = 2.964e-07 8.2.3 Brown-Forsythe test of equality of variance The Brown-Forsythe test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group medians. The test is considered to be robust against departures from normality and outliers. Brown, M. B., Forsythe. A. B. (1974a). The small sample behavior of some statistics which test the equality of several means. Technometrics, 16, 129-132. Dag, O., Dolgun, A., Konar, N.M. (2018). onewaytests: An R Package for One-Way Tests in Independent Groups Designs. The R Journal, 10:1, 175-199. library(onewaytests) # install and activate the package ## ## Attaching package: &#39;onewaytests&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## describe bf.test(values~ind, data=dlfsub1) ## ## Brown-Forsythe Test (alpha = 0.05) ## ------------------------------------------------------------- ## data : values and ind ## ## statistic : 225.7812 ## num df : 1 ## denom df : 580.6262 ## p.value : 2.392677e-43 ## ## Result : Difference is statistically significant. ## ------------------------------------------------------------- 8.2.4 Levene’s Test: testing for assumption of normality among groups Levene’s test is a test of the null hypothesis that the variances in the different groups are equal. The test is based on the absolute deviations of the observations from the group means. The test is considered to be robust against departures from normality, but it is sensitive to the presence of outliers. leveneTest(dlf$day1, dlf$day2) ## Warning in leveneTest.default(dlf$day1, dlf$day2): dlf$day2 coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 101 11.661 &lt; 2.2e-16 *** ## 162 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ANOTHER alternative Levene’s test #Levene&#39;s test for comparison of variances of exam scores in the two universities. library(ggplot2) library(car) library(readr) RExam &lt;- read_csv(&quot;Data/RExam.csv&quot;) ## Rows: 100 Columns: 5 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (5): exam, computer, lectures, numeracy, uni ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. rexam=RExam head(rexam) ## # A tibble: 6 × 5 ## exam computer lectures numeracy uni ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 18 54 75 7 0 ## 2 30 47 8.5 1 0 ## 3 40 58 69.5 6 0 ## 4 30 37 67 6 0 ## 5 40 53 44.5 2 0 ## 6 15 48 76.5 8 0 ggplot(rexam, aes(numeracy, fill=as.factor(uni)))+ geom_histogram()+ facet_wrap(~uni) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. leveneTest(rexam$lectures, rexam$uni, center=median) # using the median as the center ## Warning in leveneTest.default(rexam$lectures, rexam$uni, center = median): ## rexam$uni coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 1.4222 0.2359 ## 98 leveneTest(rexam$lectures, rexam$uni, center = mean) # using the mean as the center ## Warning in leveneTest.default(rexam$lectures, rexam$uni, center = mean): ## rexam$uni coerced to factor. ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 1 1.7306 0.1914 ## 98 "],["non-parametric-tests.html", "Chapter 9 Non-parametric tests 9.1 Spearman vs Kendall correlations comparison", " Chapter 9 Non-parametric tests What is a non-parametric test? Non-parametric tests are used when the assumptions of the parametric tests are violated. The assumptions of the parametric tests are: The data is normally distributed The variances of the groups are equal The data is homoscedastic The data is continuous The data is independent (this should be part of all experimental design unless it is a repreated measures analysis) If the data does not meet these assumptions, then non-parametric tests should be used. The most common non-parametric tests are: Wilcoxon signed-rank test (which is the non-parametric equivalent of the paired t-test) Mann-Whitney U test (which is the non-parametric equivalent of the independent t-test) Kruskal-Wallis test (which is the non-parametric equivalent of the one-way ANOVA; there is not non-parametric equivalent of the two-way ANOVA as far as I know). Friedman test (which is the non-parametric equivalent of the repeated measures ANOVA) Spearman’s rank correlation (which is the non-parametric equivalent of the Pearson’s correlation) Kendall’s tau (which is another non-parametric correlation test) Chi-square test (which is the non-parametric for count data) A common confusion when using non-parametric is that I see in many publications is that the authors use the mean and some measure of dispersion based on normality, such as mean, variance, standard deviation or standar error. This is not correct. If the data is normally distributed, then the parametric parameters should NOT be used. Instead, the median and the interquartile range should be used. The typical non-parametric test converts the data into ranks and then compares the ranks. Thus the original data is not used, but the ranks of the data. The ranks are used to compare the groups. 9.0.1 Wilcoxon signed-rank test The Wilcoxon signed-rank test is the non-parametric equivalent of the paired t-test. It is used when the data is not normally distributed and the variances are not equal. The Wilcoxon signed-rank test is used to compare the median of two related groups. The null hypothesis is that the median of the two groups are equal. The alternative hypothesis is that the median of the two groups are not equal. These are individuals that were RESAMPLED. Thus 2 information from the same individual is compared. Let’s compare the cleansiness of participants who participated at a Festival in the UKL, data on the first day and third day the festival. library(readr) DownloadFestival &lt;- read_csv(&quot;Data/DownloadFestival.csv&quot;) ## Rows: 810 Columns: 5 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): gender ## dbl (4): ticknumb, day1, day2, day3 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Festival=DownloadFestival active packages library(tidyverse) ## ── Attaching core tidyverse packages ─────────────────────── tidyverse 2.0.0 ── ## ✔ lubridate 1.9.4 ✔ stringr 1.5.1 ## ✔ purrr 1.0.4 ✔ tidyr 1.3.1 ## ── Conflicts ───────────────────────────────────────── tidyverse_conflicts() ── ## ✖ ggplot2::%+%() masks psych::%+%() ## ✖ ggplot2::alpha() masks psych::alpha() ## ✖ dplyr::between() masks data.table::between() ## ✖ tidyr::extract() masks pastecs::extract() ## ✖ dplyr::filter() masks stats::filter() ## ✖ pastecs::first() masks dplyr::first(), data.table::first() ## ✖ lubridate::hour() masks data.table::hour() ## ✖ lubridate::isoweek() masks data.table::isoweek() ## ✖ dplyr::lag() masks stats::lag() ## ✖ pastecs::last() masks dplyr::last(), data.table::last() ## ✖ lubridate::mday() masks data.table::mday() ## ✖ lubridate::minute() masks data.table::minute() ## ✖ lubridate::month() masks data.table::month() ## ✖ lubridate::quarter() masks data.table::quarter() ## ✖ car::recode() masks dplyr::recode() ## ✖ lubridate::second() masks data.table::second() ## ✖ purrr::some() masks car::some() ## ✖ purrr::transpose() masks data.table::transpose() ## ✖ lubridate::wday() masks data.table::wday() ## ✖ lubridate::week() masks data.table::week() ## ✖ lubridate::yday() masks data.table::yday() ## ✖ lubridate::year() masks data.table::year() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors First let use select the data for the first and third day of the festival and then remove all rows which have missing data. The data come from Field, Miles and Field (2012) “Discovering Statistics Using R”. day1: Cleanliness of the festival on the first day day3: Cleanliness of the festival on the third day The range of the data is from 0 to 4, where 0 is smells like dead corps and 4 you smell like roses. Each row represents data from the SAME individual on the first and third day of the festival. Festival1=Festival %&gt;% select(day1, day3) %&gt;% na.omit() str(Festival1) # Observe the data ## tibble [123 × 2] (S3: tbl_df/tbl/data.frame) ## $ day1: num [1:123] 2.64 0.97 1.11 0.82 1.76 2.17 0.97 2.57 1.94 2.08 ... ## $ day3: num [1:123] 1.61 0.29 0.55 0.47 1.58 0.76 0.76 0.02 1.67 0.96 ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:687] 3 4 5 6 7 8 9 11 13 15 ... ## ..- attr(*, &quot;names&quot;)= chr [1:687] &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; ... summary(Festival1) # Observe the summary statistics ## day1 day3 ## Min. :0.430 Min. :0.0200 ## 1st Qu.:1.160 1st Qu.:0.4400 ## Median :1.550 Median :0.7600 ## Mean :1.651 Mean :0.9765 ## 3rd Qu.:2.095 3rd Qu.:1.5250 ## Max. :3.380 Max. :3.4100 Now for the Wilcoxon signed-rank test, we can use the wilcox.test() function. The first argument is the first day of the festival, the second argument is the third day of the festival, and the paired=TRUE argument indicates that the data is paired. wilcox.test(Festival1$day1, Festival1$day3, paired=TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: Festival1$day1 and Festival1$day3 ## V = 6829, p-value = 4.782e-16 ## alternative hypothesis: true location shift is not equal to 0 The p-value is 0.0001, which is less than 0.05, so we reject the null hypothesis that the median of the two groups are equal. Thus, the cleanliness of the festival on the first day is different from the cleanliness of the festival on the third day. 9.0.2 Mann-Whitney U test The Mann-Whitney U test is the non-parametric equivalent of the independent t-test. It is used when the data is not normally distributed and the variances are not equal. The Mann-Whitney U test is used to compare the median of two independent groups. The null hypothesis is that the median of the two groups are equal. The alternative hypothesis is that the median of the two groups are not equal. NOTE here no individual was resampled, but two different groups were compared and the data are independent. We can use the same function as above but without the paired=TRUE argument. wilcox.test(Festival1$day1, Festival1$day3) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Festival1$day1 and Festival1$day3 ## W = 11693, p-value = 1.382e-13 ## alternative hypothesis: true location shift is not equal to 0 Note that in this case it would be innapropriate to use this test as the data are not indpendent in this data set. 9.0.3 Kruskal-Wallis test The Kruskal-Wallis test is the non-parametric equivalent of the one-way ANOVA. It is used when the data is not normally distributed and the variances are not equal. The Kruskal-Wallis test is used to compare the median of three or more independent groups. The null hypothesis is that the median of the three groups are equal. The alternative hypothesis is that the median of the three groups are not equal. Let’s compare the seed production of Sorghum plants that were treated with different Rust. The data is from the file Sorghum_2017_2018_Diseaase_traits.csv. the variable needed are Seedwt: Seed weight Rust: Rust treatment library(readr) Sorghum_2017_2018_Diseaase_traits &lt;- read_csv(&quot;Data/Sorghum_2017_2018_Diseaase_traits.csv&quot;) ## Rows: 2412 Columns: 10 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Cultivar ## dbl (9): Anthrac, Rust, GrainM, Seedwt, %Germ, Pani Hght, Pani Length, Grain... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Sorghum=Sorghum_2017_2018_Diseaase_traits Sorghum$Rust=as.factor(Sorghum$Rust) # convert &quot;Rust&quot; to a factor variable The function kruskal.test() is used to perform the Kruskal-Wallis test. The first argument is the dependent variable, the second argument is the independent variable, and the data argument is the data frame. kruskal.test(Seedwt ~ Rust, data=Sorghum) ## ## Kruskal-Wallis rank sum test ## ## data: Seedwt by Rust ## Kruskal-Wallis chi-squared = 10.697, df = 5, p-value = 0.05772 9.0.4 Friedman test This test is the non-parametric equivalent of the repeated measures ANOVA. The Friedman test is used to compare the median of three or more related groups. The null hypothesis is that the median of the three groups are equal. FIND a Dataset 9.0.5 Spearman’s rank correlation The Spearman’s rank correlation is the non-parametric equivalent of the Pearson’s correlation. It is used when the data is not normally distributed and the variances are not equal. The Spearman’s rank correlation is used to determine the strength and direction of the relationship between two variables. The null hypothesis is that there is no relationship between the two variables. The alternative hypothesis is that there is a relationship between the two variables. Using the Sorghum data, let’s compare the relationship between the seed weight and the panicle Length. names(Sorghum) ## [1] &quot;Cultivar&quot; &quot;Anthrac&quot; &quot;Rust&quot; &quot;GrainM&quot; &quot;Seedwt&quot; ## [6] &quot;%Germ&quot; &quot;Pani Hght&quot; &quot;Pani Length&quot; &quot;Grain Yld&quot; &quot;Julian days&quot; cor.test(Sorghum$Seedwt, Sorghum$&#39;Pani Length&#39;, method=&quot;spearman&quot;) ## Warning in cor.test.default(Sorghum$Seedwt, Sorghum$&quot;Pani Length&quot;, method = ## &quot;spearman&quot;): Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: Sorghum$Seedwt and Sorghum$&quot;Pani Length&quot; ## S = 33589567, p-value = 0.002102 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.1293585 9.0.6 Kendall’s tau The Kendall’s tau is another non-parametric correlation test. The Kendall’s tau is used to determine the strength and direction of the relationship between two variables. Kendall’s correlation is often used when sample size are small and appears to be robust under that situation where here are outliers and many tied ranks. cor.test(Sorghum$Seedwt, Sorghum$&#39;Pani Length&#39;, method=&quot;kendall&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: Sorghum$Seedwt and Sorghum$&quot;Pani Length&quot; ## z = -2.9927, p-value = 0.002766 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## -0.09213576 9.1 Spearman vs Kendall correlations comparison The difference between the Spearman’s rank correlation and the Kendall’s tau is that the Kendall’s tau is based on the number of concordant and discordant pairs, while the Spearman’s rank correlation is based on the difference between the ranks of the two variables. The Kendall’s tau is more robust to outliers and tied ranks than the Spearman’s rank correlation. While the Spearman’s rank correlation is likely to give larger coefficients but easier to calculates. 9.1.1 Altenative to non-parametric tests It is important to note that uch of the statistical literature to deal with non-parametric data and the development of Non-parametric statistics was prior to the time of computers. Now with comptuters many other approaches have been devlopped that can deal with non-parametric data and are usuallu more robust alternatives than non-parametric statistical apporaches. See the following for a short intro to GLM (Generalized Linear Models) in R "],["scatter-plots.html", "Chapter 10 Scatter Plots 10.1 Basic scatter plots 10.2 the lm() function 10.3 Visualization of the regression 10.4 The impact of outliers 10.5 Selling music records 10.6 Linear Regression Assumptions 10.7 Cook’s Distance 10.8 Steps to do a linear regression analysis 10.9 Alternative to Graph the Residuals 10.10 Scatter plots with jitter 10.11 Scatter plots with marginal histograms 10.12 Scatter plots for multiple variables", " Chapter 10 Scatter Plots 10.1 Basic scatter plots Last Revision ## [1] &quot;2025-02-13&quot; The following command verifies that you have the necessary libraries installed and activates them if available on your system. The package pacman evaluates if you have them installed and installs them if necessary. if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(QuantPsyc, car, ggplot2, tidyverse, sjPlot, janitor) library(QuantPsyc) # Package for Quantitative Psychology library(car) # Companion to applied regression library(ggplot2) # Data Visualization library(tidyverse) # Data Manipulation library(sjPlot) # Data Visualization library(janitor) # Data Cleaning library(conflicted) conflicts_prefer(lubridate::minute, lubridate::second, lubridate::month) Linear regression is the basic model to evaluate whether there is a relationship linear that is, a straight line between two variables. This relationship between the variables can be positive or negative. There are other types of regression, which includes nonlinear regression such as quadratic \\(y ~ ax^2+bx+c\\) or cubic \\(y ~ ax^3 +bx^2+cx+d\\), logarithmic \\(y ~ a + b*log(x)\\) among many other alternatives. There are also alternatives are you will see in that are nonlinear regressions module. Here we will be evaluating only linear regression We see a fictitious example The data is from some neighborhoods of Macondo where the numbers of bars in a neighborhood and the number of homicides in that neighborhood. The data you will find the data section below library(readr) pubs &lt;- read_csv(&quot;Data/pubs.csv&quot;) ## Rows: 8 Columns: 2 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (2): pubs, mortality ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. pubs ## # A tibble: 8 × 2 ## pubs mortality ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10 1000 ## 2 20 2000 ## 3 30 3000 ## 4 40 4000 ## 5 50 5000 ## 6 60 6000 ## 7 70 7000 ## 8 500 10000 10.2 the lm() function Let us do a simple linear regression, using the function lm(), for “linear model”= linear model. A regression takes two continuous variables. It is important that these variables have a normal distribution. The difference between a correlation and a regression is that the first is an analysis that describes the general pattern between the variables and the second is that not only is the pattern described but a prediction is made about the relationship between the variables. Using regression one also calculates a line that describes the relationship between the variables. This variable can be described as \\(y=m_x+b\\) where m represents the slope and b represents the intercept. You can also see it in books in the following way \\(y=\\alpha+\\beta_x\\) where the \\(\\beta\\) beta represents the slope and the \\(\\alpha\\) the intercept, this is the preferred method in the books on English. The linear regression function in R is lm() is composed of lm(y~x, data= “df”). Note the accent ~. There are two tests, the first is for determine if \\(\\alpha\\) is non-zero. The null hypothesis is Ho: the intercept \\(\\alpha\\) is equal to zero Ha: the intercept, \\(\\alpha\\) is not equal to zero. So the point where the line intercepts zero can be greater than or less that zero. The second null hypothesis is that the slope is different from zero. This means that the slope does not suggest/support a pattern of increasing and decrease between the two variables. Ho: the slope \\(\\beta\\) is equal to zero Ha: the slope, \\(\\alpha\\) is not equal to zero. So the relationship between the two variables is either positive or negative. Now we evaluate the results of the regression between the number of bars in a neighborhood (the barrios) and mortality in this same sector. HE Note that the coefficients of the line are \\(y=3352+14.3*x\\). Then the intercept at zero starts at 3352 fatalities and for each bar. Additionally there are 14.3 more fatalities. This means that if there are no bars x=0, the expected number of fatalities is 3352. Now to determine if these values are significant, we must evaluate the value of p in each line. The null hypothesis of the intercept has a value of p =0.005, which suggests that the hypothesis should be rejected null, and consequently we accept the alternative hypothesis, that the intercept is not equal to zero. The slope has a value of p=0.015 and the null hypothesis is also rejected, this suggests that as increases the number of bars increases the number of fatalities, for example for each additional bar we expect 14.3 additional fatalities. pubReg &lt;- lm(mortality~pubs, data = pubs) summary(pubReg) # The summary of the regression ## ## Call: ## lm(formula = mortality ~ pubs, data = pubs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2495.3 -996.3 -223.5 1145.2 2644.3 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3351.955 781.236 4.291 0.00515 ** ## pubs 14.339 4.301 3.334 0.01572 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1864 on 6 degrees of freedom ## Multiple R-squared: 0.6495, Adjusted R-squared: 0.591 ## F-statistic: 11.12 on 1 and 6 DF, p-value: 0.01572 pubReg$residuals # to visualize the rediduals ## 1 2 3 4 5 6 7 ## -2495.3445 -1638.7337 -782.1229 74.4879 931.0987 1787.7095 2644.3203 ## 8 ## -521.4153 cooks.distance(pubReg)# To calculate Cooks indices ## 1 2 3 4 5 6 ## 2.132784e-01 8.530493e-02 1.814286e-02 1.547980e-04 2.293965e-02 8.092291e-02 ## 7 8 ## 1.710655e-01 2.271429e+02 tab_model( pubReg,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) ) ## Registered S3 method overwritten by &#39;parameters&#39;: ## method from ## predict.kmeans statip   mortality Predictors Estimates CI p df (Intercept) 3351.96 1440.34 – 5263.57 0.005 6.00 pubs 14.34 3.82 – 24.86 0.016 6.00 Observations 8 R2 / R2 adjusted 0.649 / 0.591 10.3 Visualization of the regression It is observed that there is an increase in fatalities with an increase in the number of bars. But notice the value on the right that seems to be very atypical compared to the others. ggplot(pubs, aes(x=pubs, y=mortality))+ geom_smooth(method = lm)+ # modelo lineal geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.4 The impact of outliers On certain occasions, values outside the normal range can make great changes in the result, in this case the regression result. What is the effect of the large value? We remove that value from the file data and re-evaluate the model (linear regression). Note that now the model is extremely different \\(y=-163.7+103.2*x\\). Are the two hypotheses? pubsnew &lt;- pubs[ which(pubs$pubs&lt;80), ] # remove the outlier pubsnew=pubsnew %&gt;% add_row(pubs = 4, mortality = 0) # adding a pair of values pubRegNew &lt;- lm(mortality~pubs, data = pubsnew) summary(pubRegNew) ## ## Call: ## lm(formula = mortality ~ pubs, data = pubsnew) ## ## Residuals: ## Min 1Q Median 3Q Max ## -249.11 -36.48 19.57 75.62 131.67 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -163.701 86.000 -1.903 0.106 ## pubs 103.203 2.055 50.229 4.18e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 128.9 on 6 degrees of freedom ## Multiple R-squared: 0.9976, Adjusted R-squared: 0.9972 ## F-statistic: 2523 on 1 and 6 DF, p-value: 4.177e-09 tab_model( pubRegNew,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) )   mortality Predictors Estimates CI p df (Intercept) -163.70 -374.14 – 46.73 0.106 6.00 pubs 103.20 98.18 – 108.23 &lt;0.001 6.00 Observations 8 R2 / R2 adjusted 0.998 / 0.997 ggplot(pubsnew,aes(x=pubs, y=mortality))+ geom_smooth(method=lm)+ geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.5 Selling music records We now evaluate a similar more complex and more realistic data set those one would encounter in a study in medicine, sociology or ecological. The data represents the amount of money dedicated to the promotion of different CD’s from a music company and the number of CD’s (CD/downloads) sold. In the first line we see the amount of pounds sterling, £ (UK) dedicated to the promotion of the album music, in the first line we see that he spent £10,256, and then the number of CDs sold was 330. We have information about 200 albums different. library(readr) Album_Sales_1_new &lt;- read_csv(&quot;Data/Album_Sales_1_new.csv&quot;) ## Rows: 200 Columns: 4 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): adverts, sales, airplay, attract ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(Album_Sales_1_new) ## # A tibble: 6 × 4 ## adverts sales airplay attract ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.3 330 43 10 ## 2 986. 120 28 7 ## 3 1446. 360 35 7 ## 4 1188. 270 33 7 ## 5 575. 220 44 5 ## 6 569. 170 19 5 length(Album_Sales_1_new$adverts) # count how many data points we have. ## [1] 200 shapiro.test(Album_Sales_1_new$adverts) # test of normality using Shapiro-Wilks ## ## Shapiro-Wilk normality test ## ## data: Album_Sales_1_new$adverts ## W = 0.92542, p-value = 1.482e-08 library(nortest) #Anderson-Darling ad.test(Album_Sales_1_new$adverts) # test of normality using Anderson_Darling test ## ## Anderson-Darling normality test ## ## data: Album_Sales_1_new$adverts ## A = 3.8762, p-value = 1.089e-09 We begin by graphing the relationship between the two variables. Note that in the geom_smooth() part, has to include method=lm, this means that the method of constructing the line will use linear regression. It is added to the linear function \\(\\epsilon\\) that represents the errors of the values when comparing with the line that represents the best model. \\[Y_{ i }=\\beta _{ 0 }+\\beta _{ 1 }X_{ i }+\\epsilon _{ i }\\] Remember that \\(\\beta _{ 0 }\\) is the intercept and the \\(\\beta _{ 1 }x_{ i }\\) is the earring. The shaded area is the area of 95% interval of trust. This means that the best line, intercept and slope could vary in this range if we repeat the experiment. Note here all the alternatives, I added the two extreme slopes, with a slope major (red) and a minor (violet). Each point represents the sale of a CD with its corresponding amount dedicated to the promotion. The \\(epsilon\\) would be the difference between the best line and the original value. This is also called the residuals. library(ggplot2) ggplot(Album_Sales_1_new,aes(x=adverts, y=sales))+ geom_smooth(method=lm, se = TRUE)+ geom_point()+ geom_segment(aes(x=0, y=120, xend=2250, yend=380), colour=&quot;red&quot;)+ geom_segment(aes(x=0, y=150, xend=2250, yend=320), colour=&quot;purple&quot;) ## Warning in geom_segment(aes(x = 0, y = 120, xend = 2250, yend = 380), colour = &quot;red&quot;): All aesthetics have length 1, but the data has 200 rows. ## ℹ Please consider using `annotate()` or provide this layer with data ## containing a single row. ## Warning in geom_segment(aes(x = 0, y = 150, xend = 2250, yend = 320), colour = &quot;purple&quot;): All aesthetics have length 1, but the data has 200 rows. ## ℹ Please consider using `annotate()` or provide this layer with data ## containing a single row. ## `geom_smooth()` using formula = &#39;y ~ x&#39; The linear model with the lm() function. How do you interpret the coefficients and if these are significant? Are the null hypotheses rejected? library(sjPlot) model1=lm(sales~adverts, Album_Sales_1_new) #summary(model1) tab_model( model1,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) )   sales Predictors Estimates CI p df (Intercept) 134.14 119.28 – 149.00 &lt;0.001 198.00 adverts 0.10 0.08 – 0.12 &lt;0.001 198.00 Observations 200 R2 / R2 adjusted 0.335 / 0.331 ggplot(Album_Sales_1_new, aes(x=adverts, y=sales))+ geom_smooth(method=lm)+ geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.6 Linear Regression Assumptions Equality of variance: In the first graph you must evaluate whether the data are distributed more or less equally. I mean you must determine that not there is more variation in one area of the graph compared to another area. Note that the data are more or less equally distributed over above and below the zero line through the distribution of the Fitted” values, which are the predicted values. Normality of the data, evaluate the graph #2 with the graph of qqplot we see that the data follows the null model (the line) almost perfectly fits the data, then one can assume that the data complies with a normal distribution. But note that the data in the upper quartile they are not above the line. Evaluate whether there are biased data (outliers) that influence the results, evaluate graph #3. If the standardized values of “Student” are greater than 3, this suggests that there are outliers. In the fourth graph, Cook’s distance evaluate if there are values that have a lot of weight if whether or not they are included in the analysis, evaluate graph #4. These are going to be identified. The values to worry about are those that are above or below the broken line. In this graph there are three values that must be evaluated (1, 42, 169), these values must be ensured to be correct. It is always good to remove the biased values and redo the analysis to observe how different the results are. Typically more data in an analysis, the less weight a biased value will have on the results, unless this value is very different from most of the data. plot(model1) # Evaluate the assumptions, 1. Equality of variance, 2. Normality, 3. Students biased estimator, 4. Biased data (Cook&#39;s Distance) 10.7 Cook’s Distance Continuing with the theme of evaluating whether there are values that could influence the analysis a lot, we can use one of the tools to evaluate the weight of each value on a linear regression based on least squares methods, called Cook’s Distance. This analysis was developed by R. Dennis Cook in 1977 and has as its objective to evaluate each value in the data matrix and the weight it has about the result (when it is included or not in the analysis). Produces an index for each of the values on the result based on the residual values is called Cook’s Distance. Therefore, this analysis evaluates the relative impact of each value about the index. Unfortunately it is not clear what the value is critical; That is, what value can tell us that we are overweight? about the results. The two main suggestions are: Distance of Cook, Di, is greater than 1 (suggested by R. Dennis Cook Cook himself in 1982); and that Di &gt; 4/n, where n is the number of observations (Bollen et al. 1990). To make an illustration, we will continue with the model model1 using the values calculated in the previous model. The graph is will be re-organized using the seq_along option, so that the values in the X axis are based on the sequence of data in the file and the values on the Y axis are based on the values of the Distance of Cook. In this case, we see that all the values are well below of 1, suggesting that none of the individual values would greatly influence the results even if they were excluded. Now let us consider a second alternative of Di &gt; 4/n, then we note 6 \\(D_i\\) values that are greater than 4/200=0.02 should be of concern, where 200 is the amount of data in the file. If you consider this second alternative, it would be necessary to evaluate 6 values in the table of data that could be suspicious (values above the line red). Note that it’s not that they are incorrect; rather, this result is only a tool to evaluate values that appear to have a considerable impact on the results. Below is how to add the values of Cook’s distance to the data file, add a sequence column to the data, create a graph of Cook’s distance, and determine if there are values of Cook’s \\(D_i\\) greater than 1, or 4/n. the “cook.distance” values to your file Add a “sequence” column to the data Create a graph of Cook’s distance. Determine if there are values of Cook’s \\(D_i\\) greater than 1, or 4/n. 4/length(Album_Sales_1_new$adverts) ## [1] 0.02 Album_Sales_1_new$cooks.distance&lt;-cooks.distance(model1) Album_Sales_1_new$sequence=c(1:200) ggplot(Album_Sales_1_new, aes(sequence, cooks.distance))+ geom_point()+ geom_hline(aes(yintercept=4/length(Album_Sales_1_new$adverts), colour=&quot;red&quot;)) 10.8 Steps to do a linear regression analysis 10.8.1 Step 1 First step, build your model and evaluate the coefficients. The result: The coefficient (intercept) and slope of the model 10.8.2 Step 2 Evaluate if the coefficients are different from zero. The first hypothesis: Determine if the intercept is equal to zero. Look at the value of p, Pr(&gt;|t|), determine if the value is less than 0.05, if it is, the Ho is rejected and consequently we have confidence that the intercept does not include zero. The second null hypothesis: Determine if the slope is equal to zero. Look at the value of p, Pr(&gt;|t|), how the value is less than p=0.05, Ho is rejected and consequently we are confident that the slope does not include zero. 10.8.3 Step 3 Evaluate whether the data meet the assumptions of the model: Equality of variance, use the residual graph Normality, qqplot Biased values, Cook’s test advertizingReg &lt;- lm(sales~adverts, data = Album_Sales_1_new) summary(advertizingReg) ## ## Call: ## lm(formula = sales ~ adverts, data = Album_Sales_1_new) ## ## Residuals: ## Min 1Q Median 3Q Max ## -152.949 -43.796 -0.393 37.040 211.866 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.341e+02 7.537e+00 17.799 &lt;2e-16 *** ## adverts 9.612e-02 9.632e-03 9.979 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.99 on 198 degrees of freedom ## Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 ## F-statistic: 99.59 on 1 and 198 DF, p-value: &lt; 2.2e-16 tab_model( advertizingReg,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) )   sales Predictors Estimates CI p df (Intercept) 134.14 119.28 – 149.00 &lt;0.001 198.00 adverts 0.10 0.08 – 0.12 &lt;0.001 198.00 Observations 200 R2 / R2 adjusted 0.335 / 0.331 10.9 Alternative to Graph the Residuals 10.9.1 Plot the residuals If the assumption of equality of variance is met, what we will observe that the distribution of the residuals looks more or less uniform around the average of the residuals (zero). There are approximately equal number of values greater than zero (above the blue line) and less than zero (below the blue line) that are distributed to through the variable on the X axis, or estimated values. In addition that the residuals (negative or positive) are not limited to sub groups of the estimated values (in the X). # &quot;model1&quot;, nota que este no es un data frame pero un modelo # La figura principal ggplot(Album_Sales_1_new, aes(x=adverts, y=sales))+ geom_smooth(method=lm, se = TRUE)+ geom_point() ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Graficando los residuales con ggplot2 ggplot(model1, aes(x=.fitted, y=.resid))+ # note here we use the model1 with the .fitted and .resid to make the figure geom_point()+ geom_smooth(method=lm) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 10.9.2 Exercise 1 The data state.x77 comes from the package datasets where represents information about the American states, their level population, income, level of illiteracy, life expectancy, and other variables. Select the illiteracy and life variables expectancy”, and evaluates if there is a relationship between the two variables. Note you have to clean the variable names, because some of the names has spaces. The janitor package and the function are used clean_names(). Use illiteracy data and evaluate it as a predictor of life expectancy (life_exp). Using the following data do a complete regression analysis linear. Determines whether the assumptions of the linear regression are met. library(datasets) state.x77=as.data.frame(state.x77) # convert the data into a data frame state.x77=janitor::clean_names(state.x77) # clean the names of the variables head(state.x77) ## population income illiteracy life_exp murder hs_grad frost area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 ## Colorado 2541 4884 0.7 72.06 6.8 63.9 166 103766 10.9.3 Exercise 2 The ToothGrowth data set in the datasets package contains the result of an experiment that studies the effect of vitamin C on the growth of teeth in 60 guinea pigs. each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two methods of administration (orange juice or ascorbic acid (a form of vitamin C and coded as VC). Using the following data do a complete regression analysis linear. library(datasets) head(ToothGrowth) ## len supp dose ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10.0 VC 0.5 10.10 Scatter plots with jitter 10.11 Scatter plots with marginal histograms 10.12 Scatter plots for multiple variables "],["ring-chart-alternative-to-the-pie-chart.html", "Chapter 11 Ring chart (alternative to the pie chart)", " Chapter 11 Ring chart (alternative to the pie chart) An alternative method of naming ring chart are “Donut chart” or “Circle chart”. It is a circular statistical graphic which is divided into slices to illustrate numerical proportion. The ring chart is a variation of the pie chart, with a hole in the center. The ring chart allows for a better visual comparison of the proportions of the data. The ring chart is a good way to represent data when the number of slices is small. When the number of slices. Pie chart should be avoided in all cicumstances because it is difficult to compare the size of the slices. The human eye is not good at comparing angles. The ring chart is an alternative to the pie chart. # load library library(ggplot2) # Create test data. data &lt;- data.frame( category=c(&quot;Sp1&quot;, &quot;Sp2&quot;, &quot;Sp3&quot;), count=c(100, 500, 30) ) # Compute percentages data$fraction = data$count/sum(data$count) # Compute the cumulative percentages data$ymax = cumsum(data$fraction) # Compute the bottom of each rectangle data$ymin = c(0, head(data$ymax, n=-1)) data # Note the ymin and ymax columns and their values ## category count fraction ymax ymin ## 1 Sp1 100 0.15873016 0.1587302 0.0000000 ## 2 Sp2 500 0.79365079 0.9523810 0.1587302 ## 3 Sp3 30 0.04761905 1.0000000 0.9523810 # Make the plot ggplot(data, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=category)) + geom_rect() + coord_polar(theta=&quot;y&quot;) + # Remove the last two lines by adding a hashtag in from of the line to understand how the chart is built initially xlim(c(1, 4))+ # Try to remove that to see how to make a pie chart (note that his creates the donut chart) theme_void() # Remove the axis and the grid 11.0.1 Adding values We can add values to donut chart by adding the following code: Compute a good label data\\(label &lt;- paste0(data\\)category, “value:”, data$count) # load library library(ggplot2) # Create test data. data &lt;- data.frame( category=c(&quot;Sp1&quot;, &quot;Sp2&quot;, &quot;Sp3&quot;), count=c(100, 500, 30) ) # Compute percentages data$fraction = data$count/sum(data$count) # Compute the cumulative percentages data$ymax = cumsum(data$fraction) # Compute the bottom of each rectangle data$ymin = c(0, head(data$ymax, n=-1)) data # Note the ymin and ymax columns and their values ## category count fraction ymax ymin ## 1 Sp1 100 0.15873016 0.1587302 0.0000000 ## 2 Sp2 500 0.79365079 0.9523810 0.1587302 ## 3 Sp3 30 0.04761905 1.0000000 0.9523810 # Compute label position data$labelPosition &lt;- (data$ymax + data$ymin) / 2 # Compute a good label data$label &lt;- paste0(data$category, &quot;\\n value: &quot;, data$count) # Make the plot ggplot(data, aes(ymax=ymax, ymin=ymin, xmax=4, xmin=3, fill=category)) + geom_rect() + coord_polar(theta=&quot;y&quot;) + # Remove the last two lines by adding a hashtag in from of the line to understand how the chart is built initially xlim(c(1, 4))+ # Try to remove that to see how to make a pie chart (note that his creates the donut chart) theme_void()+ # Remove the axis and the grid geom_text( x=2.5, aes(y=labelPosition, label=label, color=category), size=3) 11.0.2 Example of why PIE chart should NOT be used There is a good example of why pie chart should not be used in the following link: https://www.tableau.com/blog/5-unusual-alternatives-pie-charts Many visual experts argue that pie charts are not the best way to represent data. The main reason is that it is difficult to compare the size of the slices. The human eye is not good at comparing angles. The ring chart is a good alternative to the pie chart. It is easier to compare the size of the slices. The ring chart is a good way to represent data when the number of slices is small. When the number of slices is large, it is better to use a bar chart or a line chart. In the link above some good examples are shown why pie chart are problematic and some alternatives explained. "],["histogramsdensity-plots.html", "Chapter 12 Histograms/Density Plots 12.1 Histograms with geom_histogram 12.2 Changing bin sizes with binwidth 12.3 Numbers of hospital beds per 1000 inhabitants in different countries 12.4 Modifying continuous scales with scale_y_continuous and scale_x_continuous 12.5 Frequency and color intensity overlap 12.6 Position the bars next to each other with the dodge function 12.7 Group graphics with Facet_wrap 12.8 geom_histogram Options and Parameters: 12.9 DensityPlots 12.10 Area chart with geom_area 12.11 geom_area with multiple groups 12.12 The density function aes(y=..density..) 12.13 Density plot with geom_density 12.14 geom_density and simulated data 12.15 Polygon frequency graph with geom_freqpoly", " Chapter 12 Histograms/Density Plots library(ggversa) library(ggplot2) library(janitor) 12.1 Histograms with geom_histogram A histogram is a graphical representation of data grouped into compartments or bins. These compartments include individuals with similar or numerically close factors or groupings of values. Subsequent to the determination of the compartments, the number of observations for each of them is added. A histogram is a very common type of graph to visualize the spread of that data. We will continue with the Dipodium example to demonstrate the function of geom_histogram below. In the first case of geom_histogram the number of observations (data) is added according to the position of the compartments or bins. In the second case, to show fewer bars and see the grouping by groups of 2, it is written with geom_histogram (binwidth=1). As you can see, the number of compartments is changed using the binwidth parameter, where its value represents the width of the compartment (in our case it is equal to 1). The default number of bins is 30, but not necessarily all of them will have a bar since that depends on the distribution of the data. Each bar represents the frequency of observations in the category at one meter distance. In both cases, color=white was used to put a white line around each bar and thus better differentiate the groups or bins. Note that the program returns the message stat_bin() using bins = 30. Pick better value with binwidth. This is because the predetermined compartment amount was accepted. library(janitor) DW=clean_names(dipodium) # the data is from the package ggversa names(DW) ## [1] &quot;tree_number&quot; &quot;tree_species&quot; ## [3] &quot;dbh&quot; &quot;plant_number&quot; ## [5] &quot;ramet_number&quot; &quot;distance&quot; ## [7] &quot;orientation&quot; &quot;number_of_flowers&quot; ## [9] &quot;height_inflo&quot; &quot;herbivory&quot; ## [11] &quot;row_position_nf&quot; &quot;number_flowers_position&quot; ## [13] &quot;number_of_fruits&quot; &quot;perc_fr_set&quot; ## [15] &quot;pardalinum_or_roseum&quot; &quot;fruit_position_effect&quot; ## [17] &quot;frutos_si_o_no&quot; &quot;p_or_r_infl_lenght&quot; ## [19] &quot;num_of_fruits&quot; &quot;species_name&quot; ## [21] &quot;cardinal_orientation&quot; library(gt) DW %&gt;% head() %&gt;% gt() #rcasbuztkz table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #rcasbuztkz thead, #rcasbuztkz tbody, #rcasbuztkz tfoot, #rcasbuztkz tr, #rcasbuztkz td, #rcasbuztkz th { border-style: none; } #rcasbuztkz p { margin: 0; padding: 0; } #rcasbuztkz .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #rcasbuztkz .gt_caption { padding-top: 4px; padding-bottom: 4px; } #rcasbuztkz .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #rcasbuztkz .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #rcasbuztkz .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rcasbuztkz .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rcasbuztkz .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #rcasbuztkz .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #rcasbuztkz .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #rcasbuztkz .gt_column_spanner_outer:first-child { padding-left: 0; } #rcasbuztkz .gt_column_spanner_outer:last-child { padding-right: 0; } #rcasbuztkz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #rcasbuztkz .gt_spanner_row { border-bottom-style: hidden; } #rcasbuztkz .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #rcasbuztkz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #rcasbuztkz .gt_from_md > :first-child { margin-top: 0; } #rcasbuztkz .gt_from_md > :last-child { margin-bottom: 0; } #rcasbuztkz .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #rcasbuztkz .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #rcasbuztkz .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #rcasbuztkz .gt_row_group_first td { border-top-width: 2px; } #rcasbuztkz .gt_row_group_first th { border-top-width: 2px; } #rcasbuztkz .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rcasbuztkz .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #rcasbuztkz .gt_first_summary_row.thick { border-top-width: 2px; } #rcasbuztkz .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rcasbuztkz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #rcasbuztkz .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #rcasbuztkz .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #rcasbuztkz .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #rcasbuztkz .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #rcasbuztkz .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rcasbuztkz .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #rcasbuztkz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #rcasbuztkz .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #rcasbuztkz .gt_left { text-align: left; } #rcasbuztkz .gt_center { text-align: center; } #rcasbuztkz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #rcasbuztkz .gt_font_normal { font-weight: normal; } #rcasbuztkz .gt_font_bold { font-weight: bold; } #rcasbuztkz .gt_font_italic { font-style: italic; } #rcasbuztkz .gt_super { font-size: 65%; } #rcasbuztkz .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #rcasbuztkz .gt_asterisk { font-size: 100%; vertical-align: 0; } #rcasbuztkz .gt_indent_1 { text-indent: 5px; } #rcasbuztkz .gt_indent_2 { text-indent: 10px; } #rcasbuztkz .gt_indent_3 { text-indent: 15px; } #rcasbuztkz .gt_indent_4 { text-indent: 20px; } #rcasbuztkz .gt_indent_5 { text-indent: 25px; } #rcasbuztkz .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #rcasbuztkz div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } tree_number tree_species dbh plant_number ramet_number distance orientation number_of_flowers height_inflo herbivory row_position_nf number_flowers_position number_of_fruits perc_fr_set pardalinum_or_roseum fruit_position_effect frutos_si_o_no p_or_r_infl_lenght num_of_fruits species_name cardinal_orientation 1 E.o 75 1 1 2.47 40 11 35 n 1 24 0 0.00 r 1 0 r 0 r 1 1 E.o 76 2 1 1.97 50 19 47 n 2 23 0 0.00 r 2 0 r 0 r 2 2 E.o 76 3 1 1.95 350 18 63 n 3 25 1 0.04 r 3 0 r 1 r 8 3 E.o 58 4 1 3.24 210 24 47 n 4 20 5 0.25 r 4 0 r 5 r 5 4 E.o NA 5 1 0.85 80 25 61 n 5 13 0 0.00 r 5 0 r 0 r 2 5 E.o 59 6 1 2.62 160 17 35 n 6 25 2 0.08 p 6 0 r 2 p 4 a=ggplot(DW, aes(distance)) a+geom_histogram(color=&quot;white&quot;, fill=&quot;blue&quot;)+ labs(x=&quot;Distancia&quot;, y=&quot;Frecuencia&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 12.2 Changing bin sizes with binwidth This histogram demonstrates the distances of the Dipodium roseum orchids from the nearest trees. This species of orchid is parasitic on fungi/mycorrhizae and does not produce photosynthesis. The hypothesis is that mycorrhizae, fungi, receive nutrients from the roots of the trees and the nutrients are transferred to the orchids through the mycorrhizae. Consequently, an optimal distance from the orchid to the trees that support the mycorrhizae should hypothetically be observed if the hypothesis has any veracity. As explained before, in the second graph, the binwidth was changed to 1, which in this case represents the distance of 1 meter, and the bins range from -.5 to .5 m., the second ** bin** from &gt;0.5 to 1.5 meters and so on. length(DW$distance) ## [1] 1363 a=ggplot(DW, aes(distance)) a+geom_histogram(binwidth=.5,color=&quot;white&quot;, fill=&quot;blue&quot;)+ labs(x=&quot;Distancia&quot;, y=&quot;Frecuencia&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) 12.3 Numbers of hospital beds per 1000 inhabitants in different countries In the following graph, we see the frequency of the number of hospital beds per 1000 inhabitants for 67 countries using the Hospital_Camas database. It was done with the information that was available only for the years 1996 and 2006. We have two overlapping graphs to visualize if the distribution has changed between the years 1996 and 2006. Note that the frequencies from 2006 appear above those from 1996. Camas_Hospital %&gt;% head() %&gt;% gt() #ayztrcoupp table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #ayztrcoupp thead, #ayztrcoupp tbody, #ayztrcoupp tfoot, #ayztrcoupp tr, #ayztrcoupp td, #ayztrcoupp th { border-style: none; } #ayztrcoupp p { margin: 0; padding: 0; } #ayztrcoupp .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ayztrcoupp .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ayztrcoupp .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ayztrcoupp .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ayztrcoupp .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ayztrcoupp .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ayztrcoupp .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ayztrcoupp .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ayztrcoupp .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ayztrcoupp .gt_column_spanner_outer:first-child { padding-left: 0; } #ayztrcoupp .gt_column_spanner_outer:last-child { padding-right: 0; } #ayztrcoupp .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ayztrcoupp .gt_spanner_row { border-bottom-style: hidden; } #ayztrcoupp .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ayztrcoupp .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ayztrcoupp .gt_from_md > :first-child { margin-top: 0; } #ayztrcoupp .gt_from_md > :last-child { margin-bottom: 0; } #ayztrcoupp .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ayztrcoupp .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ayztrcoupp .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ayztrcoupp .gt_row_group_first td { border-top-width: 2px; } #ayztrcoupp .gt_row_group_first th { border-top-width: 2px; } #ayztrcoupp .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ayztrcoupp .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ayztrcoupp .gt_first_summary_row.thick { border-top-width: 2px; } #ayztrcoupp .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ayztrcoupp .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ayztrcoupp .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ayztrcoupp .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #ayztrcoupp .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ayztrcoupp .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ayztrcoupp .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ayztrcoupp .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ayztrcoupp .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ayztrcoupp .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ayztrcoupp .gt_left { text-align: left; } #ayztrcoupp .gt_center { text-align: center; } #ayztrcoupp .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ayztrcoupp .gt_font_normal { font-weight: normal; } #ayztrcoupp .gt_font_bold { font-weight: bold; } #ayztrcoupp .gt_font_italic { font-style: italic; } #ayztrcoupp .gt_super { font-size: 65%; } #ayztrcoupp .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #ayztrcoupp .gt_asterisk { font-size: 100%; vertical-align: 0; } #ayztrcoupp .gt_indent_1 { text-indent: 5px; } #ayztrcoupp .gt_indent_2 { text-indent: 10px; } #ayztrcoupp .gt_indent_3 { text-indent: 15px; } #ayztrcoupp .gt_indent_4 { text-indent: 20px; } #ayztrcoupp .gt_indent_5 { text-indent: 25px; } #ayztrcoupp .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #ayztrcoupp div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Pais Year Poblacion Camas Armenia 1996 3173425 7.13 Australia 1996 18311000 8.50 Austria 1996 7959017 9.30 Azerbaijan 1996 7763000 9.81 Bahamas, The 1996 283792 3.94 Barbados 1996 265940 7.56 str(Camas_Hospital) ## &#39;data.frame&#39;: 134 obs. of 4 variables: ## $ Pais : Factor w/ 67 levels &quot;Armenia&quot;,&quot;Australia&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Year : int 1996 1996 1996 1996 1996 1996 1996 1996 1996 1996 ... ## $ Poblacion: int 3173425 18311000 7959017 7763000 283792 265940 10160000 213674 7717445 8362826 ... ## $ Camas : num 7.13 8.5 9.3 9.81 3.94 ... unique(Camas_Hospital$Year) # Which year were surveyed? ## [1] 1996 2006 a=ggplot(Camas_Hospital, aes(Camas, fill=factor(Year))) a+geom_histogram(stat=&quot;bin&quot;)+ xlab(&quot;Número de camas por cada 1000 habitantes&quot;)+ ylab(&quot;Frecuencia&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 12.4 Modifying continuous scales with scale_y_continuous and scale_x_continuous We demonstrate how to modify the scale, first with an impractical method by numbering on the Y axis each value where we want a line. Note that the number 10 is excluded and that is why it does not appear on the Y axis the function is scale_y_continuous(breaks=c(x,x,x…x). On the * axis *X is also numbered but this time the scale is defined with scale_x_continuous(breaks=c( ), and c(x:xx) with an initial value of 1 and final value of 15 to identify what are the values that one wants in the axes; that is more practical and includes the value 10. The information on the axes is also modified using xlab and ylab**, so that the description of the columns can be presented in a specific way and so on. make the information clearer for chart purposes. a=ggplot(Camas_Hospital, aes(Camas, fill=factor(Year)))+ geom_histogram(stat=&quot;bin&quot;, alpha=0.3)+ scale_y_continuous(breaks=c(0,1,4,5,6,7,8,9,11,12,13,14,15))+ scale_x_continuous(breaks=c(0:15))+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) a ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 12.5 Frequency and color intensity overlap We might think that, because the frequencies overlap, it is difficult to have a good appreciation of the distribution of the data. To compare the frequencies between the groups more easily, the frequencies can be differentiated with 3 colors; in this case the blue color for the year 2006, the peach color for the year 1996, and the grayish color for the frequencies that overlap with both years. position=identity and alpha= are used to modify the color intensities of the bars. Now we see that, for example, in 1996 the most common frequency in the countries was less than 2 beds per 1,000 inhabitants, and that by 2006 it was already between 2 and 8 beds per 1,000 inhabitants. a=ggplot(Camas_Hospital, aes(Camas, fill=factor(Year))) a+geom_histogram(stat=&quot;bin&quot;, alpha=0.5, position=&quot;identity&quot;)+ xlab(&quot;Número de camas por \\n 1000 habitantes&quot;)+ ylab(&quot;Frecuencia&quot;)+ scale_y_continuous(breaks=c(0:9))+ scale_x_continuous(breaks=c(0:15))+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 12.6 Position the bars next to each other with the dodge function a=ggplot(Camas_Hospital, aes(Camas, fill=factor(Year))) a+geom_histogram(color=&quot;black&quot;, bins=15, alpha=0.9, position=&quot;dodge&quot;)+ xlab(&quot;Número de camas por \\n 1000 habitantes&quot;)+ ylab(&quot;Frecuencia&quot;)+ scale_y_continuous(breaks=c(0:9))+ scale_x_continuous(breaks=c(0:15))+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ facet_wrap(~Year) 12.7 Group graphics with Facet_wrap 12.7.1 Change chart colors manually Another alternative is to use two graphs for each year; that is, one for each group. In this case we will use the facet_wrap option. facet_wrap are explained in more detail later. We can change the color of the bars using scale_fill_manual; in our case, cyan4 for 1996 and darkrange for 2006. Also, notice that with scale_color_manual we can change the line around the bars; In our case we change it to black or black. a=ggplot(Camas_Hospital, aes(Camas, fill=factor(Year), color=factor(Year))) a+geom_histogram(stat=&quot;bin&quot;, alpha=0.5)+ xlab(&quot;Número de camas \\n por cada 1000 habitantes&quot;)+ ylab(&quot;Frecuencia&quot;)+ scale_fill_manual(values=c(&quot;cyan4&quot;, &quot;darkorange&quot;))+ theme(axis.title=element_text(family=&quot;Times&quot;,size=10,face=&quot;italic&quot;, colour=&quot;#4ea7ad&quot;))+ facet_wrap(~Year)+ scale_y_continuous(breaks=c(0:9))+ scale_x_continuous(breaks=c(1:15))+ scale_color_manual(values=c(&quot;black&quot;, &quot;black&quot;))+ theme(axis.title.x=element_text(angle=10))+ # NOTE the x legend text theme(axis.text.x=element_text(size=8, angle=45)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 12.8 geom_histogram Options and Parameters: ggplot(the data file, aes(the continuous variable)) geom_histogram (), binwidth, x, y, alpha, color, fill, stat, position. binwidth: the width of the bins where the default is 1/30 of the data range. alpha: the intensity of the color fill: the color of the area; e.g., color=blue color: the color of the line around the area; e.g. color=white position: “identity”, “stack”, “dodge”. library(gt) diamonds %&gt;% head() %&gt;% gt() #ozcafllupq table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #ozcafllupq thead, #ozcafllupq tbody, #ozcafllupq tfoot, #ozcafllupq tr, #ozcafllupq td, #ozcafllupq th { border-style: none; } #ozcafllupq p { margin: 0; padding: 0; } #ozcafllupq .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #ozcafllupq .gt_caption { padding-top: 4px; padding-bottom: 4px; } #ozcafllupq .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #ozcafllupq .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #ozcafllupq .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ozcafllupq .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ozcafllupq .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #ozcafllupq .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #ozcafllupq .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #ozcafllupq .gt_column_spanner_outer:first-child { padding-left: 0; } #ozcafllupq .gt_column_spanner_outer:last-child { padding-right: 0; } #ozcafllupq .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #ozcafllupq .gt_spanner_row { border-bottom-style: hidden; } #ozcafllupq .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #ozcafllupq .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #ozcafllupq .gt_from_md > :first-child { margin-top: 0; } #ozcafllupq .gt_from_md > :last-child { margin-bottom: 0; } #ozcafllupq .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #ozcafllupq .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #ozcafllupq .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #ozcafllupq .gt_row_group_first td { border-top-width: 2px; } #ozcafllupq .gt_row_group_first th { border-top-width: 2px; } #ozcafllupq .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ozcafllupq .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #ozcafllupq .gt_first_summary_row.thick { border-top-width: 2px; } #ozcafllupq .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ozcafllupq .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #ozcafllupq .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #ozcafllupq .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #ozcafllupq .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #ozcafllupq .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #ozcafllupq .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ozcafllupq .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ozcafllupq .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #ozcafllupq .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #ozcafllupq .gt_left { text-align: left; } #ozcafllupq .gt_center { text-align: center; } #ozcafllupq .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #ozcafllupq .gt_font_normal { font-weight: normal; } #ozcafllupq .gt_font_bold { font-weight: bold; } #ozcafllupq .gt_font_italic { font-style: italic; } #ozcafllupq .gt_super { font-size: 65%; } #ozcafllupq .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #ozcafllupq .gt_asterisk { font-size: 100%; vertical-align: 0; } #ozcafllupq .gt_indent_1 { text-indent: 5px; } #ozcafllupq .gt_indent_2 { text-indent: 10px; } #ozcafllupq .gt_indent_3 { text-indent: 15px; } #ozcafllupq .gt_indent_4 { text-indent: 20px; } #ozcafllupq .gt_indent_5 { text-indent: 25px; } #ozcafllupq .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #ozcafllupq div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } carat cut color clarity depth table price x y z 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 0.29 Premium I VS2 62.4 58 334 4.20 4.23 2.63 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 Using the diamonds data file in the ggplot2 package, make a histogram of the price of diamonds, at different chunks. change the color of the bars put a white line around the bars separate diamonds of different color into different histogram evaluate how the graphics look with the different arrangements: position: “identity”, “stack”, “dodge” change color intensity change the names of the axes to Spanish save the graphics in .png, .tiff or .jpeg format. 12.9 DensityPlots 12.9.1 Area, density and line histogram plots with geom_dotplot, geom_density and geom-freqpoly Librerías necesarias para producir los gráficos que siguen library(ggversa) # paquete con los datos library(tidyverse) # paquete que instala múltiples paquetes library(gridExtra) # Un paquete para organizar las figuras de ggplot2 library(janitor) 12.10 Area chart with geom_area 12.10.1 Area Chart Section The area chart is analogous to a histogram or bar chart. The figure below shows how the quantity in X changes in frequency with respect to Y throughout the range of the variable X. Note below that the data is not grouped into bars as when using geom_histogram but represented as a continuous area. Note that unlike a histogram, the frequency changes are smoothed and not discrete. DW=dipodium DW=clean_names(DW) a=ggplot(DW, aes(distance)) a+geom_area(stat=&quot;bin&quot;, fill=&quot;aquamarine&quot;, color=&quot;black&quot;)+ labs(x=&quot;Distancia (m) al arbol más cercano&quot;, y=&quot;Frecuencia&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) In this following graph, the color of the line that follows the contour of the area is changed. To make this change you can identify the type of line with linetype, the color with color, the thickness of the line with size and the intensity of the color with alpha. a=ggplot(DW, aes(distance)) a+geom_area(stat=&quot;bin&quot;, fill=&quot;steelblue1&quot;, linetype=&quot;twodash&quot;, color=&quot;black&quot;, size=0.5, alpha=0.1)+ labs(x=&quot;Distancia&quot;, y=&quot;Frecuencia&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) 12.10.2 linetype types See “linetype” alternatives at this link linetype. Some of these lines can be called using a name “blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, “twodash”, or numbering “1F”, “F1”, “4C88C488”, “12345678. par(mar=c(0,0,0,0)) # Set up the plotting area plot(NA, xlim=c(0,1), ylim=c(10.5, -0.5), xaxt=&quot;n&quot;, yaxt=&quot;n&quot;, xlab=NA, ylab=NA ) # Draw the lines for (i in 0:10) { points(c(0.25,1), c(i,i), lty=i, lwd=2, type=&quot;l&quot;) } # Add labels text(0, 0, &quot;0. &#39;blank&#39;&quot; , adj=c(0,.5)) text(0, 1, &quot;1. &#39;solid&#39;&quot; , adj=c(0,.5)) text(0, 2, &quot;2. &#39;dashed&#39;&quot; , adj=c(0,.5)) text(0, 3, &quot;3. &#39;dotted&#39;&quot; , adj=c(0,.5)) text(0, 4, &quot;4. &#39;dotdash&#39;&quot; , adj=c(0,.5)) text(0, 5, &quot;5. &#39;longdash&#39;&quot;, adj=c(0,.5)) text(0, 6, &quot;6. &#39;twodash&#39;&quot; , adj=c(0,.5)) text(0, 7, &quot;6. &#39;1F&#39;&quot; , adj=c(0,.5)) text(0, 8, &quot;6. &#39;F1&#39;&quot; , adj=c(0,.5)) text(0, 9, &quot;6. &#39;4C88C488&#39;&quot; , adj=c(0,.5)) text(0, 10, &quot;6. &#39;12345678&#39;&quot; , adj=c(0,.5)) Now in the following graph, the color of the line is changed to black while the style of the line is changed with linetype and its thickness with size. a=ggplot(DW, aes(distance)) a+geom_area(stat=&quot;bin&quot;, fill=&quot;steelblue1&quot;, linetype=&quot;4C88C488&quot;, color=&quot;black&quot;, size=.5, alpha=0.5)+ labs(x=&quot;Distancia&quot;, y=&quot;Frecuencia&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) 12.11 geom_area with multiple groups Finally, the following graph shows the data on the frequency of hospital beds per 1,000 inhabitants in different years. tail(Camas_Hospital, n=6) ## Pais Year Poblacion Camas ## 129 Turkey 2006 68704721 2.7000 ## 130 Turkmenistan 2006 4801594 4.3331 ## 131 Ukraine 2006 46787750 8.7000 ## 132 United States 2006 298379912 3.1000 ## 133 Uruguay 2006 3331041 2.9000 ## 134 Yemen, Rep. 2006 21093973 0.7000 a=ggplot(Camas_Hospital, aes(Camas, fill=factor(Year))) a+geom_area(stat=&quot;bin&quot;,bins=60, alpha=0.5)+ xlab(&quot;Número de camas por \\n cada 1000 habitantes&quot;)+ ylab(&quot;Frecuencia&quot;)+ scale_y_continuous(breaks=c(0,1,2,3))+ # Cambio en la escala de eje scale_x_continuous(breaks=c(0:15))+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ facet_wrap(~Year) Camas_Hospital %&gt;% group_by(Year) %&gt;% count() ## # A tibble: 2 × 2 ## # Groups: Year [2] ## Year n ## &lt;int&gt; &lt;int&gt; ## 1 1996 67 ## 2 2006 67 12.12 The density function aes(y=..density..) One can use the stat option while identifying the Y axis to display the density and not the frequency of the data with the following modification aes(y=..density..) . This changes the graph’s display to density in the data instead of showing the count/frequency for each group. If one compares the two previous graphs to these new graphs one observes that the density of hospital beds per 100,000 changed with the years of inhabitants, there was an increase in density (proportionally more beds in 2006 per inhabitants). Note that there is no peak near one in 1996, but in 2006 the distribution is more dispersed among the values ​​and not concentrated near one. head(Camas_Hospital) ## Pais Year Poblacion Camas ## 1 Armenia 1996 3173425 7.13 ## 2 Australia 1996 18311000 8.50 ## 3 Austria 1996 7959017 9.30 ## 4 Azerbaijan 1996 7763000 9.81 ## 5 Bahamas, The 1996 283792 3.94 ## 6 Barbados 1996 265940 7.56 a=ggplot(Camas_Hospital, aes(Camas, fill=factor(Year))) a+geom_area(aes(y=..density..),stat=&quot;bin&quot;, alpha=0.5)+ xlab(&quot;Número de camas por cada 1000 habitantes&quot;)+ ylab(&quot;Densidad&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) 12.12.1 geom_area Options and Parameters: ggplot (the data file, aes(the continuous variable)) +geom_area(stat= bin, x, y, alpha, color, fill, linetype, size) alpha: the intensity of the color color: the color of the line around the area fill: the color of the area linetype: represents the line style size: represents the thickness of the line +stat: The default method is {identity}, which represents the data, or statistical transformation 12.13 Density plot with geom_density A density plot, also known as a probability density function, pdf or probability density function in English, is used with variables that contain continuous data. The density function is continuous over the range of values, and the sum of all the probabilities is equal to one. We saw previously that you can visualize the density also with geom_area and geom_histogram. The geom_density function facilitates the production of the graph and expands the alternatives as explained shortly. 12.13.1 What is a kernel? Estimating the density of data in a graph requires selecting a parameter, a kernel, to smooth the distribution. The most used is the Gaussian, which represents the normal distribution or commonly known as the bell-shaped distribution. If you do not specify which kernel to use, the normal distribution is the default; for example, when geom_density() is specified without any other options. Another alternative is to use geom_density(kernel = c(kernel={gaussian}), or other alternatives. Other parameters for kernel are: rectangular, triangular, epanechnikov, biweight, cosine, optcosine, gaussian The kernel is a special type of probability density function that has certain specific properties, whether it is non-negative and real-valued such that the graph is symmetrical, and the sum of the integral is equal to one. Also added geom_density to compare the result of the two functions. Note that in the following graph the parameter alpha=0.4 was used. This modifies the transparency of the blue color of the fill=blue parameter. The intensity of alpha=0.4 can vary from 0 to 1, as explained above. The example below uses Dipodium rosea data again. a=ggplot(DW, aes(distance)) a+geom_area(aes(y=..density..),stat=&quot;bin&quot;, alpha=0.5)+ geom_density(kernel = c(kernel=&quot;gaussian&quot;), alpha=0.4, fill=&quot;blue&quot;)+ labs(x=&quot;Distancia&quot;, y=&quot;Densidad&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) #ggsave(&quot;Distance_to_tree.jpeg&quot;) #.tiff, .png x=rnorm(10, 10, 4) x=data_frame(x) ## Warning: `data_frame()` was deprecated in tibble 1.1.0. ## ℹ Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. a=ggplot(x, aes(x)) a+geom_area(aes(y=..density..),stat=&quot;bin&quot;, alpha=0.5)+ geom_density(kernel = c(kernel=&quot;gaussian&quot;), alpha=0.4, fill=&quot;blue&quot;)+ labs(x=&quot;valus&quot;, y=&quot;Densidad&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 12.14 geom_density and simulated data The role of geom_density can be better understood by evaluating it with simulated data. Let’s next simulate data with different sample sizes to visualize the densities. In the simulation below, 4 data frames are created with 2000, 500, 50 and 10 data respectively with the rnorm function. Naturally, what is observed is that if the data comes from a normal distribution, the more data that is included, the closer the corresponding distribution is to what a normal distribution should look like. But, the opposite is that with little data, the density is likely not to resemble the theoretical (normal) distribution. a=rnorm(20000, 0, 1) a=as.data.frame(a) a=ggplot(a, aes(a))+ geom_density(kernel = c(kernel=&quot;gaussian&quot;), alpha=0.4, fill=&quot;yellow&quot;)+ labs(y=&quot;Densidad&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) b=rnorm(500, 0, 1) b=as.data.frame(b) b=ggplot(b, aes(b))+ geom_density(kernel = c(kernel=&quot;gaussian&quot;), alpha=0.4, fill=&quot;red&quot;)+ labs(y=&quot;Densidad&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) c=rnorm(50, 0, 1) c=as.data.frame(c) c=ggplot(c, aes(c))+ geom_density(kernel = c(kernel=&quot;gaussian&quot;), alpha=0.4, fill=&quot;blue&quot;)+ labs(y=&quot;Densidad&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) d=rnorm(10, 0, 1) d=as.data.frame(d) d=ggplot(d, aes(d))+ geom_density(kernel = c(kernel=&quot;gaussian&quot;), alpha=0.4, fill=&quot;grey&quot;)+ labs(y=&quot;Densidad&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) 12.14.1 geom_density Options and Parameters: ggplot(the data file, aes(the continuous variable)) geom_density(kernel= {…}), x, y, alpha, color, fill, linetype, size, weight ***** represents the desired parameter; e.g. gaussian, triangular, rectangular, etc. alpha: the intensity of the color fill: the color of the area color: the color of the line around the area linetype: represents the line style size: represents the thickness of the line weight: to modify the original value; then it would not be, for example, the count or sum of the values ​​but a weighted value (weighted average) 12.15 Polygon frequency graph with geom_freqpoly The polygon frequency graph is similar to the area and density graph, the difference is that the area is not filled with color. You can also change the number of bins using binwidth. In the polygon plot, it is only the line that we plot and there is no fill parameter of the area below the line. a=ggplot(DW, aes(distance)) a+geom_freqpoly(binwidth=.1, color=&quot;#e3cc36&quot;)+ # Nota como seleccionar el color con &quot;color picker&quot; en el web. labs(x=&quot;Distancia (m)&quot;, y=&quot;Frecuencia&quot;)+ # labels = labs theme(axis.title=element_text(size=14,face=&quot;italic&quot;)) Modify other options as follows: color intensity with alpha, line type with linetype and line thickness with size as shown below. DW%&gt;% drop_na()%&gt;% ggplot(aes(distance, colour=species_name))+ geom_freqpoly(alpha=1.0, size=1.0, binwidth=.1, linetype=&quot;longdash&quot;)+ labs(x=&quot;Distancia&quot;, y=&quot;Frecuencia&quot;)+ theme(axis.title=element_text(size=14,face=&quot;bold&quot;)) # ggsave(&quot;the_name_of_my_figure.tiff&quot;) # .png, .tiff, .pdf, .jpeg DW ## # A tibble: 1,363 × 21 ## tree_number tree_species dbh plant_number ramet_number distance orientation ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 E.o 75 1 1 2.47 40 ## 2 1 E.o 76 2 1 1.97 50 ## 3 2 E.o 76 3 1 1.95 350 ## 4 3 E.o 58 4 1 3.24 210 ## 5 4 E.o NA 5 1 0.85 80 ## 6 5 E.o 59 6 1 2.62 160 ## 7 5 E.o 59 7 1 2.82 170 ## 8 6 E.o 8 8 1 3.12 245 ## 9 7 E.o 11.5 9 1 1.12 208 ## 10 8 E.o 8.5 10 1 0.75 360 ## # ℹ 1,353 more rows ## # ℹ 14 more variables: number_of_flowers &lt;int&gt;, height_inflo &lt;int&gt;, ## # herbivory &lt;chr&gt;, row_position_nf &lt;int&gt;, number_flowers_position &lt;int&gt;, ## # number_of_fruits &lt;int&gt;, perc_fr_set &lt;dbl&gt;, pardalinum_or_roseum &lt;chr&gt;, ## # fruit_position_effect &lt;int&gt;, frutos_si_o_no &lt;int&gt;, ## # p_or_r_infl_lenght &lt;chr&gt;, num_of_fruits &lt;int&gt;, species_name &lt;chr&gt;, ## # cardinal_orientation &lt;int&gt; 12.15.1 Opciones y Parametros de geom_freqpoly ggplot(the data file, aes(the continuous variable)) geom_freqpoly(stat={bin}, x, y, alpha, color, linetype, size) alpha: the intensity of the color color: the color of the line around the area +linetype: represents the line style; see section size: represents the thickness of the line Activity Use the “dipodium” data set in the “ggversa” package. Presents a graph of the frequency of flowers per plant with geom_freqpoly. Change the color of the line Change axis information for more relevant text Change the color intensity of the line Change the line type "],["boxplots.html", "Chapter 13 Boxplots 13.1 Boxplot with geom_boxplot 13.2 Teoretical boxplot 13.3 geom_boxplot Options and Parameters: 13.4 Violin diagram with geom_violin 13.5 geom_violin not in proportion to sample size 13.6 geom_violin and orientation change 13.7 geom_violin Options and Parameters:", " Chapter 13 Boxplots library(conflicted) conflicts_prefer(lubridate::minute) ## [conflicted] Removing existing preference. ## [conflicted] Will prefer lubridate::minute over any other package. library(gridExtra)# Para organizar múltiples gráficos juntos library(tidyverse) library(ggversa) 13.1 Boxplot with geom_boxplot For data that does not have a normal distribution, a box plot is typically used to visualize the distribution of the data. The box plot distributes the data based on the range of size order or numerical value. First, the data is ordered from smallest to largest. The values in the corresponding graph represent the 25th, 50th, and 75th quartiles. The whiskers (the lines extending from the boxes) represent 1.5 times the interquartile range (IQR), or the distance between the first quartile (25th) and the third quartile (75). Data that is outside that range is represented by dots. Charts that use notches, or notch, calculate them according to the following formula: (1.58 x IQR) / (square root of the sample). We will again use data from the Dipodium orchid in the following examples, particularly representing the number of flowers and the number of fruits. A basic boxplot is shown in the first graph below, Fig. 1 (top left). Note that the variable in the first x has a value of 1, which means there is only one group. In the second graph, Fig. 2 (top right), it is modified with the notch option. In the third graph, Figure Fig. 3 (bottom left), the color of the box, the shape or shape of the outliers**, and their size were changed. In the first three graphs, all the data are used in the same box plot. In the fourth graph, Fig. 4 (bottom right), we observe the distribution of the number of flowers by the number of fruits observed per plant. Note that for this last case in particular, there is more than one group. The notches The notches on the sides of a boxplot can be interpreted as a comparison interval around the median values. The notch height is the median \\(+/- 1.57 x IQR/sqrt(n)\\) where IQR is the interquartile range defined by the 25th and 75th percentiles and n is the number of data points 13.2 Teoretical boxplot Below are two theoretical boxplot, the first one is a normal boxplot and the second one is a boxplot with outliers below and above the range. While the second figure shows a boxplot with outliers, the first figure shows a typical boxplot. The data used in the first figure is a tibble with 1, 8, 9, 10, 100 to 120, 125 to 135, 150 to 160, and 200. The second figure uses the same data as the first figure, but with the addition of 800 and 1000. df1=as.tibble(c(1,8:10, 100:120, 125:135,150:160, 200)) ## Warning: `as.tibble()` was deprecated in tibble 2.0.0. ## ℹ Please use `as_tibble()` instead. ## ℹ The signature and semantics have changed, see `?as_tibble`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. df1 %&gt;% ggplot(aes(value, x=1))+ geom_boxplot() df=as.tibble(c(1,8:10, 100:120, 125:135,150:160, 800, 1000)) df ## # A tibble: 49 × 1 ## value ## &lt;dbl&gt; ## 1 1 ## 2 8 ## 3 9 ## 4 10 ## 5 100 ## 6 101 ## 7 102 ## 8 103 ## 9 104 ## 10 105 ## # ℹ 39 more rows df %&gt;% ggplot(aes(value, x=1))+ geom_boxplot() Here we show a boxplot for the number of fruits per plant #names(dipodium) library(conflicted) library(janitor) library(gt) dipodium=clean_names(dipodium) gt(head(dipodium)) #whzhbofhdu table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #whzhbofhdu thead, #whzhbofhdu tbody, #whzhbofhdu tfoot, #whzhbofhdu tr, #whzhbofhdu td, #whzhbofhdu th { border-style: none; } #whzhbofhdu p { margin: 0; padding: 0; } #whzhbofhdu .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #whzhbofhdu .gt_caption { padding-top: 4px; padding-bottom: 4px; } #whzhbofhdu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #whzhbofhdu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #whzhbofhdu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #whzhbofhdu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #whzhbofhdu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #whzhbofhdu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #whzhbofhdu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #whzhbofhdu .gt_column_spanner_outer:first-child { padding-left: 0; } #whzhbofhdu .gt_column_spanner_outer:last-child { padding-right: 0; } #whzhbofhdu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #whzhbofhdu .gt_spanner_row { border-bottom-style: hidden; } #whzhbofhdu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #whzhbofhdu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #whzhbofhdu .gt_from_md > :first-child { margin-top: 0; } #whzhbofhdu .gt_from_md > :last-child { margin-bottom: 0; } #whzhbofhdu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #whzhbofhdu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #whzhbofhdu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #whzhbofhdu .gt_row_group_first td { border-top-width: 2px; } #whzhbofhdu .gt_row_group_first th { border-top-width: 2px; } #whzhbofhdu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #whzhbofhdu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #whzhbofhdu .gt_first_summary_row.thick { border-top-width: 2px; } #whzhbofhdu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #whzhbofhdu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #whzhbofhdu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #whzhbofhdu .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #whzhbofhdu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #whzhbofhdu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #whzhbofhdu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #whzhbofhdu .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #whzhbofhdu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #whzhbofhdu .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #whzhbofhdu .gt_left { text-align: left; } #whzhbofhdu .gt_center { text-align: center; } #whzhbofhdu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #whzhbofhdu .gt_font_normal { font-weight: normal; } #whzhbofhdu .gt_font_bold { font-weight: bold; } #whzhbofhdu .gt_font_italic { font-style: italic; } #whzhbofhdu .gt_super { font-size: 65%; } #whzhbofhdu .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #whzhbofhdu .gt_asterisk { font-size: 100%; vertical-align: 0; } #whzhbofhdu .gt_indent_1 { text-indent: 5px; } #whzhbofhdu .gt_indent_2 { text-indent: 10px; } #whzhbofhdu .gt_indent_3 { text-indent: 15px; } #whzhbofhdu .gt_indent_4 { text-indent: 20px; } #whzhbofhdu .gt_indent_5 { text-indent: 25px; } #whzhbofhdu .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #whzhbofhdu div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } tree_number tree_species dbh plant_number ramet_number distance orientation number_of_flowers height_inflo herbivory row_position_nf number_flowers_position number_of_fruits perc_fr_set pardalinum_or_roseum fruit_position_effect frutos_si_o_no p_or_r_infl_lenght num_of_fruits species_name cardinal_orientation 1 E.o 75 1 1 2.47 40 11 35 n 1 24 0 0.00 r 1 0 r 0 r 1 1 E.o 76 2 1 1.97 50 19 47 n 2 23 0 0.00 r 2 0 r 0 r 2 2 E.o 76 3 1 1.95 350 18 63 n 3 25 1 0.04 r 3 0 r 1 r 8 3 E.o 58 4 1 3.24 210 24 47 n 4 20 5 0.25 r 4 0 r 5 r 5 4 E.o NA 5 1 0.85 80 25 61 n 5 13 0 0.00 r 5 0 r 0 r 2 5 E.o 59 6 1 2.62 160 17 35 n 6 25 2 0.08 p 6 0 r 2 p 4 d1=dipodium %&gt;% dplyr::select(number_of_fruits, number_of_flowers) %&gt;% drop_na() #d1 boxplot1 &lt;- ggplot(d1, aes(number_of_fruits, x=1))+ geom_boxplot()+ annotate(&quot;text&quot;, x= 1,y= 5, label=&quot;Fig. 1&quot;)+ labs(x=&quot;&quot;, y=&quot;Cantidad de Frutos&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) boxplot2 &lt;- ggplot(d1, aes(number_of_fruits, x=1))+ geom_boxplot(notch=TRUE) + annotate(&quot;text&quot;, x= 0.7,y= 7, label=&quot;Fig. 2&quot;)+ labs(x=&quot;&quot;, y=&quot;Cantidad de Frutos&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) boxplot3 &lt;- ggplot(d1, aes(number_of_fruits, x=1))+ geom_boxplot(notch=TRUE,colour=&quot;blue&quot;, fill=&quot;orange&quot;, alpha=0.7, outlier.shape=18, outlier.color=&quot;red&quot;, outlier.size=3)+ annotate(&quot;text&quot;, x= 0.7,y= 7, label=&quot;Fig. 3&quot;)+ labs(x=&quot;&quot;, y=&quot;Cantidad de Frutos&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) boxplot4 &lt;- ggplot(d1, aes(factor(number_of_flowers), number_of_fruits))+ geom_boxplot(notch=FALSE)+ annotate(&quot;text&quot;, x= 2,y= 10, label=&quot;Fig. 4&quot;)+ labs(y=&quot;Cantidad de Frutos&quot;, x=&quot;Cantidad de Flores&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ theme(axis.text.x = element_text(angle = 90)) gridExtra::grid.arrange(boxplot1,boxplot2, boxplot3, boxplot4,ncol=2) 13.2.1 Change the width of the boxes Change the width of the boxes with varwidth. This produces a standard boxplot and defaults to FALSE; if TRUE, produces a plot where the width on the X axis is proportional to the square root of the number of observations in the groups. dipodium %&gt;% dplyr::select(number_of_flowers, number_of_fruits) %&gt;% drop_na() %&gt;% ggplot(aes(factor(number_of_flowers), number_of_fruits, fill=factor(number_of_flowers)))+ geom_boxplot(varwidth=TRUE)+ theme(legend.position = &quot;none&quot;)+ xlab(&quot;Cantidad de Flores&quot;)+ ylab(&quot;Cantidad de Frutos&quot;) # la función varwidth =TRUE cambia #el ancho de la barras basado en una formula indicado abajo. # Más datos más ancho la barra. Prepare a box plot or also called Box and whiskers. Uses the ElphickBirdData data in the ggversa package. It uses the AQBIRDS column, which is the number of aquatic birds sampled by Zuur, Ieno and Elphick. The bird species named = Godwit is a migratory bird that eats small mollusks on the beaches. Use the “SITE” column to create a box plot with the “notches” for each sampling site. 13.3 geom_boxplot Options and Parameters: ggplot(the data file, aes(the continuous variable, x=1)): use {x=1} if only one group or the name of the discrete variable if there are multiple groups geom_boxplot(stat, position, outlier.color, outlier.shape, outlier.size, notch, notchwidth, varwidth) stat: statistical transformation of the data; It is only needed if you want to override the default method used by geom_boxplot position: to adjust the overlap of the data outlier.color: defines the color of outliers outlier.shape: defines the shape of the outliers outlier.size: defines the size of the outliers notch: to produce the notches; is defined with the values TRUE or FALSE notchwidth: defines the width of the notches relative to the case (default is 0.5) varwidth: produces a standard boxplot and defaults to FALSE; if TRUE, produces a plot where the width on the X axis is proportional to the square root of the number of observations in the groups. 13.4 Violin diagram with geom_violin The violin plot is similar to the box plot, but the box is curved to give an appreciation of the density of the data. If the data distribution follows a normal curve, it will appear as a vertically oriented bell curve. For the examples below, we will use data from the Australian terrestrial orchid Caladenia valida. The basic model of the violin plot is shown in Figure. Note that, as with geom_boxplot, if you have only one group, you have to include x=1. What one observes is that most of the information is found close to the value 5 and 10 library(ggplot2) CalVal=caladeniavalida violinplot1 &lt;- ggplot(CalVal, aes(DCL, x=1)) violinplot1 + geom_violin()+ annotate(&quot;text&quot;, x= 0.75,y= 19, size=8, label=&quot;&quot;, na.rm=TRUE)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ ylab(&quot;EL largo del apendizaje dorsal&quot;) 13.4.1 Geom_violin with multiple groups Next we are going to produce the violin graph by different factors or groups according to the year, while adding blue color to the violins and changing the intensity of the color with alpha=(). violinplot2 &lt;- ggplot(CalVal, aes(y=LCL, x=factor(Year))) violinplot2 + geom_violin(fill=&quot;blue&quot;, alpha=0.5)+ annotate(&quot;text&quot;, x=2,y= 19, size=8, label=&quot;&quot;, na.rm=TRUE)+ labs(x=&quot;Año&quot;, y=&quot;LCL&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ ylab(&quot;EL largo del apendizaje lateral&quot;) The violin plot with normal distribution normal_data=rnorm(1000, 0, 4) #normal_data dfnorm=data.frame(normal_data) head(dfnorm) ## normal_data ## 1 0.03616168 ## 2 2.47747037 ## 3 1.53797891 ## 4 -2.00046929 ## 5 0.39941824 ## 6 -7.80536567 library(ggplot2) ggplot(dfnorm,aes(normal_data, x=1))+ geom_violin()+ geom_boxplot()+ geom_jitter(position = position_jitter(width = .1)) 13.4.2 Geom_violin used with geom_point and geom_jitter Data can be added to the previous graph to better visualize the distribution. Plant height data OH were collected discretely in 5 cm units, so all similar values will appear overlapping one another. Due to this particular situation of that type of data, adding points to visualize them with the geom_point function does not really help much to understand its distribution. This is because you cannot appreciate the amount of data used to build the violins. On the other hand, one can use geom_jitter and have a better appreciation of its distribution. It is important to take into consideration that if geom_point is added before geom_violin, the points could be hidden behind the violin. violingp=violinplot3a &lt;- ggplot(CalVal, aes(y=OH, x=factor(Year)))+ geom_violin()+ geom_point()+ # Note que los puntos serán sobrepuestos sobre el violín annotate(&quot;text&quot;, x=2,y= 20, size=8, label=&quot;&quot;, na.rm=TRUE)+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) violingv=violinplot3a + geom_violin()+ geom_jitter(position = position_jitter(width = .1))+ # Note que los puntos serán sobrepuestos sobre el violín pero no solapan uno encima del otro. annotate(&quot;text&quot;, x=2,y= 20, size=8, label=&quot;&quot;, na.rm=TRUE)+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ xlab(&quot;Overall height, mm&quot;) grid.arrange(violingp, violingv, ncol=2) ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). ## Warning: Removed 1 row containing missing values or values outside the scale range ## (`geom_point()`). ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). ## Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). ## Warning: Removed 1 row containing missing values or values outside the scale range ## (`geom_point()`). ## Removed 1 row containing missing values or values outside the scale range ## (`geom_point()`). 13.4.3 geom_jitter and geom overlap In the next graph, geom_jitter was called before geom_violin, but at the same time the color intensity was changed so that the points are better visible. We see that some of the points that were half hidden before and those that do not overlap the violin are better revealed. violinplot3 &lt;- ggplot(CalVal, aes(y=OH, x=factor(Year))) violinplot3 + geom_jitter(position = position_jitter(width = .2), colour=&quot;red&quot;)+ geom_violin(fill=&quot;yellow&quot;, colour=&quot;blue&quot;, alpha=0.3)+ annotate(&quot;text&quot;, x=2,y= 20, size=8, label=&quot;&quot;)+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). ## Warning: Removed 1 row containing missing values or values outside the scale range ## (`geom_point()`). 13.4.4 Quartiles and geom_violin The 25th quartile, median (50th), and 75th quartile are now displayed on the same violin plot with horizontal lines. violinplot3 &lt;- ggplot(CalVal, aes(y=OH, x=factor(Year))) violinplot3 + geom_violin(draw_quantiles = c(0.025, 0.25,0.5, 0.75,0.975))+ annotate(&quot;text&quot;, x=2,y= 20, size=8, label=&quot;&quot;)+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). 13.4.5 Adding color to the outline of the violin In this figure, color is added to the outline of the violins and the quartile lines. library(ggversa) names(caladeniavalida) ## [1] &quot;Population&quot; &quot;Year&quot; &quot;Plant_num&quot; &quot;OH&quot; &quot;OD&quot; ## [6] &quot;OW&quot; &quot;DSL&quot; &quot;DCL&quot; &quot;LSL&quot; &quot;LCL&quot; ## [11] &quot;LSW&quot; &quot;PL&quot; &quot;LL&quot; &quot;LW&quot; &quot;Fruit_not&quot; violinplot3 &lt;- ggplot(CalVal, aes(y=OH, x=Population)) violinplot3 + geom_violin(colour=&quot;red&quot;, alpha=.2, draw_quantiles = c(0.25, 0.5, 0.75))+ annotate(&quot;text&quot;, x=2,y= 20, size=8, label=&quot;&quot;)+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). 13.4.6 geom_violin by category or factor In the Figure, the color of each group is changed, Year. To achieve this effect, another variable is added to represent the groups (or categories) by color; in this case, the distribution of plant sizes in terms of whether they produced fruit or not. violinplot3 &lt;- ggplot(na.omit(CalVal), aes(y=OH, x=factor(Year))) violinplot3 + geom_violin(aes(fill=(factor(Fruit_not))), draw_quantiles = c(0.25, 0.5, 0.75))+ annotate(&quot;text&quot;, x=2,y= 20, size=8, label=&quot;&quot;)+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ ylab(&quot;Altura de la planta&quot;) 13.4.7 Geom_violin in proportion to sample size In Figure, the same graph is modified to take into account the sample size using the scale option with the count parameter. In other words, the violins will be modified to include the sample size; that is, the maximum width scale is proportional to the size of the sample. So, in this case we see the year where there was the largest (2007) and smallest (2008) sample size according to the width shown for each violin. violinplot3 &lt;- ggplot(CalVal, aes(y=OH, x=factor(Year))) violinplot3 + geom_violin(scale = &quot;count&quot;, draw_quantiles = c(0.25, 0.5, 0.75))+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). 13.5 geom_violin not in proportion to sample size In the Figure the width of the violin is modified to 1 on the scale of the X axis. In other words, in this case the width width of the violin for the first year goes from 2003.5 to 2004.5, for the second violin from 2004.5 to 2005.5 and so on consecutively for the other years. This approach creates a degree of uniformity for each width. In this case the width is not related to the sample size like the previous example. violinplot3 &lt;- ggplot(CalVal, aes(y=OH, x=factor(Year))) violinplot3 + geom_violin(scale = &quot;width&quot;, draw_quantiles = c(0.25, 0.5, 0.75))+ labs(x=&quot;Año&quot;, y=&quot;OH&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;)) ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). 13.6 geom_violin and orientation change In the Figure we change the orientation of the previous graphs using the coord_flip() option. violinplot3 &lt;- ggplot(CalVal, aes(y=OH, x=factor(Year))) violinplot3 + geom_violin()+ coord_flip()+ labs(x=&quot;Año&quot;)+ theme(axis.title=element_text(size=10,face=&quot;bold&quot;))+ ylab(&quot;Altura de la planta&quot;) ## Warning: Removed 1 row containing non-finite outside the scale range ## (`stat_ydensity()`). Density graphs with colors for the different quantiles dt &lt;- data.table::data.table(x=c(1:200),y=rnorm(200)) # Crear un distribucion de 200 datos al azar con una distribución normal dens &lt;- density(dt$y) # calcular la densidad df &lt;- data.frame(x=dens$x, y=dens$y) # crear un data frame de la densidad probs &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9) # determinar cual cuantil quiere evaluar quantiles &lt;- quantile(dt$y, prob=probs) # asignar cada valor a uno de los cuantiles df$quant &lt;- factor(findInterval(df$x,quantiles)) # Asignar los cuantiles a factores ggplot(df, aes(x,y)) + geom_line() + geom_ribbon(aes(ymin=0, ymax=y, fill=quant)) + scale_x_continuous(breaks=quantiles) + scale_fill_brewer(guide=&quot;none&quot;) CalVal2= CalVal %&gt;% dplyr::select(OH) %&gt;% drop_na() # Seleccionar lo datos deseados removiendo los &quot;NA&quot; dens2=density(CalVal2$OH) dens2 ## ## Call: ## density.default(x = CalVal2$OH) ## ## Data: CalVal2$OH (163 obs.); Bandwidth &#39;bw&#39; = 2.94 ## ## x y ## Min. :16.18 Min. :9.320e-06 ## 1st Qu.:34.34 1st Qu.:8.159e-04 ## Median :52.50 Median :5.032e-03 ## Mean :52.50 Mean :1.374e-02 ## 3rd Qu.:70.66 3rd Qu.:2.621e-02 ## Max. :88.82 Max. :5.660e-02 df2 &lt;- data.frame(x=dens2$x, y=dens2$y) head(df2) ## x y ## 1 16.17920 9.561940e-06 ## 2 16.32136 1.105205e-05 ## 3 16.46352 1.272153e-05 ## 4 16.60567 1.469481e-05 ## 5 16.74783 1.691094e-05 ## 6 16.88998 1.938565e-05 probs &lt;- c(0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, .99) quantiles &lt;- quantile(CalVal2$OH, prob=probs) df2 &lt;- data.frame(x=dens2$x, y=dens2$y) df2$quant &lt;- factor(findInterval(df2$x,quantiles)) head(df2) ## x y quant ## 1 16.17920 9.561940e-06 0 ## 2 16.32136 1.105205e-05 0 ## 3 16.46352 1.272153e-05 0 ## 4 16.60567 1.469481e-05 0 ## 5 16.74783 1.691094e-05 0 ## 6 16.88998 1.938565e-05 0 ggplot(df2, aes(x,y)) + geom_ribbon(aes(ymin=0, ymax=y, fill=quant)) + scale_x_continuous(breaks=quantiles) + scale_fill_brewer(palette = &quot;Set1&quot;,guide=&quot;none&quot;)+ # colores horribles theme(axis.text.x = element_text(angle = 45)) Prepare a violin chart. Uses the CypripediumA data in the ggversa package. Use the column Lip_length_mm, which is the lip length of the orchid Cypripedium acaule. The orchid species lives in northern Canada in forests (Forest) or swamps (Wetlands). Use the Forest_Wetland column to create a fiddle box for each sampling site. Color the two groups. Evaluating the distribution of the data on labellum size, do you believe that the size distribution is equal? 13.7 geom_violin Options and Parameters: ggplot(the data file, aes(the continuous variable, x=1)): {x=1} if a group or the name of the discrete variable if there are multiple groups geom_violin(x, y, alpha, color, fill, linetype, size, weight) alpha: the intensity of the color color: the color of the line around the violin fill: the color used to fill the violin linetype: represents the line style; see section size: represents the thickness of the line scale: the scale used to produce the violin taking into account the sample size proportionally; uses count or width parameters weight: to modify the original value; then it would not be, for example, the count/sum of the values but a weighted value (weighted average). "],["other-visualizations.html", "Chapter 14 Other visualizations 14.1 Barplot 14.2 Line chart 14.3 Heat map 14.4 Plot of mean and confidence intervals 14.5 MAJOR error: CAUTION", " Chapter 14 Other visualizations 14.1 Barplot Barplot is a common way to visualize the distribution of a categorical variable. In this example, we will use the ggplot2 package to create a barplot. This should be used when you have count data and want to compare the counts of different categories. Bar are not used to show mean or median values, but to show the frequency of the data. library(ggplot2) # Load the Anolis dataset from the ggversa package library(ggversa) #Create a bar plot of number of male, female and juvenile Anolis lizards in the SEX_AGE variable ggplot(Anolis, aes(x = SEX_AGE)) + geom_bar() 14.2 Line chart Line charts are used to show trends over time or other continuous variables. In this example, we will use the ggplot2 package to create a line chart of the death rate caused by Tiroide cancer variable in the Tiroide dataset in the library(ggversa). library(ggversa) ggplot(Tiroide, aes(x = Year, y = Rate, colour=Cases, group = Cases)) + geom_line() 14.3 Heat map Heat map are an effective visualization technique for showing patterns in large datasets. In this example, we will use the ggplot2 package to create a heat map of the SparrowElphick dataset in the ggversa library. Let select a subset of the variables in the SparrowElphick dataset to create a heat map. We will use the dplyr package to select the variables flatwing. tarsus, head, culmen and wt. the variables definition are lenght of the wing when expanded, the length of the tarsus, the length of the head, the length of the culmen and the weight of the bird respectively. head(SparrowsElphick) ## wingcrd flatwing tarsus head culmen nalospi wt bandstat initials Year Month ## 1 59.0 60.0 22.3 31.2 12.3 13.0 9.5 1 2 2002 9 ## 2 54.0 55.0 20.3 28.3 10.8 7.8 12.2 1 2 2002 10 ## 3 53.0 54.0 21.6 30.2 12.5 8.5 13.8 1 2 2002 10 ## 4 55.0 56.0 19.7 30.4 12.1 8.3 13.8 1 8 2002 7 ## 5 55.0 56.0 20.3 28.7 11.2 8.0 14.1 1 3 2002 10 ## 6 53.5 54.5 20.8 30.6 12.8 8.6 14.8 1 7 2004 8 ## Day Location SpeciesCode Sex Age ## 1 19 4 1 0 2 ## 2 4 4 3 0 2 ## 3 4 4 3 0 2 ## 4 30 9 1 0 2 ## 5 4 4 3 0 2 ## 6 2 1 1 0 2 Sparrow&lt;- dplyr::select(SparrowsElphick, flatwing, tarsus, head, culmen, wt) Now let us calculate the Kendall correlation between the variables in the Sparrow dataset. The Kendall correlation is a non-parametric measure of association between two variables. It is used to measure the strength and direction of the relationship between two variables. The Kendall correlation ranges from -1 to 1, where -1 indicates a perfect negative relationship, 0 indicates no relationship, and 1 indicates a perfect positive relationship. We will round the values to 3 significant figures. round(cor(Sparrow, method = &quot;kendall&quot;, use=&quot;pairwise.complete.obs&quot;), digits = 3) ## flatwing tarsus head culmen wt ## flatwing 1.000 0.347 0.350 0.233 0.457 ## tarsus 0.347 1.000 0.375 0.275 0.365 ## head 0.350 0.375 1.000 0.472 0.443 ## culmen 0.233 0.275 0.472 1.000 0.325 ## wt 0.457 0.365 0.443 0.325 1.000 The next step is creating a data frame from the correlation matrix. Note here we reshape the data frame to a long format using the melt function from the reshape2 package. This is because the ggplot2 package requires data in a long format to create a heat map. melted_cor &lt;- reshape2::melt(cor(Sparrow, method = &quot;kendall&quot;, use=&quot;pairwise.complete.obs&quot;)) melted_cor ## Var1 Var2 value ## 1 flatwing flatwing 1.0000000 ## 2 tarsus flatwing 0.3465829 ## 3 head flatwing 0.3504429 ## 4 culmen flatwing 0.2328006 ## 5 wt flatwing 0.4569716 ## 6 flatwing tarsus 0.3465829 ## 7 tarsus tarsus 1.0000000 ## 8 head tarsus 0.3754616 ## 9 culmen tarsus 0.2745705 ## 10 wt tarsus 0.3648428 ## 11 flatwing head 0.3504429 ## 12 tarsus head 0.3754616 ## 13 head head 1.0000000 ## 14 culmen head 0.4717295 ## 15 wt head 0.4429692 ## 16 flatwing culmen 0.2328006 ## 17 tarsus culmen 0.2745705 ## 18 head culmen 0.4717295 ## 19 culmen culmen 1.0000000 ## 20 wt culmen 0.3245723 ## 21 flatwing wt 0.4569716 ## 22 tarsus wt 0.3648428 ## 23 head wt 0.4429692 ## 24 culmen wt 0.3245723 ## 25 wt wt 1.0000000 Now let us create a heat map of the correlation matrix using the ggplot2 package. The blue color indicates a negative correlation, the red color indicates a positive correlation, and the white color indicates no correlation. In this case there are no negative correlation, but there are a range of positive correlation values. ggplot(melted_cor, aes(Var1, Var2, fill = value)) + geom_tile() + scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) There is an alternative to the above heat map, using the package GGally. This package provides a function called ggcorr that creates a correlation matrix plot. The ggcorr function creates a heat map of the correlation matrix with the correlation values displayed in the cells. Note now we are facilitating the visualization of the data and only add one line of code to create the heat map. To observe the correlation values, we can use the label = TRUE argument. To change the number of decimal places, we can use the label_round argument. library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ggcorr(Sparrow, label_round=3,label = TRUE) 14.4 Plot of mean and confidence intervals In this example, we will use the ggplot2 package to create a plot of the mean and confidence intervals of the weight variable in the SparrowsElphick dataset in the ggversa library. We will use the stat_summary function to calculate the mean and confidence intervals of the weight variable and the geom_errorbar function to plot the confidence intervals. We calculates the mean and confidence intervals using the mean_cl_normal for each of the species in the dataset (note the species are coded 1, 2 and 3, these were changed to factors) . ggplot(SparrowsElphick, aes(x = as.factor(SpeciesCode), y = wt)) + stat_summary(fun.data = mean_cl_normal, geom = &quot;point&quot;) + stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;, width = 0.2)+ ylab(&quot;Mean and CI of the weight variable&quot;) 14.5 MAJOR error: CAUTION In many papers the to show the mean the authors draw a barplot and then they and the CI. This is a major error. The barplot is used to show the frequency of the data, not the mean. The mean should be shown with a point and the CI with a line. SO THE figure below should NEVER be used to show the mean and CI of the data. Compare the two figures and you will see that the previous figure with a point and a line is the correct way to show the mean and CI of the data as it better reflects the data. ggplot(SparrowsElphick, aes(x = as.factor(SpeciesCode), y = wt)) + stat_summary(fun.data = mean_cl_normal, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;, width = 0.2)+ ylab(&quot;Mean and CI of the weight variable&quot;) "],["linear-regression.html", "Chapter 15 Linear Regression 15.1 Model of linear regression 15.2 Selling music records 15.3 Cook’s Distance", " Chapter 15 Linear Regression A linear regression model is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable. The linear regression model is represented by the equation: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n + \\epsilon\\] where: \\(Y\\) is the dependent variable \\(\\beta_0\\) is the intercept \\(\\beta_1, \\beta_2, ... , \\beta_n\\) are the coefficients \\(X_1, X_2, ... , X_n\\) are the independent variables \\(\\epsilon\\) is the error term The coefficients \\(\\beta_0, \\beta_1, \\beta_2, ... , \\beta_n\\) are estimated using the least squares method, which minimizes the sum of the squared differences between the observed values of the dependent variable and the values predicted by the model. In this section, we will explore the linear regression model using the lm function in R and visualize the model using the ggplot2 package. 15.1 Model of linear regression Load packages We will be evaluating only linear regression We see a fictitious example of one explanatory variable and one dependent variable, each from a normal distribution. The ramdon data is generated using the rnorm function. rnorm(n=sample size, mean, sd). The set.seed function is used to ensure reproducibility of the results. ``` r set.seed(123) response= rnorm(1000, mean=100, sd=1) # for example number of seeds per plant explanatory= rnorm(1000, mean= 40, sd=5) # height of the plant data=data.frame(response, explanatory) # join the two variables in a data frame head(data) # show the data frame ``` ## response explanatory ## 1 99.43952 35.02101 ## 2 99.76982 34.80022 ## 3 101.55871 39.91010 ## 4 100.07051 39.33912 ## 5 100.12929 27.25329 ## 6 101.71506 45.20287 Visualize the data using a scatter plot - Note that cloud of data points is scattered and is distributed more or less equally across the distrbution. r ggplot(data, aes(x=explanatory, y=response))+ geom_point() We can show the distriution of the data using the package ggside and the function geom_ysidedensity and geom_xsidedensity. This function shows the distribution of the data on the y and x axis. r library(ggside) ## Registered S3 method overwritten by 'ggside': ## method from ## +.gg GGally r ggplot(data, aes(x=explanatory, y=response))+ geom_point()+ geom_ysidedensity()+ geom_xsidedensity() We can add the linear regression model to the plot using the geom_smooth function with the argument method=lm to indicate that we want to use a linear model. The shaded area represents the 95% confidence interval of the model. we use the function geom_smooth(method=lm) to add the linear regression line to the plot. r ggplot(data, aes(x=explanatory, y=response))+ geom_smooth(method=lm)+ geom_point() ## `geom_smooth()` using formula = 'y ~ x' We observe from the model that the linear regression line is a good fit for the data, as the data points are distributed around the line. The shaded area represents the 95% confidence interval of the model. The linear regression model is represented by the equation: \\[Y = 100.01 + 0.01X + \\epsilon\\] where: - \\(Y\\) is the dependent variable (response) - \\(X\\) is the independent variable (explanatory) - \\(\\epsilon\\) is the error term ``` r # The linear regression model model=lm(response~explanatory, data=data) # the model summary(model) ``` ## ## Call: ## lm(formula = response ~ explanatory, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7168 -0.6290 -0.0060 0.6451 3.2383 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99.333005 0.251063 395.649 &lt; 2e-16 *** ## explanatory 0.016988 0.006195 2.742 0.00621 ** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9885 on 998 degrees of freedom ## Multiple R-squared: 0.007479, Adjusted R-squared: 0.006484 ## F-statistic: 7.52 on 1 and 998 DF, p-value: 0.006211 ## Assumption of linear regression There are multiple assumption of the linear regression model that need to be evaluated before interpreting the results. These assumptions include: 1. Linearity: The relationship between the dependent and independent variables is linear. 2. Independence: The residuals are independent of each other. 3. Homoscedasticity: The residuals have constant variance. 4. Normality: The residuals are normally distributed. 5. No multicollinearity: The independent variables are not highly correlated with each other (if you have multiple explanatory variables). We can evaluate these assumptions using the plot function in R. The plot function creates a series of diagnostic plots to evaluate the assumptions of the linear regression model. The plots include: 1. Residuals vs Fitted: This plot evaluates the linearity assumption. The residuals should be randomly distributed around the line y=0. 2. Normal Q-Q: This plot evaluates the normality assumption. The residuals should follow a straight line. 3. Scale-Location: This plot evaluates the homoscedasticity assumption. The residuals should be equally spread along the range of fitted values. 4. Residuals vs Leverage: This plot evaluates the influence of each data point on the regression model. Points that are outliers or have high leverage are identified. The first figure shows the residuals vs fitted values. The residuals should be randomly distributed around the line y=0. Note that any data points which appear to be outliers or have a pattern in the residuals may indicate a violation of the linearity assumption. These values are noted in the figure. In this case no large deviations are observed. The second figure shows the normal Q-Q plot. The residuals should follow a straight line. The third figure shows the scale-location plot. The residuals should be equally spread along the range of fitted values. The fourth figure shows the residuals vs leverage plot. Points that are outliers or have high leverage are identified. Cook’s distance is a measure of the influence of each data point on the regression model. Points with high Cook’s distance are considered influential points. In this case, no points have a Cook’s distance greater than 1, which indicates that there are no influential points in the model. (However see below). r plot(model) Cook’s distance is a measure of the influence of each data point on the regression model. Points with high Cook’s distance are considered influential points. Cook’s distance is calculated as: \\[D_i = \\frac{r_i^2}{p(1-r_i^2)}\\] where: - \\(D_i\\) is the Cook’s distance for data point \\(i\\) - \\(r_i\\) is the residual for data point \\(i\\) - \\(p\\) is the number of parameters in the model A common rule of thumb is that data points with a Cook’s distance greater than 1 are considered influential points. In this case, no points have a Cook’s distance greater than 1, which indicates that there are no influential points in the model. However, it is always a good idea to evaluate the influence of each data point on the regression model. However, there is alternative interpretation for determining if specific data points have large influence ont he results based on the cutoff value of 4/n, where n is the number of observations. In this case, we have 1000 observations, so the cutoff value would be 0.004. Thus in case large Cook’s are not those larger than 1, but larger than 0.004. r head(model$residuals) # to see the residuals ## 1 2 3 4 5 6 ## -0.48841388 -0.15436511 1.54771466 0.06921439 0.33330677 1.61415830 r head(cooks.distance(model)) # to see the Cook's distance ## 1 2 3 4 5 6 ## 2.523214e-04 2.633507e-05 1.232674e-03 2.530125e-06 4.384626e-04 2.648020e-03 r # function in the package sjPlot to visualize the model tab_model( model,show.df = TRUE, CSS = list( css.depvarhead = 'color: red;', css.centeralign = 'text-align: left;', css.firsttablecol = 'font-weight: bold;', css.summary = 'color: blue;' ) ) 15.2 Selling music records We now evaluate a similar more complex and more realistic data set those one would encounter in a study in medicine, sociology or ecological. The data represents the amount of money dedicated to the promotion of different CD’s from a music company and the number of CD’s (CD/downloads) sold. In the first line we see the amount of pounds sterling, £ (UK) dedicated to the promotion of the album music, in the first line we see that he spent £10,256, and then the number of CDs sold was 330. We have information about 200 albums different. library(readr) Album_Sales_1_new &lt;- read_csv(&quot;Data/Album_Sales_1_new.csv&quot;) ## Rows: 200 Columns: 4 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): adverts, sales, airplay, attract ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(Album_Sales_1_new) ## # A tibble: 6 × 4 ## adverts sales airplay attract ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10.3 330 43 10 ## 2 986. 120 28 7 ## 3 1446. 360 35 7 ## 4 1188. 270 33 7 ## 5 575. 220 44 5 ## 6 569. 170 19 5 length(Album_Sales_1_new$adverts) # how many rows of data. ## [1] 200 shapiro.test(Album_Sales_1_new$adverts) ## ## Shapiro-Wilk normality test ## ## data: Album_Sales_1_new$adverts ## W = 0.92542, p-value = 1.482e-08 #Anderson-Darling test for normality library(nortest) ad.test(Album_Sales_1_new$adverts) ## ## Anderson-Darling normality test ## ## data: Album_Sales_1_new$adverts ## A = 3.8762, p-value = 1.089e-09 We begin by graphing the relationship between the two variables. Note that in the geom_smooth() part, has to include method=lm, this means that the method we are constructing the line will use linear regression. It is added to the linear function \\(\\epsilon\\) that represents the errors of the values when comparing with the line that represents the best model. \\[Y_{ i }=\\beta _{ 0 }+\\beta _{ 1 }X_{ i }+\\epsilon _{ i }\\] Remember that \\(\\beta _{ 0 }\\) is the intercept and the \\(\\beta _{ 1 }x_{ i }\\) is the earring. The shaded area is the area of 95% interval of trust. This means that the best line, intercept and slope could vary in this range if we repeat the experiment. Note here all alternatives, I added the two extreme slopes, with a slope major (red) and a minor (violet). Each point represents the sale of a CD with its corresponding amount dedicated to the promotion. The \\(epsilon\\) would be the difference between the best line and the original value, This is also called the residuals. ggplot(Album_Sales_1_new,aes(x=adverts, y=sales))+ geom_smooth(method=lm, se = TRUE)+ geom_point(sd=TRUE) The model model1=lm(sales~adverts, Album_Sales_1_new) #summary(model1) tab_model( model1,show.df = TRUE, CSS = list( css.depvarhead = &#39;color: red;&#39;, css.centeralign = &#39;text-align: left;&#39;, css.firsttablecol = &#39;font-weight: bold;&#39;, css.summary = &#39;color: blue;&#39; ) )   sales Predictors Estimates CI p df (Intercept) 134.14 119.28 – 149.00 &lt;0.001 198.00 adverts 0.10 0.08 – 0.12 &lt;0.001 198.00 Observations 200 R2 / R2 adjusted 0.335 / 0.331 15.3 Cook’s Distance Continuing with the theme of evaluating whether there are values that could influence the analysis a lot, we can use one of the tools to evaluate the weight of each value on a linear regression based on least squares methods, called Cook’s Distance. This analysis was developed by R. Dennis Cook in 1977 and has as its objective to evaluate each value in the data matrix and the weight it has about the result (when it is included or not in the analysis). Produces an index for each of the values on the result based on the residual values is called Cook’s Distance. Therefore, this analysis evaluates the relative impact of each value about the index. Unfortunately it is not clear what the value is critical; That is, what value can tell us that we are overweight? about the results. The two main suggestions are: Distance of Cook, Di, is greater than 1 (suggested by R. Dennis Cook Cook himself in 1982); and that Di &gt; 4/n, where n is the number of observations (Bollen et al. 1990). To illustrate, we will continue with the model model1 using the values calculated in the previous model. The graph is will be constructed using the seq_along option, so that the values in the X axis are based on the sequence of data in the file and the values on the Y axis are based on the values of the Distance of Cook. In this case, we see that all the values are well below of 1, suggesting that none of the individual values would greatly influence the results even if they were excluded. If we use the second alternative of Di &gt; 4/n, then we need to evaluate the 6 Di values that are greater than 4/200=0.02, these should be of concern, where 200 is the amount of data in the file. If you consider this second alternative, it would be necessary to evaluate 6 values in the table of data that could be suspicious (values above the line red). Note that it’s not that they are incorrect; rather, this result is only a tool to evaluate values that They appear to have a considerable impact on the results. Below is how to add the “cook.distance” values to your file Add a “sequence” column to the data Create a graph of Cook’s distance. Determine if there are values of Cook’s \\(D_i\\) greater than 1, or 4/n. library(gghighlight) # package to highlight values in the graph 4/length(Album_Sales_1_new$adverts) ## [1] 0.02 Album_Sales_1_new$cooks.distance&lt;-cooks.distance(model1) Album_Sales_1_new$sequence=c(1:200) ggplot(Album_Sales_1_new, aes(sequence, cooks.distance))+ geom_point()+ geom_hline(aes(yintercept=4/length(Album_Sales_1_new$adverts)))+ gghighlight(cooks.distance &gt; 0.02)+ geom_label(aes(label=sequence), vjust=1, hjust=1, size=2) # function to label values "],["quantile-regression.html", "Chapter 16 Quantile Regression 16.1 Overview 16.2 The model 16.3 The assumptions of quantile regression 16.4 Visualization 16.5 Example: Quantile regression in R 16.6 The quantile regression model, rq() 16.7 nlrq", " Chapter 16 Quantile Regression Quantile regression is a specialized regression technique that estimates the conditional median or other quantiles of a response variable. It is particularly useful when the response variable is not normally distributed or when the relationship between the response variable and the predictor variables is not linear or there is heteroscedasticity. Quantile regression was first developed by Koenker and Bassett (1978) and Bassett and Koenker (1978) and has since been widely used in a variety of fields, including economics, finance, and epidemiology. This approach is particularly useful when the conditional distribution of the response variable is asymmetric or when the relationship between the response variable and the predictor variables varies across different quantiles. In other words it is a robust regression technique that is not sensitive to outliers and can provide a more complete picture of the relationship between the response variable and the predictor variables. In this chapter, we will provide an overview of quantile regression, discuss its advantages and disadvantages, and demonstrate how to perform quantile regression in R using the quantreg package. 16.1 Overview Quantile regression is a regression technique that estimates the conditional quantiles of a response variable given a set of predictor variables. Unlike ordinary least squares (OLS) regression, which estimates the conditional mean of the response variable, quantile regression estimates the conditional quantiles, such as the median, 25th percentile, or 75th percentile. Quantile regression is particularly useful when the response variable is not normally distributed or when the relationship between the response variable and the predictor variables is not linear. By estimating the conditional quantiles, quantile regression provides a more complete picture of the relationship between the response variable and the predictor variables, allowing for a more nuanced analysis of the data. 16.2 The model The quantile regression model can be written as: \\[Q_{\\tau}(Y|X) = X\\beta_{\\tau}\\] where: \\(Q_{\\tau}(Y|X)\\) is the \\(\\tau\\)-th quantile of the response variable \\(Y\\) given the predictor variables \\(X\\), \\(\\beta_{\\tau}\\) is the vector of coefficients for the predictor variables, \\(\\tau\\) is the quantile level, which ranges from 0 to 1. The goal of quantile regression is to estimate the coefficients \\(\\beta_{\\tau}\\) that minimize the sum of the absolute deviations between the observed values of the response variable and the estimated quantiles. Quantile regression can be performed for multiple quantile levels, allowing for a more comprehensive analysis of the relationship between the response variable and the predictor variables. 16.3 The assumptions of quantile regression Quantile regression does not make the same assumptions as OLS regression, such as linearity, homoscedasticity, and normality of residuals. Instead, quantile regression estimates the conditional quantiles of the response variable directly, making it more robust to violations of these assumptions. However, quantile regression does assume that the errors are independent and identically distributed (i.i.d.), which is a common assumption in regression analysis. 16.4 Visualization To visualize the results of quantile regression, we can create quantile regression plots, which show the estimated quantiles of the response variable as a function of the predictor variables. These plots provide a visual representation of the relationship between the response variable and the predictor variables at different quantile levels. In the following example, we will demonstrate how to perform quantile regression in R using the quantreg package and create quantile regression plots to visualize the results. 16.5 Example: Quantile regression in R In this example, we will use the quantreg package to perform quantile regression on the mtcars dataset, which contains information about various car models. We will estimate the conditional quantiles of the miles per gallon (mpg) variable given the weight (wt) variable. First, we will load the quantreg package and the mtcars dataset: https://ggplot2.tidyverse.org/reference/geom_quantile.html Usamos un conjunto de datos de ejemplo llamado mtcars, que contiene información sobre diferentes modelos de automóviles. En particular, nos interesa la relación entre el desplasamiento (displ) del motor en litros y la cantidad de milas por galon . Para visualizar esta relación, ajustaremos un modelo de regresión cuantílica y trazaremos las estimaciones de los cuantiles condicionales de mpg en función de wt. El data frame mpg contiene se encuentra el el paquete ggplot2 las siguientes variables: library(tidyverse) library(quantreg) ## Loading required package: SparseM data(mpg) head(mpg) #visualizar las primeras filas del data frame ## # A tibble: 6 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… Nota que el patron de dispersion aumenta a medida que el desplasamiento del motor aumenta. Esto sugiere que la relación entre el desplasamiento y la cantidad de millas por galon no es lineal y que la varianza de mpg aumenta con el desplasamiento del motor. Para capturar esta relación no lineal y heterocedasticidad, ajustaremos un modelo de regresión cuantílica y trazaremos las estimaciones de los cuantiles condicionales de mpg en función de wt. Sobrepuse la regressión lineal con si intervalo de confianza (en rojo) y los cuantiles de la regresión cuantílica (en azul). Note que la regresión cuantílica captura la relación no lineal entre displ y mpg y la heterocedasticidad en los errores, mientras que la regresión lineal no lo hace. ggplot(mpg, aes(x = displ, y = 1/hwy)) + geom_point() + geom_quantile()+ geom_smooth(method = &quot;lm&quot;, se = TRUE, color=&quot;red&quot;) ## Smoothing formula not specified. Using: y ~ x ## `geom_smooth()` using formula = &#39;y ~ x&#39; Adding quantiles to the plot in this figure we add only the quantile 0.5, which corresponds to the median of the conditional distribution of mpg given wt. ggplot(mpg, aes(x = displ, y = 1/hwy)) + geom_point() + geom_quantile(quantiles = 0.5, color = &quot;blue&quot;) ## Smoothing formula not specified. Using: y ~ x Adding multiples quantiles Using multiple quantiles one clearly see the relationship between the predictor and the response variable and the quantiles are not equally dispersed. The median quantile is in blue, while the 25th and 75th are in red q10 &lt;- seq(0.05, 0.95, by=0.05) #cuantiles del 5% al 95% en incrementos del 5% ggplot(mpg, aes(x = displ, y = 1/hwy)) + geom_point() + geom_quantile(quantiles = q10, colour = &quot;grey&quot;)+ geom_quantile(quantiles = 0.5, color = &quot;blue&quot;)+ geom_quantile(quantiles = 0.25, color = &quot;red&quot;)+ geom_quantile(quantiles = 0.75, color = &quot;red&quot;) 16.6 The quantile regression model, rq() To ajust a quantile regression model, we use the rq() function from the quantreg package. The rq() function takes two main arguments: the formula that specifies the model and the data frame that contains the data. In this case, we will fit a quantile regression model to estimate the median of mpg given wt. tau = 0.5 specifies the quantile we want to estimate, in this case, the median of the conditional distribution of mpg given wt. The interpretation of the quantile regression model is similar to that of a linear regression model, but instead of estimating the conditional mean of the response variable, we estimate the specified conditional quantile. In this case, the quantile regression model estimates the median of mpg given wt has a coefficient of 0.0076, which suggests that for every one-unit increase in displ, the median of 1/hwy increases by 0.0076. The p-value associated with the coefficient is less than 0.05, indicating that the coefficient is significantly different from zero. The summary of the quantile regression model also provides the standard error, t-value, and p-value for the coefficient, as well as the residual standard error and the number of observations used in the model. # djusting a quantile regression model model &lt;- rq(1/hwy ~ displ, data = mpg, tau = 0.5) summary(model) ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be ## nonunique ## ## Call: rq(formula = 1/hwy ~ displ, tau = 0.5, data = mpg) ## ## tau: [1] 0.5 ## ## Coefficients: ## coefficients lower bd upper bd ## (Intercept) 0.01927 0.01610 0.02099 ## displ 0.00761 0.00664 0.00874 16.7 nlrq to model with this function, you need to install the quantreg package and load it into the R session. nlrq() is a function that fits a non-linear quantile regression model. This function takes two main arguments: the formula that specifies the model and the data frame that contains the data. In this case, we will fit a non-linear quantile regression model to estimate the median of mpg given wt. Nice pattern, but not realistic of a biological data set. SSlogis = Self-Starting Nls Logistic Model This is used to fit a logistic curve to data. The curve is defined as: \\[f(x) = \\frac{Asym}{1 + exp((x-mid)/scal)}\\] where: Asym is the asymptote of the curve, mid is the x-value at the inflection point of the curve, scal is a scaling parameter that determines the steepness of the curve. In this example, we will use the SSlogis function to generate a logistic curve with Asym = 10, mid = 12, and scal = 2, and add some random noise to the data. We will then fit a non-linear least squares regression to the data using the nls function, and compare the results to a quantile regression using the nlrq function. Dat &lt;- NULL; Dat$x &lt;- rep(1:25, 20) set.seed(1) Dat$y &lt;- SSlogis(Dat$x, 10, 12, 2)*rnorm(500, 1, 0.1) plot(Dat) # fit first a nonlinear least-square regression Dat.nls &lt;- nls(y ~ SSlogis(x, Asym, mid, scal), data=Dat); Dat.nls ## Nonlinear regression model ## model: y ~ SSlogis(x, Asym, mid, scal) ## data: Dat ## Asym mid scal ## 9.968 11.947 1.962 ## residual sum-of-squares: 241.8 ## ## Number of iterations to convergence: 0 ## Achieved convergence tolerance: 6.921e-07 lines(1:25, predict(Dat.nls, newdata=list(x=1:25)), col=1) # then fit the median using nlrq Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Dat, tau=0.5, trace=TRUE) ## 109.059 : 9.968027 11.947208 1.962113 ## final value 108.942725 ## converged ## lambda = 1 ## 108.9427 : 9.958648 11.943273 1.967144 ## final value 108.490939 ## converged ## lambda = 0.9750984 ## 108.4909 : 9.949430 11.987472 1.998607 ## final value 108.471416 ## converged ## lambda = 0.9999299 ## 108.4714 : 9.94163 11.99077 1.99344 ## final value 108.471243 ## converged ## lambda = 1 ## 108.4712 : 9.941008 11.990550 1.992921 ## final value 108.470935 ## stopped after 4 iterations ## lambda = 0.8621249 ## 108.4709 : 9.942734 11.992773 1.993209 ## final value 108.470923 ## converged ## lambda = 0.9999613 ## 108.4709 : 9.942629 11.992728 1.993136 ## final value 108.470919 ## converged ## lambda = 1 ## 108.4709 : 9.942644 11.992737 1.993144 ## final value 108.470919 ## converged ## lambda = 1 ## 108.4709 : 9.942644 11.992737 1.993144 ## final value 108.470919 ## converged ## lambda = 1 ## 108.4709 : 9.942644 11.992737 1.993144 lines(1:25, predict(Dat.nlrq, newdata=list(x=1:25)), col=2) # the 1st and 3rd quartiles regressions Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Dat, tau=0.25, trace=TRUE) ## 108.6656 : 9.968027 11.947208 1.962113 ## final value 89.108243 ## converged ## lambda = 1 ## 89.10824 : 9.432250 11.803924 1.923472 ## final value 85.688895 ## converged ## lambda = 1 ## 85.6889 : 9.183598 11.794244 1.929699 ## final value 85.473712 ## converged ## lambda = 0.6405076 ## 85.47371 : 9.212527 11.844090 1.938003 ## final value 85.447786 ## converged ## lambda = 1 ## 85.44779 : 9.234097 11.863975 1.949241 ## final value 85.446407 ## converged ## lambda = 1 ## 85.44641 : 9.242009 11.866644 1.954192 ## final value 85.445691 ## converged ## lambda = 1 ## 85.44569 : 9.234247 11.864554 1.952338 ## final value 85.444920 ## converged ## lambda = 1 ## 85.44492 : 9.232975 11.863979 1.953587 ## final value 85.443854 ## converged ## lambda = 0.363237 ## 85.44385 : 9.233661 11.864280 1.957197 ## final value 85.443668 ## stopped after 3 iterations ## lambda = 0.8495473 ## 85.44367 : 9.233453 11.860020 1.957831 ## final value 85.443667 ## converged ## lambda = 0.008522445 ## 85.44367 : 9.233449 11.860007 1.957814 ## final value 85.443584 ## converged ## lambda = 1 ## 85.44358 : 9.232996 11.859020 1.955928 ## final value 85.443586 ## converged ## lambda = 0.9999957 ## 85.44359 : 9.232995 11.859024 1.955916 lines(1:25, predict(Dat.nlrq, newdata=list(x=1:25)), col=3) Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Dat, tau=0.75, trace=TRUE) ## 109.4525 : 9.968027 11.947208 1.962113 ## final value 89.561436 ## converged ## lambda = 1 ## 89.56144 : 10.64021 12.13202 2.02044 ## final value 87.302043 ## converged ## lambda = 1 ## 87.30204 : 10.652294 11.966018 1.958371 ## final value 87.200715 ## converged ## lambda = 1 ## 87.20072 : 10.666754 11.953497 1.962447 ## final value 87.131462 ## converged ## lambda = 0.8659451 ## 87.13146 : 10.639094 11.949236 1.971242 ## final value 87.125795 ## converged ## lambda = 0.6273927 ## 87.1258 : 10.647784 11.962635 1.975851 ## final value 87.122717 ## converged ## lambda = 0.8041119 ## 87.12272 : 10.647957 11.963190 1.973657 ## final value 87.121592 ## converged ## lambda = 1 ## 87.12159 : 10.649877 11.962363 1.973516 ## final value 87.121427 ## converged ## lambda = 1 ## 87.12143 : 10.649051 11.961685 1.973086 ## final value 87.121355 ## converged ## lambda = 0.5468903 ## 87.12135 : 10.648209 11.961208 1.972643 ## final value 87.121400 ## converged ## lambda = 0.9999045 ## 87.1214 : 10.648073 11.961122 1.972568 lines(1:25, predict(Dat.nlrq, newdata=list(x=1:25)), col=3) # and finally &quot;external envelopes&quot; holding 95 percent of the data Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Dat, tau=0.025, trace=TRUE) ## 108.3114 : 9.968027 11.947208 1.962113 ## final value 62.166616 ## converged ## lambda = 1 ## 62.16662 : 9.432250 11.803924 1.923472 ## final value 16.887325 ## converged ## lambda = 1 ## 16.88732 : 8.006640 11.718631 1.979243 ## final value 15.823276 ## converged ## lambda = 0.7133884 ## 15.82328 : 8.135460 12.048708 1.987995 ## final value 15.732737 ## stopped after 3 iterations ## lambda = 0.7726586 ## 15.73274 : 8.042059 12.019442 1.994386 ## final value 15.732737 ## converged ## lambda = 0 ## 15.73274 : 8.042059 12.019442 1.994386 lines(1:25, predict(Dat.nlrq, newdata=list(x=1:25)), col=4) Dat.nlrq &lt;- nlrq(y ~ SSlogis(x, Asym, mid, scal), data=Dat, tau=0.975, trace=TRUE) ## 109.8066 : 9.968027 11.947208 1.962113 ## final value 56.575819 ## converged ## lambda = 1 ## 56.57582 : 10.672415 12.148657 2.027285 ## final value 20.551829 ## converged ## lambda = 1 ## 20.55183 : 11.923558 12.366710 2.121476 ## final value 17.268734 ## converged ## lambda = 1 ## 17.26873 : 12.266850 12.051876 2.060768 ## final value 17.194623 ## converged ## lambda = 0.5512476 ## 17.19462 : 12.176373 12.020546 2.003537 ## final value 17.175845 ## converged ## lambda = 0.900139 ## 17.17585 : 12.180837 12.005129 2.019783 ## final value 17.175761 ## converged ## lambda = 0.1504766 ## 17.17576 : 12.177202 12.003960 2.011709 ## final value 17.175612 ## converged ## lambda = 1 ## 17.17561 : 12.18154 12.00534 2.01894 ## final value 17.175603 ## converged ## lambda = 1 ## 17.1756 : 12.181679 12.005403 2.019175 ## final value 17.175518 ## converged ## lambda = 1 ## 17.17552 : 12.17954 12.00469 2.01453 ## final value 17.175518 ## converged ## lambda = 0 ## 17.17552 : 12.17954 12.00469 2.01453 lines(1:25, predict(Dat.nlrq, newdata=list(x=1:25)), col=4) leg &lt;- c(&quot;least squares&quot;,&quot;median (0.5)&quot;,&quot;quartiles (0.25/0.75)&quot;,&quot;.95 band (0.025/0.975)&quot;) legend(1, 12.5, legend=leg, lty=1, col=1:4) https://stackoverflow.com/questions/51223379/how-get-plot-from-nlrq-in-r Need a better data set. To adjust a non-linear quantile regression model, we use the nlrq() function from the quantreg package. The nlrq() function takes two main arguments: the formula that specifies the model and the data frame that contains the data. In this case, we will fit a non-linear quantile regression model to estimate the median of mpg given wt. The formula specifies the model, which in this case is a non-linear model that estimates the conditional median of mpg given wt using the SSlogis function. The SSlogis function is a self-starting logistic model that defines a logistic curve with three parameters: Asym, mid, and scal. The tau argument specifies the quantile level we want to estimate, in this case, the median of the conditional distribution of mpg given wt. Asym refers to the asymptote of the curve, mid is the x-value at the inflection point of the curve, scal is a scaling parameter that determines the steepness of the curve. The summary of the non-linear quantile regression model provides the estimated coefficients for the parameters Asym, mid, and scal, as well as the standard errors, t-values, and p-values for the coefficients. The residual standard error and the number of observations used in the model are also provided. # Adjusting a non-linear quantile regression model model_nlrq &lt;- nlrq(1/hwy ~ SSlogis(displ, Asym, mid, scal), data = mpg, tau=0.5) summary(model_nlrq) ## ## Call: nlrq(formula = 1/hwy ~ SSlogis(displ, Asym, mid, scal), data = mpg, ## tau = 0.5, control = list(maxiter = 100, k = 2, InitialStepSize = 1, ## big = 1e+20, eps = 1e-07, beta = 0.97), trace = FALSE) ## ## tau: [1] 0.5 ## ## Coefficients: ## Value Std. Error t value Pr(&gt;|t|) ## Asym 0.07233 0.01221 5.92518 0.00000 ## mid 2.20424 0.74844 2.94513 0.00356 ## scal 2.18291 0.69912 3.12238 0.00202 plot(1/hwy ~ displ, mpg) lines(fitted(model_nlrq) ~ displ, mpg) https://www.rdocumentation.org/packages/ggstatsplot/versions/0.6.5 See also https://broom.tidymodels.org/reference/augment.nlrq.html library(quantreg) library(ggstatsplot) ## You can cite this package as: ## Patil, I. (2021). Visualizations with statistical details: The &#39;ggstatsplot&#39; approach. ## Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167 # model mod &lt;- quantreg::rq(formula = mpg ~ am * cyl, data = mtcars) ## Warning in rq.fit.br(x, y, tau = tau, ...): Solution may be nonunique # plot ggstatsplot::ggcoefstats(mod) ## Warning in rq.fit.br(x, y, tau = tau, ci = TRUE, ...): Solution may be ## nonunique References: Koenker, Roger, and Gilbert Bassett Jr. “Regression quantiles.” Econometrica: journal of the Econometric Society (1978): 33-50. Bassett, G., &amp; Koenker, R. (1978). Asymptotic Theory of Least Absolute Error Regression. Journal of the American Statistical Association, 73(363), 618–622. https://doi.org/10.1080/01621459.1978.10480065 "],["generalized-linear-models.html", "Chapter 17 Generalized Linear Models 17.1 Linear models 17.2 Necesito cambiar este gráfico, ya que viene del web (del siguiente website “https://towardsdatascience.com”). 17.3 When the dependent variable is not linear 17.4 Link available 17.5 The assumptions 17.6 The advantages of GLM 17.7 Binomial or Bernoulli models 17.8 The basic parameters of the gamma. 17.9 Evaluating distributions 17.10 Binomial regression 17.11 Poisson regression 17.12 Negative binomial regression", " Chapter 17 Generalized Linear Models Test 17.1 Linear models Generalized linear models, GLM, are an extension of linear models where the dependent variable has a normal distribution. We remember that in a linear regression the dependent variable follows the model \\(y_i = \\beta_0 + \\beta_1 x_i\\), where \\(\\beta_0\\) is the intercept and the \\(\\beta_1\\) is the coefficient, that is, the slope and the \\(x_i\\) are the values of x’s. One of the assumptions is that the variation in the values of \\(\\mu_i\\), which are the \\(y_i\\), have a normal distribution in each x’s and that there is homogeneity of variance. \\[\\mu_i=\\beta_0+\\beta_1x_i\\] An important assumption is the assumption that the variation in the \\(y_i\\) are normally distributed and that there is homogeneity of variance. \\[y_i\\sim N\\left(\\mu_i,\\ \\epsilon\\right)\\] We can visualize it with the following figure, where the values of y’s have a normal distribution and that this distribution is homogeneous across the values of x’s. 17.2 Necesito cambiar este gráfico, ya que viene del web (del siguiente website “https://towardsdatascience.com”). 17.3 When the dependent variable is not linear The main problem for a long time has been that the dependent variable does not have a normal distribution and consequently did not meet the assumptions of linear regression. The method of linearizing the response variable was developed by Nelder and Wedderburn 1972. Information about the test and its evolution of GLM can be found at https://encyclopediaofmath.org/wiki/Generalized_linear_models. With the advancement of the use of computers in the 80s, it ended up being one of the most used statistical methods. The term GLM refers to a wide variety of regression models. The assumption in these models is that the response variable \\(y_i\\) follows a distribution within the family of exponential distributions with an average \\(\\mu_i\\), where a function \\(\\mu_i^T\\beta\\) is assumed that is frequently non-linear . To linearize the variable it is necessary to use a “link” to convert the dependent variable, \\(y_i\\). 17.4 Link available Here I show a partial list of the different types of “links” for different types of data (or distributions) of the independent variable, \\(y_i\\). The decision of which transformation or link to use is necessary will depend on the data types and their distribution. This is only a subset of avaliable link functions. (#tab:GLM_2) Common Link function in GLM. ModeloVariable_DepLinkVariables_Independiente Linear regressionlNormalIdentidyContinuous ANOVANormalIdentityCategorical Logistic RegressionBinomialLogitMix Poisson RegressionPoissonLogMix Although the previous distributions are very common, they are not the only links available to transform the data. Here is supplementary information on some other R links that are available in certain packages. https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family (#tab:GLM_3) Common link functions in GLM. FamiliaLinks Gaussian/Normalidentity, log, inverse Binomiallogit, probit, cauchit, log, cloglog Gammainverse, identity, log Poissonlog, identity, square root Inverse Gausian1/µ^2, inverse, identity, log 17.5 The assumptions The advantage is that now the dependent variable does not have to have a normal distribution. Furthermore, untransformed data does not have to have a linear relationship. However, the data must come from a distribution of the exponential family, for example, binomial, Poisson, multinomial, normal, inverse normal, etc. GLM does not assume a linear relationship between the dependent and independent variable, but assumes a linear relationship of the transformed dependent variable data and the independent variable. Consequently, a linear relationship is assumed between the binary variable and the explanatory variable after using the transformation with the link, \\(logit\\left(\\pi\\right)=\\beta_0+\\beta_1\\cdot x_i\\). The assumption of homogeneity of variance does not have to be taken into account. The errors have to be independent but it does not matter if they comply with a normal distribution. 17.6 The advantages of GLM The parameters are estimated using likelihood (MLE = Maximum Likelihood estimators), not least squares (OLS = ordinary least square). The models are estimated via likelihood, then they optimize the estimators, \\(\\beta\\). There is no need to transform the data of the dependent variable to normalize them. The selection of the “link” is independent of the dependent variable(s), which makes it easier to create models. Tools to evaluate inferences and models are available such as evaluating residuals, confidence intervals, deviation, likelihood ratio test and Akaike Information Criterio index among others. 17.7 Binomial or Bernoulli models If the variable is binomial, there are only two alternatives, 0 and 1, or whether or not, dead or alive, the binomial distribution is used. The logit function is used as a link function and the binomial/Bernoulli distribution as a probability distribution, the model is called Logistic Regression. \\[\\log\\frac{q_i}{1-q_i}=\\beta_0+\\beta_1X_i\\] where the distribution is a binomial with \\(y_i\\sim Binomial\\left(q_i\\right)\\). #library(wakefield) x=wakefield::r_sample_binary(50, x = 0:1, prob =c(0.3, 0.7), name = &quot;Binary&quot;) #x df=as.data.frame(as.factor(x)) #df ggplot(df, aes(x))+ geom_histogram(fill=&quot;blue&quot;)+ scale_x_continuous(breaks = c(0, 1))+ xlab(&quot;Binary Values&quot;) 17.7.1 Gamma Distribution The gamma distribution is frequently used to take into account variables that have very long and large tails (Heavy-Tailed distributions). The distribution is widely used in the area of econometrics and survival estimates. The gamma distribution can be parameterized with a “shape term \\(\\alpha = k\\) and the inverse of a scale parameter \\(\\beta=1/\\theta\\) which is known as a * parameter *rate**. \\[f\\left(x\\right)=\\frac{\\left(\\beta^{\\alpha}\\cdot x^{\\alpha-1}e^{-\\beta x}\\right)}{\\Gamma\\left(\\alpha\\right)}\\ para\\ x&gt;0,\\ \\ \\alpha,\\ \\beta&gt;0\\] where \\(\\Gamma\\left(\\alpha\\right)\\) is the gamma function. For each integer value \\(\\Gamma\\left(\\alpha\\right)=\\left(\\alpha-1\\right)!\\) In another word, the gamma distribution is for modeling continuous variables that are always positive and have skewed distributions. Examples where the gamma distribution is used the time until the moment of failure of a team the time until the death of individuals the amount of water accumulated in a lake the size of unpaid loans Let’s look at some gamma distributions x = 0:20 curve(dgamma(x, shape=1, scale=2), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 3, lwd = 3, main = &quot;Examples of Gamma Distributions&quot;) curve(dgamma(x, shape=2, scale=2), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 5, lwd = 3, add = TRUE) curve(dgamma(x, shape=2, scale=4), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 6, lwd = 3, add = TRUE) curve(dgamma(x,shape =5, scale=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 7, lwd = 3, add = TRUE) curve(dgamma(x, shape=9, scale=0.5), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 1, lwd = 3, add = TRUE) curve(dgamma(x, shape=7.5, scale=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, 0.4, 20, col = 2, lwd = 3, add = TRUE) legend(&quot;topright&quot;, c(&quot;k=1,theta=2&quot;, &quot;k=2,theta=2&quot;, &quot;k=2,theta=4&quot;,&quot;k=5,theta=1&quot;,&quot;k=9,theta=0.5&quot;, &quot;k=7.5,theta=1.0&quot;), col = c(3, 5,6, 7,1,2), lwd = 3, inset = 0.05) 17.8 The basic parameters of the gamma. Note: The scale is a dispersion index, and the higher the number, the longer the tail. The average is equal to the multiplication of shape=k by scale = theta, \\[average=k\\cdot\\theta\\]. The variance is equal to the multiplication of shape=k by scale = (theta)^2, \\[variance=k\\cdot\\theta^2\\] You can find the formulas to calculate other parameters on the Wikipedia page. https://en.wikipedia.org/wiki/Gamma_distribution 17.8.1 LThe inverse Gaussian distribution In probability theory, the inverse Gaussian distribution is a two-parameter family of continuous probability distributions with support at (0, ∞). The distribution follows the following form \\[f\\left(x;µ,\\lambda\\right)=\\sqrt{\\frac{\\lambda}{2\\pi x^3}}\\exp\\left(-\\frac{\\lambda\\left(x-µ\\right)^2}{2µ^2x}\\right)\\] where \\(x&gt;0\\), \\(µ&gt;0\\), and \\(\\lambda\\) It is the form of distribution and is always greater than zero. The larger the \\(\\lambda\\) (the shape parameter) the more symmetrical the distribution. As λ tends to infinity, the inverse Gaussian distribution looks like a normal (Gaussian) distribution. The inverse Gaussian distribution is also known as the Wald distribution. The distribution is used when the population distribution where the lognormal distribution has too heavy a right tail. When referring to heavy tails in statistics, it means that there are more probabilities in this region than a normal distribution. Consequently, the probabilities in this region are greater. The distribution is used to model non-negative data that is positively skewed. In other words, all values are positive and the tail tends to decrease more slowly than in a normal distribution. 17.8.1.1 Reverse Gaussian history Historical information on the inverse Gaussian distribution is somewhat limited. The distribution was apparently first derived by Louis Bachelier in 1900, when he was trying to estimate the stock market price for different companies. But the name “Inverse Gaussian” was suggested by Maurice Tweedie in 1945. See this link for more details. https://en.wikipedia.org/wiki/Normal-inverse_Gaussian_distribution Examples that could be of this type of distribution the time it takes to get to a place. the distribution of house prices the number of children in a family the survival of organisms (survival data) Note that the distribution is not symmetrical, and the larger the \\(\\lambda\\) the more symmetrical the distribution. library(SuppDists) #dinvGauss(x, nu, lambda, log=FALSE) x = -1:3 curve(dinvGauss(x, nu=.7, lambda=0.2, log=FALSE), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 3, lwd = 2, main = &quot;Gráfico distribución Gausiana Inversa&quot;) curve(dinvGauss(x, nu=.7, lambda=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 5, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=.7, lambda=3), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 6, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=2, lambda=0.2), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 7, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=2, lambda=1), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 1, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=2, lambda=4), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 2, lwd = 2, add = TRUE) curve(dinvGauss(x, nu=1, lambda=40), xlab = &quot;x&quot;, ylab = &quot;f(x;k,theta)&quot;, -0.3, 3, col = 4, lwd = 2, add = TRUE) legend(&quot;topright&quot;, c(&quot;nu=.7,lambda=0.2&quot;, &quot;nu=.7,lambda=1&quot;, &quot;nu=.7,lambda=3&quot;,&quot;nu=2,lambda=0.2&quot;,&quot;nu=2,lambda=1&quot;, &quot;nu=2,lambda=4&quot;,&quot;nu=1,lambda=40&quot;), col = c(3, 5,6, 7,1,2,4), lwd = 3, inset = 0.05) 17.8.1.2 The parameters of the inverse Gaussian distribution The average is calculated as follows \\[E\\left[X\\right]=\\frac{1}{\\mu}+\\frac{1}{\\lambda}\\] The variance is calculated in the following way \\[Var\\left[X\\right]=\\frac{1}{\\mu\\lambda}+\\frac{2}{\\lambda^2}\\] For other parameters see this link https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution 17.9 Evaluating distributions Evaluate a data set to visualize the distribution. If you need to do regression analysis with data that has an inverse Gaussian distribution the following package is available invGauss and if you want to determine which is the best distribution for your data with the package univariateML there are some functions to help you determine the type of distribution of your data. I suggest this link. https://www.cienciadedatos.net/documentos/55_ajuste_distribuciones_con_r.html library(invGauss) library(univariateML) data(d.oropha.rec) #d.oropha.rec ggplot(data = d.oropha.rec) + geom_histogram(aes(x = time, y = after_stat(density)), bins = 40, alpha = 0.3, color = &quot;black&quot;) + geom_rug(aes(x = time)) + stat_function(fun = function(.x){dml(x = .x, obj = mlnorm(d.oropha.rec$time))}, aes(color = &quot;normal&quot;), size = 1) + stat_function(fun = function(.x){dml(x = .x, obj = mlinvgauss(d.oropha.rec$time))}, aes(color = &quot;inverse-normal&quot;), size = 1) + scale_color_manual(breaks = c(&quot;normal&quot;, &quot;inverse-normal&quot;), values = c(&quot;normal&quot; = &quot;red&quot;, &quot;inverse-normal&quot; = &quot;blue&quot;)) + labs(title = &quot;Distribution of time of development&quot;, color = &quot;Distribución&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;) Example of diamond prices from the ggplot2 package #library(univariateML) ggplot(data = diamonds) + geom_histogram(aes(x = price, y = after_stat(density)), bins = 40, alpha = 0.3, color = &quot;black&quot;) + geom_rug(aes(x = price)) + stat_function(fun = function(.x){dml(x = .x, obj = mllnorm(diamonds$price))}, aes(color = &quot;log-normal&quot;), size = 1) + stat_function(fun = function(.x){dml(x = .x, obj = mlinvgauss(diamonds$price))}, aes(color = &quot;inverse-normal&quot;), size = 1) + scale_color_manual(breaks = c(&quot;log-normal&quot;, &quot;inverse-normal&quot;), values = c(&quot;log-normal&quot; = &quot;red&quot;, &quot;inverse-normal&quot; = &quot;blue&quot;)) + labs(title = &quot;Distribution of diamond prices&quot;, color = &quot;Distribution&quot;) + theme_bw() + theme(legend.position = &quot;bottom&quot;) 17.10 Binomial regression The binomial regression is used when the dependent variable is binary, that is, it has two possible outcomes, 0 or 1, dead or alive, success or failure, etc. The binomial distribution is used as a probability distribution and the logit function is used as a link function. The model is called Logistic Regression. \\[\\log\\frac{q_i}{1-q_i}=\\beta_0+\\beta_1X_i\\] where the distribution is a binomial with \\(y_i\\sim Binomial\\left(q_i\\right)\\). Here let use the data from Karn and Penrose of the survival or death on infants and birth weight. Survival: 1 = Infant survived, 0 = the Infant died Weigth_lb = The weigth of the infant in pounds at birth. library(readr) Karn_Penrose_infant_survivorship &lt;- read_csv(&quot;Data/Karn_Penrose_infant_survivorship.csv&quot;) Infant= Karn_Penrose_infant_survivorship head(Infant) (#tab:GLM_8) row_numSurvivalWeigth_lbGestation_Time_days 101&nbsp;&nbsp;155 201&nbsp;&nbsp;165 301&nbsp;&nbsp;165 405.5170 501&nbsp;&nbsp;180 601.5180 Visualize the data The data is not normally distributed. The data is binary, 0 or 1. The regression line is not linear and is probability The 95% confidence interval is shown in the graph. ggplot(Infant, aes(x=Weigth_lb, y=Survival))+ geom_jitter(width=0.1, height = 0.1)+ # point represents an infants outcome geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;binomial&quot;), se=T)+ xlab(&quot;Weight in pounds&quot;)+ ylab(&quot;Probability of Survival&quot;)+ scale_x_continuous(breaks = seq(0, 14, by = 1)) Creating the binomial model Infant_model=glm(Survival ~ Weigth_lb, data=Infant, family=binomial) summary(Infant_model) ## ## Call: ## glm(formula = Survival ~ Weigth_lb, family = binomial, data = Infant) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.76189 0.22628 -7.786 6.91e-15 *** ## Weigth_lb 0.67591 0.03825 17.670 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2199.2 on 4051 degrees of freedom ## Residual deviance: 1833.2 on 4050 degrees of freedom ## AIC: 1837.2 ## ## Number of Fisher Scoring iterations: 6 17.11 Poisson regression A poisson regression is used when the dependent variable is a count variable, that is, the number of events that occur in a fixed period of time. The Poisson distribution is used as a probability distribution and the log function is used as a link function. The model is called Poisson Regression. \\[\\log\\left(\\lambda_i\\right)=\\beta_0+\\beta_1X_i\\] where the distribution is a Poisson with \\(y_i\\sim Poisson\\left(\\lambda_i\\right)\\). Here let use the data from the number of accidents in a city and the number of cars in the city. Brooklyn_Bridge: The number cyclist on the Brooklyn Bridge on each day. Precipitation = The precipitation in inches on that day library(readr) NY_CITY_CYCLIST &lt;- read_csv(&quot;Data/NY_CITY_CYCLIST.csv&quot;) NY=NY_CITY_CYCLIST head(NY) (#tab:GLM_11) DateDayHigh_Tem_FLow_Temp_FPrecipitationBrooklyn_BridgeManhattan_BridgeWilliamsburg_BridgeQueensboro_BridgeTotal 1-AprFriday78.166&nbsp;&nbsp;0.011.7e+03&nbsp;3.13e+034.12e+032.55e+031.15e+04 2-AprSaturday55&nbsp;&nbsp;48.90.15827&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.65e+032.56e+031.88e+036.92e+03 3-AprSunday39.934&nbsp;&nbsp;0.09526&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.23e+031.7e+03&nbsp;1.31e+034.76e+03 4-AprMonday44.133.10.47521&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.07e+031.44e+031.31e+034.34e+03 5-AprTuesday42.126.10&nbsp;&nbsp;&nbsp;1.42e+032.62e+033.08e+032.36e+039.47e+03 6-AprWednesday45&nbsp;&nbsp;30&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;1.88e+033.33e+033.86e+032.85e+031.19e+04 Visualize the data The data is not normally distributed. The data is a count variable. The regression line is not linear and is count (number of cylcist) The 95% confidence interval is shown in the graph. ggplot(NY, aes(x=Precipitation, y=Brooklyn_Bridge))+ geom_jitter(width=0.01, height = 0.1)+ # point represents a count of number of cyclist for a specific day geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;poisson&quot;), se=T)+ xlab(&quot;Precipitation in inches&quot;)+ ylab(&quot;Number of cyclist&quot;)+ scale_x_continuous(breaks = seq(0, 2, by = 0.1)) Creating the Poisson model NY_model=glm(Brooklyn_Bridge ~ Precipitation, data=NY, family=poisson) summary(NY_model) ## ## Call: ## glm(formula = Brooklyn_Bridge ~ Precipitation, family = poisson, ## data = NY) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.086430 0.001378 5869.0 &lt;2e-16 *** ## Precipitation -0.565548 0.006023 -93.9 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 91743 on 197 degrees of freedom ## Residual deviance: 81450 on 196 degrees of freedom ## (17 observations deleted due to missingness) ## AIC: 83389 ## ## Number of Fisher Scoring iterations: 5 17.12 Negative binomial regression A negative binomial regression is used when the dependent variable is a count variable and that the data suggests overdipersion. Overdispersion is when the variance is greater than the mean. The Poisson distribution is used as a probability distribution and the log function is used as a link function. The model is called Negative Binomial Regression. \\[\\log\\left(\\lambda_i\\right)=\\beta_0+\\beta_1X_i\\] where the distribution is a negative binomial with \\(y_i\\sim NegBinomial\\left(\\lambda_i\\right)\\). We will start by calculating the mean and vaiance of the above data set and detemrine if there is overdisperion The function requiered dispersiontest is available in the package AER. The function will return the dispersion parameter and the p-value. If the p-value is less than 0.05, then there is overdispersion. library(AER) model=glm(Brooklyn_Bridge ~ Precipitation, data=NY, family=poisson) disp=dispersiontest(model, trafo=1) Note that the alpha is very large and the p-value is less than 0.05, which indicates that there is overdispersion. We will now create the negative binomial model. We will use the MASS library and the function glm.nb to create the model. The function is used to create a negative binomial model. The function is used in the same way as the glm function, but the family is set to negative binomial. library(MASS) ggplot(NY, aes(x=Precipitation, y=Brooklyn_Bridge))+ geom_jitter(width=0.01, height = 0.1)+ geom_smooth(method=&quot;glm.nb&quot;, se=T)+ xlab(&quot;Precipitation in inches&quot;)+ ylab(&quot;Number of cyclist&quot;)+ scale_x_continuous(breaks = seq(0, 2, by = 0.1)) NY_model_nb=MASS::glm.nb(Brooklyn_Bridge ~ Precipitation, data=NY) summary(NY_model_nb) ## ## Call: ## MASS::glm.nb(formula = Brooklyn_Bridge ~ Precipitation, data = NY, ## init.theta = 6.243688849, link = log) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 8.08310 0.03105 260.33 &lt; 2e-16 *** ## Precipitation -0.53141 0.10670 -4.98 6.35e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(6.2437) family taken to be 1) ## ## Null deviance: 226.06 on 197 degrees of freedom ## Residual deviance: 203.38 on 196 degrees of freedom ## (17 observations deleted due to missingness) ## AIC: 3360.2 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 6.244 ## Std. Err.: 0.613 ## ## 2 x log-likelihood: -3354.199 We interpret the results as follows The precipitation has a negative effect on the number of cyclist (negative sign). With precipitation (exp(coef) = -0.53) which states that there is 47% decrease in the number of cyclist for each inch of precipitation. We can reconvert the coefficient to a positive value by taking the exponential of the coefficient. exp(coef(model)) ## (Intercept) Precipitation ## 3250.0627649 0.5680487 Overlaying the Poisson and the negative binomial models NEED To FIND a better example ggplot(NY, aes(x=Precipitation, y=Brooklyn_Bridge))+ geom_jitter(width=0.01, height = 0.1)+ geom_smooth(method=&quot;glm&quot;, method.args=list(family=&quot;poisson&quot;), se=T, color=&quot;red&quot;)+ geom_smooth(method=&quot;glm.nb&quot;, se=T, color=&quot;blue&quot;)+ xlab(&quot;Precipitation in inches&quot;)+ ylab(&quot;Number of cyclist&quot;)+ scale_x_continuous(breaks = seq(0, 2, by = 0.1)) "],["multinomial-logit-analysis.html", "Chapter 18 Multinomial logit analysis", " Chapter 18 Multinomial logit analysis A multinomial logit model is a type of regression analysis used to model nominal outcome variables. In this model, the dependent variable is a nominal variable with more than two levels. The model is used to estimate the probability of each level of the nominal variable. The model is an extension of the binary logit model, which is used to model binary outcome variables. The multinomial logit model is used in a wide range of fields, including economics, political science, and marketing. The model is based on the assumption that the log-odds of each level of the nominal variable are a linear function of the predictor variables. The model is estimated using maximum likelihood estimation, and the coefficients of the model are interpreted as the change in the log-odds of each level of the nominal variable associated with a one-unit change in the predictor variable. In this tutorial, we will walk through an example of how to estimate a multinomial logit model in R using the nnet package. We will use the iris dataset, which is a built-in dataset in R that contains measurements of iris flowers. The outcome variable in the dataset is the species of the iris flower, which has three levels: setosa, versicolor, and virginica. We will estimate a multinomial logit model to predict the species of the iris flower based on the measurements of the flower. 18.0.1 Load libraries library(tidyverse) library(nnet) 18.0.2 Data The data used in this analysis is the iris dataset, which is a built-in dataset in R. The dataset contains 150 observations of iris flowers, with four predictor variables (sepal length, sepal width, petal length, and petal width) and one outcome variable (species of iris flower). The outcome variable has three levels: setosa, versicolor, and virginica. data(iris) head(iris) Sepal.LengthSepal.WidthPetal.LengthPetal.WidthSpecies 5.13.51.40.2setosa 4.93&nbsp;&nbsp;1.40.2setosa 4.73.21.30.2setosa 4.63.11.50.2setosa 5&nbsp;&nbsp;3.61.40.2setosa 5.43.91.70.4setosa 18.0.3 Model 18.0.4 Assumptions 18.0.5 Post estimation analysis "],["beta-regression.html", "Chapter 19 Beta Regression 19.1 What is a beta regression? 19.2 What is the problem with the data? 19.3 First step, what is a beta distribution? 19.4 Wikipedia 19.5 Proportion of smokers in different countries 19.6 Convert the mean and variance into shape \\(\\alpha\\) and \\(\\beta\\) 19.7 Visualization of the distribution with CI. 19.8 Regression model 19.9 Regresión beta, proporción de frutos por cantidad de flores 19.10 Visualize the beta regression 19.11 Compare the traditional linear model versus a beta model 19.12 Distribution of specific values", " Chapter 19 Beta Regression The method presented here is quite innovative (2010 onwards). Unfortunately there is not much information in the literature or the web about the method. You can find supplementary information in the Vignette of the package betareg. References: Cribari-Neto, F., and Zeileis, A. (2010). Beta Regression in R. Journal of Statistical Software, 34(2), 1–24. http://www.jstatsoft.org/v34/i02/. Grün, B., Kosmidis, I., and Zeileis, A. (2012). Extended Beta Regression in R: Shaken, Stirred, Mixed, and Partitioned. Journal of Statistical Software, 48(11), 1–25. http://www.jstatsoft.org/v48/i11/. Article on the beta regression and other R packages with other functions can be found here, Douma and Weedon 2019 19.1 What is a beta regression? The beta regression is an approach under GLM “Generalized linear models”. The beta regression models dependent variables distributed with the beta distribution. Data with the beta distribution include proportions and ratios, where the values \\(x\\) are between 0 and 1 but not inclusive (i.e. \\(0 &lt; x&lt; 1\\)). In some packahes 0 and 1 can be part of the data set. In addition to producing a regression that maximizes the likelihood (both for the mean and the precision of a response distributed in beta), bias-corrected estimates are provided. The values of the response variable satisfy \\(0 &lt; x &lt; 1\\). Consequently, if the values are 0 or 1, it is necessary to change them to \\(0= 0.001\\) and \\(1 = 0.999\\). The numbers cannot be 0 or 1, they must be greater than 0 and less than 1. Indeed, changing the values to \\(0.001\\) and \\(0.999\\) has no impact on the interpretation of the data, unless all the data are only \\(0\\) or \\(1\\), in which case this tool should not be used but a logistic regression. The betaref package is used to perform the beta regression. The package is quite powerful and provides a lot of information about the data. The package is used to calculate the mean, the variance, the precision, and the quantiles of the data. The package also provides the AIC, BIC, and the log-likelihood of the model. Note that the focus of the GLM model is to develop a regression with the response through a link function and a linear predictor. Just like normal GLM, there are numerous link functions, which can be useful such as “logit”, “probit”, “cloglog”, “cauchit”, “log”, “loglog” to linearize the data. Almosty all of the information presented here comes from Cribari-Neto and Zeileis (Beta Regression in R). Consult the pdf, https://cran.r-project.org/web/packages/betareg/betareg.pdf for a package description and more details. Typical analysis errors with data that are fractions. We look at an example. Here the relationship between per capita public health spending in 156 countries and the percentage of girls who are not in school. Data from the “World Development Agency” 19.2 What is the problem with the data? library(ggversa) #Edu_Salud_Gastos_GDP ggplot(Edu_Salud_Gastos_GDP, aes(Gasto_Salud_percapita, Porc_Ninas_no_escuela))+ geom_point()+ geom_smooth(method = lm)+ xlab(&quot;Per capita public health spending&quot;)+ ylab(&quot;Percentage of girls not going to school&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 46 rows containing non-finite outside the scale range ## (`stat_smooth()`). ## Warning: Removed 46 rows containing missing values or values outside the scale range ## (`geom_point()`). Note: there are negative proportion values the confidence interval is also negative the dispersion of the data in y around the mean is not equal as public health spending per capita changes (in x). Models using the beta distribution solve these issues. 19.3 First step, what is a beta distribution? The most important thing about the beta distribution is that the values NEVER are less than 0 or greater than 1 (i.e. \\(0 &lt; x &lt; 1\\)). In addition, the confidence intervals cannot be less than 0 or greater than 1. Here are some examples of the beta distribution. The beta distribution is calculated with two parameters, shape 1 or \\(\\alpha\\) and shape 2 or \\(\\beta\\). We will not go into these parameters, although you can go to the Wikipedia page for more information. Note that if the parameters are not equal (\\(\\alpha \\neq \\beta\\)), the distribution is not symmetrical. There is always a tail that extends to small or large values. Here a series beta distributions. We will not go into how these these parameters are calculated, although you can go to the Wikipedia page for more information. Note that if the parameters are not equal (\\(\\alpha \\neq \\beta\\)), the distribution is not symmetrical. There is always a tail that extends to small or large values. 19.4 Wikipedia On the wikipedia page, you can see how the distribution changes when the parameters change. 19.5 Proportion of smokers in different countries The data comes from the World Bank at the following link, Smokers. The file contains information on 187 countries and the proportion of the population over 15 years old who smoke. library(readr) Proportion_smokers_world &lt;- read_csv(&quot;Data/Proportion_smokers_world.csv&quot;) ## Rows: 187 Columns: 13 ## ── Column specification ─────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): Country_Name, Country_Code, Indicator_Name, Indicator_Code ## dbl (9): Y2000, Y2005, Y2010, Y2011, Y2012, Y2013, Y2014, Y2015, Y2016 ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Smokers=Proportion_smokers_world gt(head(Smokers)) #fdnjgmjbcu table { font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji'; -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; } #fdnjgmjbcu thead, #fdnjgmjbcu tbody, #fdnjgmjbcu tfoot, #fdnjgmjbcu tr, #fdnjgmjbcu td, #fdnjgmjbcu th { border-style: none; } #fdnjgmjbcu p { margin: 0; padding: 0; } #fdnjgmjbcu .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; } #fdnjgmjbcu .gt_caption { padding-top: 4px; padding-bottom: 4px; } #fdnjgmjbcu .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; } #fdnjgmjbcu .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; } #fdnjgmjbcu .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fdnjgmjbcu .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fdnjgmjbcu .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; } #fdnjgmjbcu .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 6px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; } #fdnjgmjbcu .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; } #fdnjgmjbcu .gt_column_spanner_outer:first-child { padding-left: 0; } #fdnjgmjbcu .gt_column_spanner_outer:last-child { padding-right: 0; } #fdnjgmjbcu .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; } #fdnjgmjbcu .gt_spanner_row { border-bottom-style: hidden; } #fdnjgmjbcu .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; } #fdnjgmjbcu .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; } #fdnjgmjbcu .gt_from_md > :first-child { margin-top: 0; } #fdnjgmjbcu .gt_from_md > :last-child { margin-bottom: 0; } #fdnjgmjbcu .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; } #fdnjgmjbcu .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; } #fdnjgmjbcu .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; } #fdnjgmjbcu .gt_row_group_first td { border-top-width: 2px; } #fdnjgmjbcu .gt_row_group_first th { border-top-width: 2px; } #fdnjgmjbcu .gt_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fdnjgmjbcu .gt_first_summary_row { border-top-style: solid; border-top-color: #D3D3D3; } #fdnjgmjbcu .gt_first_summary_row.thick { border-top-width: 2px; } #fdnjgmjbcu .gt_last_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fdnjgmjbcu .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; text-transform: inherit; padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; } #fdnjgmjbcu .gt_first_grand_summary_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-top-style: double; border-top-width: 6px; border-top-color: #D3D3D3; } #fdnjgmjbcu .gt_last_grand_summary_row_top { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; border-bottom-style: double; border-bottom-width: 6px; border-bottom-color: #D3D3D3; } #fdnjgmjbcu .gt_striped { background-color: rgba(128, 128, 128, 0.05); } #fdnjgmjbcu .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; } #fdnjgmjbcu .gt_footnotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fdnjgmjbcu .gt_footnote { margin: 0px; font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fdnjgmjbcu .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; } #fdnjgmjbcu .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; } #fdnjgmjbcu .gt_left { text-align: left; } #fdnjgmjbcu .gt_center { text-align: center; } #fdnjgmjbcu .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #fdnjgmjbcu .gt_font_normal { font-weight: normal; } #fdnjgmjbcu .gt_font_bold { font-weight: bold; } #fdnjgmjbcu .gt_font_italic { font-style: italic; } #fdnjgmjbcu .gt_super { font-size: 65%; } #fdnjgmjbcu .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; } #fdnjgmjbcu .gt_asterisk { font-size: 100%; vertical-align: 0; } #fdnjgmjbcu .gt_indent_1 { text-indent: 5px; } #fdnjgmjbcu .gt_indent_2 { text-indent: 10px; } #fdnjgmjbcu .gt_indent_3 { text-indent: 15px; } #fdnjgmjbcu .gt_indent_4 { text-indent: 20px; } #fdnjgmjbcu .gt_indent_5 { text-indent: 25px; } #fdnjgmjbcu .katex-display { display: inline-flex !important; margin-bottom: 0.75em !important; } #fdnjgmjbcu div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after { height: 0px !important; } Country_Name Country_Code Indicator_Name Indicator_Code Y2000 Y2005 Y2010 Y2011 Y2012 Y2013 Y2014 Y2015 Y2016 Honduras HND Smoking prevalence, total (ages 15+) SH.PRV.SMOK 3.9 3.1 2.5 2.4 2.4 2.3 2.2 2.1 2.0 Ethiopia ETH Smoking prevalence, total (ages 15+) SH.PRV.SMOK 4.8 4.6 4.5 4.4 4.5 4.4 4.4 4.4 4.4 Congo, Rep. COG Smoking prevalence, total (ages 15+) SH.PRV.SMOK 5.7 9.1 14.7 16.1 17.9 19.8 22.0 24.2 26.9 Ghana GHA Smoking prevalence, total (ages 15+) SH.PRV.SMOK 5.9 5.0 4.4 4.3 4.3 4.1 4.1 4.0 3.9 Niger NER Smoking prevalence, total (ages 15+) SH.PRV.SMOK 6.4 6.7 7.1 7.2 7.3 7.4 7.5 7.6 7.7 Nigeria NGA Smoking prevalence, total (ages 15+) SH.PRV.SMOK 7.7 7.0 6.3 6.2 6.1 6.1 5.9 5.8 5.8 First convert the data in proportions since the program has to use data greater than 0 and less than 1. Select the year 2000 and create a histogram of the distribution. Smokers$Y2000P=(Smokers$Y2000)/100 # convertir en proporción Smokers %&gt;% dplyr::select(Country_Name, Y2000P) (#tab:beta_7) Country_NameY2000P Honduras0.039 Ethiopia0.048 Congo, Rep.0.057 Ghana0.059 Niger0.064 Nigeria0.077 Oman0.082 Barbados0.083 Eritrea0.087 Benin0.091 Eswatini0.093 Togo0.1&nbsp;&nbsp; Bahamas, The0.106 Senegal0.109 Haiti0.121 Peru0.121 Cabo Verde0.13&nbsp; Mali0.13&nbsp; Sub-Saharan Africa (excluding high income)0.134 Sub-Saharan Africa0.134 Sub-Saharan Africa (IDA &amp; IBRD countries)0.134 Liberia0.138 Ecuador0.139 Saudi Arabia0.145 Panama0.15&nbsp; Uzbekistan0.158 El Salvador0.16&nbsp; Singapore0.163 Sri Lanka0.165 Qatar0.165 Algeria0.166 Kenya0.166 Brunei Darussalam0.17&nbsp; Uganda0.171 Burkina Faso0.172 Iran, Islamic Rep.0.175 IDA blend0.175 Djibouti0.176 Egypt, Arab Rep.0.176 Lesotho0.176 Rwanda0.178 Costa Rica0.18&nbsp; Zambia0.18&nbsp; Zimbabwe0.18&nbsp; Jamaica0.184 Dominican Republic0.187 Morocco0.187 Middle East &amp; North Africa (excluding high income)0.189 Middle East &amp; North Africa (IDA &amp; IBRD countries)0.189 Middle East &amp; North Africa0.19&nbsp; Arab World0.191 Botswana0.195 Gambia, The0.199 Colombia0.201 IDA total0.206 Malawi0.21&nbsp; Comoros0.211 India0.212 Namibia0.223 Tanzania0.223 Bahrain0.224 Lower middle income0.224 IDA only0.226 South Asia0.227 South Asia (IDA &amp; IBRD)0.227 South Africa0.227 Moldova0.233 Yemen, Rep.0.233 Mozambique0.234 Least developed countries: UN classification0.235 Early-demographic dividend0.239 Mexico0.24&nbsp; Australia0.245 Pakistan0.245 Latin America &amp; Caribbean (excluding high income)0.246 Kuwait0.248 Thailand0.249 Mauritius0.25&nbsp; Seychelles0.251 Vietnam0.251 United Arab Emirates0.252 Brazil0.252 Latin America &amp; the Caribbean (IDA &amp; IBRD countries)0.253 Latin America &amp; Caribbean0.258 Portugal0.259 Small states0.26&nbsp; Other small states0.262 Low &amp; middle income0.263 Italy0.265 IDA &amp; IBRD total0.265 Azerbaijan0.267 Middle income0.267 Slovenia0.268 Kyrgyz Republic0.273 IBRD only0.277 World0.277 Canada0.282 Malaysia0.282 New Zealand0.294 Palau0.296 Finland0.297 China0.301 Iceland0.301 Cambodia0.301 Upper middle income0.301 East Asia &amp; Pacific (excluding high income)0.304 East Asia &amp; Pacific (IDA &amp; IBRD countries)0.304 Late-demographic dividend0.304 East Asia &amp; Pacific0.305 Paraguay0.308 North America0.311 Switzerland0.312 Bangladesh0.314 United States0.314 Armenia0.318 Tunisia0.318 Israel0.319 Kazakhstan0.319 Vanuatu0.319 Slovak Republic0.321 Georgia0.322 Mongolia0.322 Sweden0.323 Myanmar0.325 Indonesia0.329 Japan0.33&nbsp; OECD members0.33&nbsp; Croatia0.331 High income0.336 Post-demographic dividend0.338 Czech Republic0.341 Korea, Rep.0.343 Philippines0.344 Luxembourg0.347 Albania0.348 France0.349 Euro area0.35&nbsp; Fiji0.351 Germany0.353 Malta0.353 Lithuania0.359 Ukraine0.36&nbsp; European Union0.361 Tonga0.363 Belarus0.367 Europe &amp; Central Asia0.37&nbsp; Maldives0.371 Andorra0.374 Belgium0.374 Lebanon0.375 Netherlands0.377 Ireland0.378 United Kingdom0.382 Europe &amp; Central Asia (excluding high income)0.383 Pacific island small states0.383 Denmark0.383 Turkey0.384 Europe &amp; Central Asia (IDA &amp; IBRD countries)0.385 Latvia0.388 Nepal0.389 Spain0.395 Central Europe and the Baltics0.395 Estonia0.396 Romania0.396 Poland0.407 Hungary0.411 Argentina0.414 Sierra Leone0.422 Cyprus0.424 Lao PDR0.427 Russian Federation0.428 Norway0.431 Samoa0.451 Cuba0.457 Bosnia and Herzegovina0.477 Serbia0.487 Austria0.491 Suriname0.5&nbsp;&nbsp; Timor-Leste0.518 Bulgaria0.52&nbsp; Montenegro0.527 Uruguay0.527 Greece0.535 Chile0.566 Papua New Guinea0.609 Nauru0.638 Kiribati0.734 19.6 Convert the mean and variance into shape \\(\\alpha\\) and \\(\\beta\\) Conver the mean and variance of the data into the values of the shape \\(\\alpha\\) and \\(\\beta\\). The following equation is used to calculate the shapes. The expected values and the variance behave differently. The mean of a beta distribution is equal to \\[E(X) = \\frac{\\alpha}{\\alpha+\\beta}\\] The variance os beta distribution is \\[V(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta+1)(\\alpha+\\beta)^2}\\] Using the mean and variance, the following equations can be used to convert them into \\(\\alpha\\) and \\(\\beta\\). \\[\\alpha = \\frac{1-mu}{(var-1)/mu}*mu^2\\] \\[\\beta = \\alpha*(\\frac{1}{mu}-1)\\] Here is the script to convert the parameters into shape. estBetaParams &lt;- function(mu, var) { alpha &lt;- ((1 - mu) / var - 1 / mu) * mu ^ 2 beta &lt;- alpha * (1 / mu - 1) return(params = list(alpha = alpha, beta = beta)) } #mean(Smokers$Y2000P) #var(Smokers$Y2000P) estBetaParams((mean(Smokers$Y2000P)), (var(Smokers$Y2000P))) ## $alpha ## [1] 3.592488 ## ## $beta ## [1] 9.181559 Now visualize de distibution of the proportion of smokers Green line is the normal distribution Red line is the beta distribution Smokers$Y2000P=(Smokers$Y2000)/100 # convertir en proporción x &lt;- seq(0, 1, len = 100) #mean(Smokers$Y2000P) #var(Smokers$Y2000P) ggplot(Smokers, aes(Y2000P))+ geom_histogram(aes(y=..density..), colour=&quot;white&quot;, fill=&quot;grey50&quot;)+ stat_function(aes(x = Smokers$Y2000P, y = ..y..), fun = dbeta, colour=&quot;red&quot;, n = 100, args = list(shape1 = 3.593, shape2 = 9.185))+ stat_function(fun = dnorm, args = list(mean = mean(Smokers$Y2000P, na.rm = TRUE), sd = sd(Smokers$Y2000P, na.rm = TRUE)), colour = &quot;green&quot;, size = 1)+ xlab(&quot;Proporción de fumadores de mayor \\n de 15 años por Pais&quot;)+ ylab(&quot;Densidad&quot;)+ annotate(&quot;text&quot;, x = .5, y = 4.2, label = &quot;Verde: dist Normal&quot;, color=&quot;darkgreen&quot;)+ annotate(&quot;text&quot;, x = .5, y = 3.7, label = &quot;Verde: dist Beta&quot;, color=&quot;red&quot;)+ xlim(-0.1, 0.9) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 2 rows containing missing values or values outside the scale range ## (`geom_bar()`). Calculate the confidence interval of the average of a beta distribution. The following packages are needed simpleboot, boot. library(simpleboot) ## Simple Bootstrap Routines (1.1-8) library(boot) # package to calculate the confidence interval of a beta distribution n=187 # The smaple size of the data set. alpha = 3.593 # The alpha parameter beta = 9.185 # the beta parameter x = rbeta(n, alpha, beta) x.boot = one.boot(x, median, R=10^4) # Here we use the median as the central tendency because the *mean* is bias to the right boot.ci(x.boot, type=&quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = x.boot, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% ( 0.2463, 0.2961 ) ## Calculations and Intervals on Original Scale 19.7 Visualization of the distribution with CI. Overlay the confidence interval of the median on the beta distribution. The confidence interval is between 0.2320 and 0.2768. The confidence interval is not symmetric, and the distribution is not symmetric. The distribution is skewed to the right. ggplot(Smokers, aes(Y2000P))+ geom_histogram(aes(y=..density..), bins=20, colour=&quot;white&quot;, fill=&quot;grey50&quot;)+ stat_function(aes(x = Smokers$Y2000P, y = ..y..), fun = dbeta, colour=&quot;red&quot;, n = 100, args = list(shape1 = 3.593, shape2 = 9.185))+ geom_vline(xintercept =0.2320, colour=&quot;blue&quot;)+ # the confidence interval, lower limit geom_vline(xintercept =0.2768, colour=&quot;blue&quot;)+ # the confidence interval, upper limit rlt_theme+ xlab(&quot;Proportion of smokers over 15 years old by country&quot;) 19.8 Regression model Now let us view an example using regression model, where the response is a proportion. library(betareg) # El paquete para hacer regresión beta library(ggversa) # paquete para los datos attach(dipodium) head(dipodium, n=4) (#tab:beta_12) tree_numbertree_speciesdbhplant_numberramet_numberdistanceorientationnumber_of_flowersheight_infloherbivoryrow_position_nfnumber_flowers_positionnumber_of_fruitsperc_fr_setpardalinum_or_roseumfruit_position_effectfrutos_si_o_nop_or_r_infl_lenghtnum_of_fruitsspecies_namecardinal_orientation 1E.o75112.47401135n12400&nbsp;&nbsp;&nbsp;r10r0r1 1E.o76211.97501947n22300&nbsp;&nbsp;&nbsp;r20r0r2 2E.o76311.953501863n32510.04r30r1r8 3E.o58413.242102447n42050.25r40r5r5 19.9 Regresión beta, proporción de frutos por cantidad de flores Now we will do the first regression analysis where our answer is a proportion. The data comes from an Australian orchid species, Dipodium roseum, collected by RLT in 2004-2005. We are going to evaluate the relationship between the number of flowers and the proportion of fruits per plant. The first step is to ensure that there are no values of 0 and 1. In this case there is not a single plant that has 100% fruits, but there are individuals that have zero fruits. REMEMBER x &gt;0 y &lt;1. 0 or 1 is NOT accepted. So a minimum value such as 0.001 can be added to the values of 0 and 0.001 can be subtracted from the 1 values. In reality, this modification does not impact the interpretation of the results. the NAs are also removed from the file. Note that the model is constructed as a linear model betareg(y~x, data =na.omit(df)). The variables in the file are PropFR, the proportion of fruits (number of fruits/numbers of flowers) for each individual and the number of flowers, Number_of_Flowers. #head(dipodium) library(tidyverse) library(janitor) dipodium_clean_names=clean_names(dipodium) # clean the names of the variables dipodium2= dipodium_clean_names |&gt; dplyr::select(perc_fr_set, number_of_flowers, height_inflo, distance) |&gt; mutate(propfr=perc_fr_set+0.0001) |&gt; drop_na()# solucionar para remover los cero head(dipodium2) (#tab:beta_14) perc_fr_setnumber_of_flowersheight_inflodistancepropfr 0&nbsp;&nbsp;&nbsp;11352.470.0001 0&nbsp;&nbsp;&nbsp;19471.970.0001 0.0418631.950.0401 0.2524473.240.25&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp;25610.850.0001 0.0817352.620.0801 modelpropFr=betareg(propfr~number_of_flowers+height_inflo+distance, data =dipodium2) summary(modelpropFr) ## ## Call: ## betareg(formula = propfr ~ number_of_flowers + height_inflo + distance, ## data = dipodium2) ## ## Quantile residuals: ## Min 1Q Median 3Q Max ## -1.9667 -1.0253 0.3277 0.8273 1.6420 ## ## Coefficients (mean model with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.01521 0.75104 -2.683 0.00729 ** ## number_of_flowers 0.06069 0.02958 2.052 0.04020 * ## height_inflo -0.02469 0.01896 -1.302 0.19284 ## distance -0.12250 0.09850 -1.244 0.21361 ## ## Phi coefficients (precision model with identity link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (phi) 3.5330 0.7936 4.452 8.52e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Type of estimator: ML (maximum likelihood) ## Log-likelihood: 139.5 on 5 Df ## Pseudo R-squared: 0.1018 ## Number of iterations: 20 (BFGS) + 1 (Fisher scoring) 19.10 Visualize the beta regression dipodiumbeta=dipodium2[,c(&quot;number_of_flowers&quot;,&quot;propfr&quot;)] # Create a df with only the variables of interest dp2=dipodiumbeta[complete.cases(dipodiumbeta),] # remove all &quot;NA&quot; modelpropFr=betareg(propfr~number_of_flowers, data=dp2) # The beta model using **betareg** summary(modelpropFr) ## ## Call: ## betareg(formula = propfr ~ number_of_flowers, data = dp2) ## ## Quantile residuals: ## Min 1Q Median 3Q Max ## -1.9515 -1.0815 0.2592 0.8719 1.6788 ## ## Coefficients (mean model with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.95068 0.45003 -6.557 5.51e-11 *** ## number_of_flowers 0.03047 0.01873 1.627 0.104 ## ## Phi coefficients (precision model with identity link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (phi) 3.4044 0.7682 4.432 9.35e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Type of estimator: ML (maximum likelihood) ## Log-likelihood: 138.4 on 3 Df ## Pseudo R-squared: 0.0644 ## Number of iterations: 14 (BFGS) + 2 (Fisher scoring) predict(modelpropFr, type = &quot;response&quot;) # calculate the predicted values ## 1 2 3 4 5 6 7 ## 0.06814386 0.08534612 0.08299774 0.09801287 0.10073942 0.08070828 0.08534612 ## 8 9 10 11 12 13 14 ## 0.07418174 0.07847656 0.09535228 0.07418174 0.05578661 0.09535228 0.09022432 ## 15 16 17 18 19 20 21 ## 0.09022432 0.07847656 0.07211634 0.12506577 0.07630144 0.08070828 0.07418174 ## 22 23 24 25 26 27 28 ## 0.09535228 0.11540253 0.10932659 0.08299774 0.08070828 0.13188690 0.07211634 ## 29 30 31 32 33 34 35 ## 0.11540253 0.12176997 0.09022432 0.10932659 0.07418174 0.07418174 0.07847656 ## 36 37 38 39 40 41 42 ## 0.09801287 0.07630144 0.10353312 0.10353312 0.08775459 0.07418174 0.09535228 ## 43 44 45 46 47 48 49 ## 0.09801287 0.09022432 0.08534612 0.10353312 0.09275650 0.08534612 0.05420339 ## 50 51 52 53 54 55 56 ## 0.13541430 0.12176997 0.09022432 0.09535228 0.09535228 0.07847656 0.09275650 ## 57 58 59 60 61 62 ## 0.07211634 0.09801287 0.10932659 0.10932659 0.09801287 0.05741327 dp2$response=predict(modelpropFr, type = &quot;response&quot;) dp2$precision=predict(modelpropFr, type = &quot;precision&quot;) dp2$variance=predict(modelpropFr, type = &quot;variance&quot;) dp2$quantile_.01=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.01)) # calculate the quantiles of 1% dp2$quantile_.05=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.05)) dp2$quantile_.10=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.10)) # calculate the quantiles of 10% dp2$quantile_.15=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.15)) dp2$quantile_.20=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.20)) dp2$quantile_.25=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.25)) dp2$quantile_.30=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.30)) dp2$quantile_.35=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.35)) dp2$quantile_.40=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.40)) dp2$quantile_.45=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.45)) dp2$quantile_.50=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.50)) # calculate the quantiles of 50% (the median) dp2$quantile_.55=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.55)) dp2$quantile_.60=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.60)) dp2$quantile_.65=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.65)) dp2$quantile_.70=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.70)) dp2$quantile_.75=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.75)) dp2$quantile_.80=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.80)) dp2$quantile_.85=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.85)) dp2$quantile_.90=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.90)) # calculate the quantiles of 90% dp2$quantile_.95=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.95)) dp2$quantile_.99=predict(modelpropFr, type = &quot;quantile&quot;, at = c(0.99)) # calculate the quantiles of 99% dp2 (#tab:beta_15) number_of_flowerspropfrresponseprecisionvariancequantile_.01quantile_.05quantile_.10quantile_.15quantile_.20quantile_.25quantile_.30quantile_.35quantile_.40quantile_.45quantile_.50quantile_.55quantile_.60quantile_.65quantile_.70quantile_.75quantile_.80quantile_.85quantile_.90quantile_.95quantile_.99 110.00010.06813.40.01445.69e-105.87e-071.16e-056.69e-050.0002310.0006050.00133&nbsp;0.00259&nbsp;0.004620.007720.0123&nbsp;0.01870.02760.03990.05650.07910.11&nbsp;&nbsp;0.1540.22&nbsp;0.3350.565 190.00010.08533.40.01773.27e-088.32e-069.04e-050.0003650.0009840.00212&nbsp;0.00399&nbsp;0.00682&nbsp;0.0109&nbsp;0.0164&nbsp;0.0239&nbsp;0.03370.04650.06290.084&nbsp;0.111&nbsp;0.147&nbsp;0.1960.2660.3820.605 180.04010.083&nbsp;3.40.01732.07e-086.17e-067.18e-050.0003020.0008350.00184&nbsp;0.00352&nbsp;0.00611&nbsp;0.009860.0151&nbsp;0.0222&nbsp;0.03150.04380.05960.08020.107&nbsp;0.142&nbsp;0.19&nbsp;0.26&nbsp;0.3760.6&nbsp;&nbsp; 240.25&nbsp;&nbsp;0.098&nbsp;3.40.02012.63e-073.27e-050.0002610.00088&nbsp;0.00209&nbsp;0.00409&nbsp;0.00709&nbsp;0.0113&nbsp;&nbsp;0.0171&nbsp;0.0246&nbsp;0.0342&nbsp;0.04640.06160.08070.104&nbsp;0.135&nbsp;0.173&nbsp;0.2240.2960.4130.631 250.00010.101&nbsp;3.40.02063.84e-074.2e-05&nbsp;0.0003170.00103&nbsp;0.0024&nbsp;&nbsp;0.00461&nbsp;0.00789&nbsp;0.0125&nbsp;&nbsp;0.0186&nbsp;0.0265&nbsp;0.0366&nbsp;0.04920.065&nbsp;0.08450.109&nbsp;0.139&nbsp;0.179&nbsp;0.23&nbsp;0.3030.42&nbsp;0.636 170.08010.08073.40.01681.3e-08&nbsp;4.54e-065.66e-050.0002480.0007060.00159&nbsp;0.0031&nbsp;&nbsp;0.00545&nbsp;0.008920.0138&nbsp;0.0205&nbsp;0.02940.04120.05650.07650.103&nbsp;0.137&nbsp;0.1850.2540.37&nbsp;0.595 190.00010.08533.40.01773.27e-088.32e-069.04e-050.0003650.0009840.00212&nbsp;0.00399&nbsp;0.00682&nbsp;0.0109&nbsp;0.0164&nbsp;0.0239&nbsp;0.03370.04650.06290.084&nbsp;0.111&nbsp;0.147&nbsp;0.1960.2660.3820.605 140.00010.07423.40.01562.92e-091.71e-062.66e-050.0001320.0004140.001&nbsp;&nbsp;&nbsp;0.00207&nbsp;0.00382&nbsp;0.0065&nbsp;0.0104&nbsp;0.016&nbsp;&nbsp;0.02360.034&nbsp;0.04770.066&nbsp;0.09040.123&nbsp;0.1690.2370.3520.58&nbsp; 160.03710.07853.40.01648.01e-093.31e-064.43e-050.0002020.0005940.00137&nbsp;0.00272&nbsp;0.00486&nbsp;0.008050.0126&nbsp;0.0189&nbsp;0.02740.03870.05350.07290.09840.133&nbsp;0.18&nbsp;0.2480.3640.59&nbsp; 230.333&nbsp;0.09543.40.01961.77e-072.52e-050.0002140.0007450.00181&nbsp;0.00361&nbsp;0.00636&nbsp;0.0103&nbsp;&nbsp;0.0157&nbsp;0.0228&nbsp;0.0319&nbsp;0.04360.05840.07690.1&nbsp;&nbsp;&nbsp;0.13&nbsp;&nbsp;0.168&nbsp;0.2180.29&nbsp;0.4070.626 140.364&nbsp;0.07423.40.01562.92e-091.71e-062.66e-050.0001320.0004140.001&nbsp;&nbsp;&nbsp;0.00207&nbsp;0.00382&nbsp;0.0065&nbsp;0.0104&nbsp;0.016&nbsp;&nbsp;0.02360.034&nbsp;0.04770.066&nbsp;0.09040.123&nbsp;0.1690.2370.3520.58&nbsp; 40.00010.05583.40.012&nbsp;6.76e-123.24e-081.25e-061.05e-054.79e-050.0001550.0004050.0009140.001850.003450.006030.01&nbsp;&nbsp;0.01610.02490.03760.05590.08250.1220.1840.2950.53&nbsp; 230.00010.09543.40.01961.77e-072.52e-050.0002140.0007450.00181&nbsp;0.00361&nbsp;0.00636&nbsp;0.0103&nbsp;&nbsp;0.0157&nbsp;0.0228&nbsp;0.0319&nbsp;0.04360.05840.07690.1&nbsp;&nbsp;&nbsp;0.13&nbsp;&nbsp;0.168&nbsp;0.2180.29&nbsp;0.4070.626 210.179&nbsp;0.09023.40.01867.81e-081.47e-050.0001410.0005270.00135&nbsp;0.00279&nbsp;0.00507&nbsp;0.00842&nbsp;0.0131&nbsp;0.0194&nbsp;0.0277&nbsp;0.03850.05220.06970.09180.12&nbsp;&nbsp;0.157&nbsp;0.2070.2780.3950.615 210.154&nbsp;0.09023.40.01867.81e-081.47e-050.0001410.0005270.00135&nbsp;0.00279&nbsp;0.00507&nbsp;0.00842&nbsp;0.0131&nbsp;0.0194&nbsp;0.0277&nbsp;0.03850.05220.06970.09180.12&nbsp;&nbsp;0.157&nbsp;0.2070.2780.3950.615 160.00010.07853.40.01648.01e-093.31e-064.43e-050.0002020.0005940.00137&nbsp;0.00272&nbsp;0.00486&nbsp;0.008050.0126&nbsp;0.0189&nbsp;0.02740.03870.05350.07290.09840.133&nbsp;0.18&nbsp;0.2480.3640.59&nbsp; 130.03580.07213.40.01521.72e-091.21e-062.03e-050.0001060.0003430.0008510.00179&nbsp;0.00337&nbsp;0.005820.009470.0147&nbsp;0.02190.03170.045&nbsp;0.06270.08650.119&nbsp;0.1640.2310.3460.575 330.154&nbsp;0.125&nbsp;3.40.02485.59e-060.0002450.00125&nbsp;0.00325&nbsp;0.00641&nbsp;0.0109&nbsp;&nbsp;0.0169&nbsp;&nbsp;0.0245&nbsp;&nbsp;0.0339&nbsp;0.0455&nbsp;0.0594&nbsp;0.076&nbsp;0.09580.119&nbsp;0.148&nbsp;0.182&nbsp;0.225&nbsp;0.28&nbsp;0.3550.4710.677 150.00010.07633.40.016&nbsp;4.87e-092.39e-063.44e-050.0001640.0004970.00117&nbsp;0.00237&nbsp;0.00431&nbsp;0.007250.0115&nbsp;0.0174&nbsp;0.02550.03630.05050.06940.09440.128&nbsp;0.1740.2430.3580.585 170.333&nbsp;0.08073.40.01681.3e-08&nbsp;4.54e-065.66e-050.0002480.0007060.00159&nbsp;0.0031&nbsp;&nbsp;0.00545&nbsp;0.008920.0138&nbsp;0.0205&nbsp;0.02940.04120.05650.07650.103&nbsp;0.137&nbsp;0.1850.2540.37&nbsp;0.595 140.105&nbsp;0.07423.40.01562.92e-091.71e-062.66e-050.0001320.0004140.001&nbsp;&nbsp;&nbsp;0.00207&nbsp;0.00382&nbsp;0.0065&nbsp;0.0104&nbsp;0.016&nbsp;&nbsp;0.02360.034&nbsp;0.04770.066&nbsp;0.09040.123&nbsp;0.1690.2370.3520.58&nbsp; 230.105&nbsp;0.09543.40.01961.77e-072.52e-050.0002140.0007450.00181&nbsp;0.00361&nbsp;0.00636&nbsp;0.0103&nbsp;&nbsp;0.0157&nbsp;0.0228&nbsp;0.0319&nbsp;0.04360.05840.07690.1&nbsp;&nbsp;&nbsp;0.13&nbsp;&nbsp;0.168&nbsp;0.2180.29&nbsp;0.4070.626 300.1&nbsp;&nbsp;&nbsp;0.115&nbsp;3.40.02322.2e-06&nbsp;0.0001320.0007740.00218&nbsp;0.00454&nbsp;0.00806&nbsp;0.0129&nbsp;&nbsp;0.0193&nbsp;&nbsp;0.0274&nbsp;0.0375&nbsp;0.05&nbsp;&nbsp;&nbsp;0.06510.08340.106&nbsp;0.133&nbsp;0.166&nbsp;0.207&nbsp;0.2610.3350.4510.662 280.06910.109&nbsp;3.40.02211.13e-068.53e-050.00055&nbsp;0.00164&nbsp;0.00355&nbsp;0.0065&nbsp;&nbsp;0.0107&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0236&nbsp;0.0328&nbsp;0.0443&nbsp;0.05840.07570.09680.123&nbsp;0.155&nbsp;0.195&nbsp;0.2480.3220.4390.651 180.00010.083&nbsp;3.40.01732.07e-086.17e-067.18e-050.0003020.0008350.00184&nbsp;0.00352&nbsp;0.00611&nbsp;0.009860.0151&nbsp;0.0222&nbsp;0.03150.04380.05960.08020.107&nbsp;0.142&nbsp;0.19&nbsp;0.26&nbsp;0.3760.6&nbsp;&nbsp; 170.00010.08073.40.01681.3e-08&nbsp;4.54e-065.66e-050.0002480.0007060.00159&nbsp;0.0031&nbsp;&nbsp;0.00545&nbsp;0.008920.0138&nbsp;0.0205&nbsp;0.02940.04120.05650.07650.103&nbsp;0.137&nbsp;0.1850.2540.37&nbsp;0.595 350.25&nbsp;&nbsp;0.132&nbsp;3.40.026&nbsp;9.96e-060.0003590.00168&nbsp;0.00417&nbsp;0.00795&nbsp;0.0132&nbsp;&nbsp;0.0199&nbsp;&nbsp;0.0284&nbsp;&nbsp;0.0388&nbsp;0.0513&nbsp;0.0662&nbsp;0.08380.105&nbsp;0.129&nbsp;0.159&nbsp;0.194&nbsp;0.238&nbsp;0.2930.3680.4840.687 130.09390.07213.40.01521.72e-091.21e-062.03e-050.0001060.0003430.0008510.00179&nbsp;0.00337&nbsp;0.005820.009470.0147&nbsp;0.02190.03170.045&nbsp;0.06270.08650.119&nbsp;0.1640.2310.3460.575 300.154&nbsp;0.115&nbsp;3.40.02322.2e-06&nbsp;0.0001320.0007740.00218&nbsp;0.00454&nbsp;0.00806&nbsp;0.0129&nbsp;&nbsp;0.0193&nbsp;&nbsp;0.0274&nbsp;0.0375&nbsp;0.05&nbsp;&nbsp;&nbsp;0.06510.08340.106&nbsp;0.133&nbsp;0.166&nbsp;0.207&nbsp;0.2610.3350.4510.662 320.111&nbsp;0.122&nbsp;3.40.02434.14e-060.0002010.00107&nbsp;0.00285&nbsp;0.00573&nbsp;0.00988&nbsp;0.0155&nbsp;&nbsp;0.0226&nbsp;&nbsp;0.0316&nbsp;0.0427&nbsp;0.0561&nbsp;0.07220.09160.115&nbsp;0.143&nbsp;0.177&nbsp;0.219&nbsp;0.2740.3480.4640.672 210.273&nbsp;0.09023.40.01867.81e-081.47e-050.0001410.0005270.00135&nbsp;0.00279&nbsp;0.00507&nbsp;0.00842&nbsp;0.0131&nbsp;0.0194&nbsp;0.0277&nbsp;0.03850.05220.06970.09180.12&nbsp;&nbsp;0.157&nbsp;0.2070.2780.3950.615 280.00010.109&nbsp;3.40.02211.13e-068.53e-050.00055&nbsp;0.00164&nbsp;0.00355&nbsp;0.0065&nbsp;&nbsp;0.0107&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0236&nbsp;0.0328&nbsp;0.0443&nbsp;0.05840.07570.09680.123&nbsp;0.155&nbsp;0.195&nbsp;0.2480.3220.4390.651 140.131&nbsp;0.07423.40.01562.92e-091.71e-062.66e-050.0001320.0004140.001&nbsp;&nbsp;&nbsp;0.00207&nbsp;0.00382&nbsp;0.0065&nbsp;0.0104&nbsp;0.016&nbsp;&nbsp;0.02360.034&nbsp;0.04770.066&nbsp;0.09040.123&nbsp;0.1690.2370.3520.58&nbsp; 140.231&nbsp;0.07423.40.01562.92e-091.71e-062.66e-050.0001320.0004140.001&nbsp;&nbsp;&nbsp;0.00207&nbsp;0.00382&nbsp;0.0065&nbsp;0.0104&nbsp;0.016&nbsp;&nbsp;0.02360.034&nbsp;0.04770.066&nbsp;0.09040.123&nbsp;0.1690.2370.3520.58&nbsp; 160.125&nbsp;0.07853.40.01648.01e-093.31e-064.43e-050.0002020.0005940.00137&nbsp;0.00272&nbsp;0.00486&nbsp;0.008050.0126&nbsp;0.0189&nbsp;0.02740.03870.05350.07290.09840.133&nbsp;0.18&nbsp;0.2480.3640.59&nbsp; 240.04560.098&nbsp;3.40.02012.63e-073.27e-050.0002610.00088&nbsp;0.00209&nbsp;0.00409&nbsp;0.00709&nbsp;0.0113&nbsp;&nbsp;0.0171&nbsp;0.0246&nbsp;0.0342&nbsp;0.04640.06160.08070.104&nbsp;0.135&nbsp;0.173&nbsp;0.2240.2960.4130.631 150.179&nbsp;0.07633.40.016&nbsp;4.87e-092.39e-063.44e-050.0001640.0004970.00117&nbsp;0.00237&nbsp;0.00431&nbsp;0.007250.0115&nbsp;0.0174&nbsp;0.02550.03630.05050.06940.09440.128&nbsp;0.1740.2430.3580.585 260.05010.104&nbsp;3.40.02115.56e-075.35e-050.0003830.00121&nbsp;0.00274&nbsp;0.00519&nbsp;0.00875&nbsp;0.0137&nbsp;&nbsp;0.0201&nbsp;0.0285&nbsp;0.039&nbsp;&nbsp;0.05220.06840.08850.113&nbsp;0.144&nbsp;0.184&nbsp;0.2360.3090.4260.641 260.00010.104&nbsp;3.40.02115.56e-075.35e-050.0003830.00121&nbsp;0.00274&nbsp;0.00519&nbsp;0.00875&nbsp;0.0137&nbsp;&nbsp;0.0201&nbsp;0.0285&nbsp;0.039&nbsp;&nbsp;0.05220.06840.08850.113&nbsp;0.144&nbsp;0.184&nbsp;0.2360.3090.4260.641 200.00010.08783.40.01825.09e-081.11e-050.0001130.00044&nbsp;0.00115&nbsp;0.00244&nbsp;0.00451&nbsp;0.00759&nbsp;0.0119&nbsp;0.0179&nbsp;0.0258&nbsp;0.03610.04930.06620.08790.116&nbsp;0.152&nbsp;0.2010.2720.3880.61&nbsp; 140.00010.07423.40.01562.92e-091.71e-062.66e-050.0001320.0004140.001&nbsp;&nbsp;&nbsp;0.00207&nbsp;0.00382&nbsp;0.0065&nbsp;0.0104&nbsp;0.016&nbsp;&nbsp;0.02360.034&nbsp;0.04770.066&nbsp;0.09040.123&nbsp;0.1690.2370.3520.58&nbsp; 230.05270.09543.40.01961.77e-072.52e-050.0002140.0007450.00181&nbsp;0.00361&nbsp;0.00636&nbsp;0.0103&nbsp;&nbsp;0.0157&nbsp;0.0228&nbsp;0.0319&nbsp;0.04360.05840.07690.1&nbsp;&nbsp;&nbsp;0.13&nbsp;&nbsp;0.168&nbsp;0.2180.29&nbsp;0.4070.626 240.179&nbsp;0.098&nbsp;3.40.02012.63e-073.27e-050.0002610.00088&nbsp;0.00209&nbsp;0.00409&nbsp;0.00709&nbsp;0.0113&nbsp;&nbsp;0.0171&nbsp;0.0246&nbsp;0.0342&nbsp;0.04640.06160.08070.104&nbsp;0.135&nbsp;0.173&nbsp;0.2240.2960.4130.631 210.278&nbsp;0.09023.40.01867.81e-081.47e-050.0001410.0005270.00135&nbsp;0.00279&nbsp;0.00507&nbsp;0.00842&nbsp;0.0131&nbsp;0.0194&nbsp;0.0277&nbsp;0.03850.05220.06970.09180.12&nbsp;&nbsp;0.157&nbsp;0.2070.2780.3950.615 190.238&nbsp;0.08533.40.01773.27e-088.32e-069.04e-050.0003650.0009840.00212&nbsp;0.00399&nbsp;0.00682&nbsp;0.0109&nbsp;0.0164&nbsp;0.0239&nbsp;0.03370.04650.06290.084&nbsp;0.111&nbsp;0.147&nbsp;0.1960.2660.3820.605 260.107&nbsp;0.104&nbsp;3.40.02115.56e-075.35e-050.0003830.00121&nbsp;0.00274&nbsp;0.00519&nbsp;0.00875&nbsp;0.0137&nbsp;&nbsp;0.0201&nbsp;0.0285&nbsp;0.039&nbsp;&nbsp;0.05220.06840.08850.113&nbsp;0.144&nbsp;0.184&nbsp;0.2360.3090.4260.641 220.296&nbsp;0.09283.40.01911.18e-071.94e-050.0001740.0006280.00156&nbsp;0.00318&nbsp;0.00569&nbsp;0.00932&nbsp;0.0143&nbsp;0.021&nbsp;&nbsp;0.0298&nbsp;0.041&nbsp;0.05520.07320.096&nbsp;0.125&nbsp;0.162&nbsp;0.2130.2840.4010.621 190.182&nbsp;0.08533.40.01773.27e-088.32e-069.04e-050.0003650.0009840.00212&nbsp;0.00399&nbsp;0.00682&nbsp;0.0109&nbsp;0.0164&nbsp;0.0239&nbsp;0.03370.04650.06290.084&nbsp;0.111&nbsp;0.147&nbsp;0.1960.2660.3820.605 30.00010.05423.40.01163.31e-122.03e-088.7e-07&nbsp;7.83e-063.72e-050.0001250.0003350.0007740.0016&nbsp;0.003030.005390.00910.01470.02310.03530.053&nbsp;0.07890.1180.1790.29&nbsp;0.525 360.00010.135&nbsp;3.40.02661.31e-050.0004310.00194&nbsp;0.0047&nbsp;&nbsp;0.00882&nbsp;0.0144&nbsp;&nbsp;0.0216&nbsp;&nbsp;0.0306&nbsp;&nbsp;0.0414&nbsp;0.0544&nbsp;0.0698&nbsp;0.08790.109&nbsp;0.134&nbsp;0.164&nbsp;0.2&nbsp;&nbsp;&nbsp;0.244&nbsp;0.3&nbsp;&nbsp;0.3750.4910.693 320.08580.122&nbsp;3.40.02434.14e-060.0002010.00107&nbsp;0.00285&nbsp;0.00573&nbsp;0.00988&nbsp;0.0155&nbsp;&nbsp;0.0226&nbsp;&nbsp;0.0316&nbsp;0.0427&nbsp;0.0561&nbsp;0.07220.09160.115&nbsp;0.143&nbsp;0.177&nbsp;0.219&nbsp;0.2740.3480.4640.672 210.00010.09023.40.01867.81e-081.47e-050.0001410.0005270.00135&nbsp;0.00279&nbsp;0.00507&nbsp;0.00842&nbsp;0.0131&nbsp;0.0194&nbsp;0.0277&nbsp;0.03850.05220.06970.09180.12&nbsp;&nbsp;0.157&nbsp;0.2070.2780.3950.615 230.231&nbsp;0.09543.40.01961.77e-072.52e-050.0002140.0007450.00181&nbsp;0.00361&nbsp;0.00636&nbsp;0.0103&nbsp;&nbsp;0.0157&nbsp;0.0228&nbsp;0.0319&nbsp;0.04360.05840.07690.1&nbsp;&nbsp;&nbsp;0.13&nbsp;&nbsp;0.168&nbsp;0.2180.29&nbsp;0.4070.626 230.05890.09543.40.01961.77e-072.52e-050.0002140.0007450.00181&nbsp;0.00361&nbsp;0.00636&nbsp;0.0103&nbsp;&nbsp;0.0157&nbsp;0.0228&nbsp;0.0319&nbsp;0.04360.05840.07690.1&nbsp;&nbsp;&nbsp;0.13&nbsp;&nbsp;0.168&nbsp;0.2180.29&nbsp;0.4070.626 160.077&nbsp;0.07853.40.01648.01e-093.31e-064.43e-050.0002020.0005940.00137&nbsp;0.00272&nbsp;0.00486&nbsp;0.008050.0126&nbsp;0.0189&nbsp;0.02740.03870.05350.07290.09840.133&nbsp;0.18&nbsp;0.2480.3640.59&nbsp; 220.04560.09283.40.01911.18e-071.94e-050.0001740.0006280.00156&nbsp;0.00318&nbsp;0.00569&nbsp;0.00932&nbsp;0.0143&nbsp;0.021&nbsp;&nbsp;0.0298&nbsp;0.041&nbsp;0.05520.07320.096&nbsp;0.125&nbsp;0.162&nbsp;0.2130.2840.4010.621 130.00010.07213.40.01521.72e-091.21e-062.03e-050.0001060.0003430.0008510.00179&nbsp;0.00337&nbsp;0.005820.009470.0147&nbsp;0.02190.03170.045&nbsp;0.06270.08650.119&nbsp;0.1640.2310.3460.575 240.00010.098&nbsp;3.40.02012.63e-073.27e-050.0002610.00088&nbsp;0.00209&nbsp;0.00409&nbsp;0.00709&nbsp;0.0113&nbsp;&nbsp;0.0171&nbsp;0.0246&nbsp;0.0342&nbsp;0.04640.06160.08070.104&nbsp;0.135&nbsp;0.173&nbsp;0.2240.2960.4130.631 280.00010.109&nbsp;3.40.02211.13e-068.53e-050.00055&nbsp;0.00164&nbsp;0.00355&nbsp;0.0065&nbsp;&nbsp;0.0107&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0236&nbsp;0.0328&nbsp;0.0443&nbsp;0.05840.07570.09680.123&nbsp;0.155&nbsp;0.195&nbsp;0.2480.3220.4390.651 280.235&nbsp;0.109&nbsp;3.40.02211.13e-068.53e-050.00055&nbsp;0.00164&nbsp;0.00355&nbsp;0.0065&nbsp;&nbsp;0.0107&nbsp;&nbsp;0.0163&nbsp;&nbsp;0.0236&nbsp;0.0328&nbsp;0.0443&nbsp;0.05840.07570.09680.123&nbsp;0.155&nbsp;0.195&nbsp;0.2480.3220.4390.651 240.06260.098&nbsp;3.40.02012.63e-073.27e-050.0002610.00088&nbsp;0.00209&nbsp;0.00409&nbsp;0.00709&nbsp;0.0113&nbsp;&nbsp;0.0171&nbsp;0.0246&nbsp;0.0342&nbsp;0.04640.06160.08070.104&nbsp;0.135&nbsp;0.173&nbsp;0.2240.2960.4130.631 50.00010.05743.40.01231.35e-115.09e-081.76e-061.4e-05&nbsp;6.12e-050.0001920.0004880.00107&nbsp;0.002130.003910.006730.01110.01750.02670.04&nbsp;&nbsp;0.059&nbsp;0.08620.1260.1890.3010.535 When constructing the figure for beta regression, one of the main advantages of using this approach is that the quartiles are calculated with a beta distribution. Therefore, the margin of error does NOT fall below 0 and does NOT exceed 1. Evaluate the following figure at each through regression. library(ggplot2) ggplot(dp2, aes(x=number_of_flowers, y=propfr))+ geom_point()+ geom_line(aes(y=quantile_.05), linetype=&quot;twodash&quot;, colour=&quot;blue&quot;)+ geom_line(aes(y=quantile_.25),linetype=2, colour=&quot;green&quot;)+ geom_line(aes(y=quantile_.50), colour=&quot;red&quot;)+ geom_line(aes(y=quantile_.75), linetype=2, colour=&quot;green&quot;)+ geom_line(aes(y=quantile_.95), linetype=&quot;twodash&quot;, colour=&quot;blue&quot;)+ ylab(&quot;Predicción de la proporción de frutos&quot;)+ xlab(&quot;Números de Flores&quot;)+ annotate(&quot;text&quot;, x=25, y=0.50, label=&quot;95th quartile&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=32, y=0.39, label=&quot;75th quartile&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=33, y=0.14, label=&quot;25th quartile&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=33, y=0.27, label=&quot;Median&quot;, fontface=&quot;italic&quot;)+ annotate(&quot;text&quot;, x=35, y=-0.02, label=&quot;5th quartile&quot;, fontface=&quot;italic&quot;)+ theme(axis.title.y = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;), axis.text.x = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;), axis.text.y = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;), axis.title.x = element_text(colour=&quot;grey20&quot;,size=20,face=&quot;bold&quot;))+ theme(legend.position=&quot;none&quot;)+ rlt_theme ggsave(&quot;Figures/Beta_number_flowers.png&quot;) ## Saving 7 x 5 in image This is a representation of the beta distributions in the x, number of flowers. In red we simulate the distribution of the expected proportion of fruits in plants that have 15 flowers and in the blue line we simulate the expected distribution of the proportion of fruits in plants with 30 flowers. 19.11 Compare the traditional linear model versus a beta model To compare the effectiveness of the models we use the Akaike Information Criterion (AIC). In a model selection approach, the smallest AIC index, which represents the most parsimonious model, is accepted as the best model. An AIC difference of 4 is significant. Note that the beta model is much better (AIC = -219) than the linear regression model (AIC = -102) #AIC(modelpropFr) # modelo beta modelpropFr_lm=lm(propfr~number_of_flowers, data=dp2) AIC(modelpropFr,modelpropFr_lm) (#tab:beta_17) dfAIC 3-271&nbsp;&nbsp; 3-99.7 19.12 Distribution of specific values Evaluating the beta distribution for specific values of x = Ratio of fruits per plant based on the number of flowers on the plant. Select the different values of x and calculate the mean and variance and convert these to \\(\\alpha\\) and \\(\\beta\\). With these parameters, the density of the distribution can be constructed for each value of x. Specific values are selected to display the distribution, plants that have 25, 30 and 35 flowers. The data has to be rearranged to calculate the mean and variance. dpQ=dp2 %&gt;% dplyr::select(c(1, 6:26)) dpQ2=dpQ%&gt;% dplyr::filter(number_of_flowers== 35) %&gt;% dplyr::select(c(2:22))%&gt;% t %&gt;% as.data.frame dpQ2$Quartiles=c(.01, 0.05, .1, .15, .2, .25, .30, .35, .4, .45, .5, .55, .6, .65, .7,.75, .8, .85, .9, .95, .99 ) mean(dpQ2$V1, na.rm=FALSE) ## [1] 0.1416034 var(dpQ2$V1, na.rm=FALSE) ## [1] 0.03357852 #dpQ2 ggplot(dpQ2, aes(Quartiles, V1))+ geom_line() Using the variance calculated in the previous chunk, the \\(\\alpha\\) and \\(\\beta\\) can be calculated. estBetaParams &lt;- function(mu, var) { alpha &lt;- ((1 - mu) / var - 1 / mu) * mu ^ 2 beta &lt;- alpha * (1 / mu - 1) return(params = list(alpha = alpha, beta = beta)) } #mean(Smokers$Y2000P) #var(Smokers$Y2000P) estBetaParams(0.2757771,0.04226223) ## $alpha ## [1] 1.027498 ## ## $beta ## [1] 2.698331 Graphics production. It is observed that for plants that have 25 and 30 flowers the density is skewed to the left, in other words the probability of having few fruits dominates the distributions. library(gridExtra) a=ggplot(dipodium2, aes(propfr))+ stat_function(aes(x = dipodium2$propfr, y = ..y..), fun = dbeta, colour=&quot;red&quot;, n = 62, args = list(shape1 =0.5418068, shape2 =3.135593))+ ylab(&quot;Beta \\nDensity&quot;)+ xlab(&quot;Probabilidad de tener frutos&quot;)+ coord_cartesian(xlim=c(0,0.05))+ ggtitle(&quot;Densidad beta para plantas con 25 flores&quot;) b=ggplot(dipodium2, aes(propfr))+ stat_function(aes(x = dipodium2$propfr, y = ..y..), fun = dbeta, colour=&quot;blue&quot;, n = 62, args = list(shape1 = 0.7549048, shape2 =2.949404))+ ylab(&quot;Beta \\nDensity&quot;)+ xlab(&quot;Probabilidad de tener frutos&quot;)+ coord_cartesian(xlim=c(0,0.1))+ ggtitle(&quot;Densidad beta para plantas con 30 flores&quot;) c=ggplot(dipodium2, aes(propfr))+ stat_function(aes(x = dipodium2$propfr, y = ..y..), fun = dbeta, colour=&quot;black&quot;, n = 62, args = list(shape1 = 1.027498, shape2 =2.698331))+ ylab(&quot;Beta \\nDensity&quot;)+ xlab(&quot;Probabilidad de tener frutos&quot;)+ coord_cartesian(xlim=c(0,0.10))+ ggtitle(&quot;Densidad beta para plantas con 35 flores&quot;) tresDensidad=grid.arrange(a,b,c, ncol=1) ## Warning: Use of `dipodium2$propfr` is discouraged. ## ℹ Use `propfr` instead. ## Use of `dipodium2$propfr` is discouraged. ## ℹ Use `propfr` instead. ## Use of `dipodium2$propfr` is discouraged. ## ℹ Use `propfr` instead. tresDensidad ## TableGrob (3 x 1) &quot;arrange&quot;: 3 grobs ## z cells name grob ## 1 1 (1-1,1-1) arrange gtable[layout] ## 2 2 (2-2,1-1) arrange gtable[layout] ## 3 3 (3-3,1-1) arrange gtable[layout] ggsave(&quot;Figures/tresDensidad.png&quot;) ## Saving 7 x 5 in image ## Warning: Use of `dipodium2$propfr` is discouraged. ## ℹ Use `propfr` instead. For an excellent new step by step use of the beta regression and why it is usefull see this website. https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/ "],["auto-regressive-vectorial-analysis.html", "Chapter 20 Auto Regressive Vectorial Analysis", " Chapter 20 Auto Regressive Vectorial Analysis "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
