# Generalized Linear Models


Test

```{r, GLM_1, echo=FALSE, message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,  huxtable,  SuppDists, univariateML, invgamma)


#detach("package::lubridate", unload=TRUE)
library(tidyverse) # this is a package that includes dplyr, ggplot2, tidyr, readr, purrr, tibble, stringr, forcats
library(huxtable) # this is a package to create tables
#library(wakefield) # this is a package to create random data
library(SuppDists) # this is a package to create distributions
library(univariateML) # this is a package to create distributions
library(invgamma) # this is a package to create distributions
#library(conflicted) # this is a package to solve conflicts in functions
#conflict_prefer("lubridate")
#conflicts_prefer(lubridate::hour) # this is state that if there is a conflict in function to use this package
```

------------------------------------------------------------------------



## Linear models

Generalized linear models, GLM, are an extension of linear models where the dependent variable has a normal distribution. We remember that in a linear regression the dependent variable follows the model $y_i = \beta_0 + \beta_1 x_i$, where $\beta_0$ is the intercept and the $\beta_1$ is the coefficient, that is, the slope and the $x_i$ are the values of x's. One of the assumptions is that the variation in the values of $\mu_i$, which are the $y_i$, have a normal distribution in each x's and that there is homogeneity of variance.

$$\mu_i=\beta_0+\beta_1x_i$$ An important assumption is the assumption that the variation in the $y_i$ are normally distributed and that there is homogeneity of variance.

$$y_i\sim N\left(\mu_i,\ \epsilon\right)$$ We can visualize it with the following figure, where the values of y's have a normal distribution and that this distribution is homogeneous across the values of x's.



## Necesito cambiar este gráfico, ya que viene del web (del siguiente website "<https://towardsdatascience.com>").

```{r, echo=FALSE, fig.show = "hold", out.width = "50%", fig.align = "default", fig.cap="Del siguiente website; https://towardsdatascience.com/generalized-linear-models-9cbf848bb8ab"}
#knitr::include_graphics(c("Figures/Reg_Normal.png"))
```

------------------------------------------------------------------------

## When the dependent variable is not linear


The main problem for a long time has been that the dependent variable does not have a normal distribution and consequently did not meet the assumptions of linear regression. The method of linearizing the response variable was developed by Nelder and Wedderburn 1972. Information about the test and its evolution of GLM can be found at <https://encyclopediaofmath.org/wiki/Generalized_linear_models>. With the advancement of the use of computers in the 80s, it ended up being one of the most used statistical methods.


The term GLM refers to a wide variety of regression models. The assumption in these models is that the response variable $y_i$ follows a distribution within the family of exponential distributions with an average $\mu_i$, where a function $\mu_i^T\beta$ is assumed that is frequently non-linear . To linearize the variable it is necessary to use a "link" to convert the dependent variable, $y_i$.


------------------------------------------------------------------------


## Link available

Here I show a partial list of the different types of "links" for different types of data (or distributions) of the independent variable, $y_i$. The decision of which transformation or **link** to use is necessary will depend on the data types and their distribution. This is only a subset of avaliable link functions.




```{r, GLM_2, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(huxtable)

LinkFunctions<-tribble(
  ~Modelo, ~Variable_Dep, ~Link, ~Variables_Independiente,
  "Linear regressionl", "Normal", "Identidy", "Continuous",
  "ANOVA", "Normal", "Identity", "Categorical",
  "Logistic Regression", "Binomial", "Logit","Mix",
  "Poisson Regression", "Poisson", "Log", "Mix",
)

huxtable(LinkFunctions) %>% 
  set_bold(1, 1:4) %>% 
  set_text_color(1, 1:4, "red") %>% 
  set_bottom_border(row = 1, col = everywhere, value = 1) %>%
set_bottom_border(row = 5, col = everywhere, value = 1) %>%
set_caption("Common Link function in GLM.")
```

Although the previous distributions are very common, they are not the only links available to transform the data. Here is supplementary information on some other R links that are available in certain packages.

<https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family>



```{r, GLM_3, echo=FALSE, message=FALSE, warning=FALSE}
Family<-tribble(
  ~Familia, ~Links, 
  "Gaussian/Normal", "identity, log, inverse", 
  "Binomial", "logit, probit, cauchit, log, cloglog", 
  "Gamma", "inverse, identity, log",
  "Poisson", "log, identity, square root",
  "Inverse Gausian", "1/µ^2, inverse, identity, log"
)

huxtable(Family) %>% 
  set_bold(1, 1:2) %>% 
  set_text_color(1, 1:2, "red") %>% 
  set_bottom_border(row = 1, col = everywhere, value = 1) %>%
set_bottom_border(row = 6, col = everywhere, value = 1) %>%
set_caption("Common link functions in GLM.")

```

------------------------------------------------------------------------

## The assumptions

 - The advantage is that now the dependent variable does not have to have a normal distribution.
 - Furthermore, untransformed data does not have to have a linear relationship. However, the data must come from a distribution of the exponential family, for example, binomial, Poisson, multinomial, normal, inverse normal, etc.\
 - GLM does not assume a linear relationship between the dependent and independent variable, but assumes a linear relationship of the transformed dependent variable data and the independent variable. Consequently, a linear relationship is assumed between the binary variable and the explanatory variable after using the transformation with the link, $logit\left(\pi\right)=\beta_0+\beta_1\cdot x_i$.
 - The assumption of homogeneity of variance does not have to be taken into account.
 - The errors have to be independent but it does not matter if they comply with a normal distribution.

------------------------------------------------------------------------

## The advantages of GLM

  - The parameters are estimated using likelihood (MLE = Maximum Likelihood estimators), not least squares (OLS = ordinary least square). The models are estimated via likelihood, then they optimize the estimators, $\beta$.
  - There is no need to transform the data of the dependent variable to normalize them.
  - The selection of the "link" is independent of the dependent variable(s), which makes it easier to create models.
  - Tools to evaluate inferences and models are available such as evaluating residuals, confidence intervals, deviation, likelihood ratio test and Akaike Information Criterio index among others.

------------------------------------------------------------------------

## Binomial or Bernoulli models

If the variable is binomial, there are only two alternatives, 0 and 1, or whether or not, dead or alive, the binomial distribution is used. The **logit** function is used as a link function and the binomial/Bernoulli distribution as a probability distribution, the model is called **Logistic Regression**.

$$\log\frac{q_i}{1-q_i}=\beta_0+\beta_1X_i$$
where the distribution is a binomial with $y_i\sim Binomial\left(q_i\right)$.

```{r, warning=FALSE, message=FALSE}
#library(wakefield)
x=wakefield::r_sample_binary(50, x = 0:1, prob =c(0.3, 0.7), name = "Binary")
#x
df=as.data.frame(as.factor(x))
#df
ggplot(df, aes(x))+
  geom_histogram(fill="blue")+
  scale_x_continuous(breaks = c(0, 1))+
  xlab("Binary Values")
```

------------------------------------------------------------------------

### Gamma Distribution

The gamma distribution is frequently used to take into account variables that have very long and large tails (Heavy-Tailed distributions). The distribution is widely used in the area of econometrics and survival estimates.

The gamma distribution can be parameterized with a "**shape** term $\alpha = k$ and the inverse of a scale parameter $\beta=1/\theta$ which is known as a * parameter *rate**.

$$f\left(x\right)=\frac{\left(\beta^{\alpha}\cdot x^{\alpha-1}e^{-\beta x}\right)}{\Gamma\left(\alpha\right)}\ para\ x>0,\ \ \alpha,\ \beta>0$$

where $\Gamma\left(\alpha\right)$ is the gamma function. For each integer value $\Gamma\left(\alpha\right)=\left(\alpha-1\right)!$

In another word, the gamma distribution is for modeling continuous variables that are always positive and have skewed distributions.

Examples where the gamma distribution is used

- the time until the moment of failure of a team
- the time until the death of individuals
- the amount of water accumulated in a lake
- the size of unpaid loans

Let's look at some gamma distributions





```{r, GLM_4}

x = 0:20
curve(dgamma(x, shape=1, scale=2), xlab = "x", ylab = "f(x;k,theta)", 0.4, 20, col = 3, lwd = 3, 
    main = "Examples of Gamma Distributions")
curve(dgamma(x, shape=2, scale=2), xlab = "x", ylab = "f(x;k,theta)", 0.4, 20, col = 5, lwd = 3, 
    add = TRUE)
curve(dgamma(x, shape=2, scale=4), xlab = "x", ylab = "f(x;k,theta)", 0.4, 20, col = 6, lwd = 3, 
    add = TRUE)
curve(dgamma(x,shape =5, scale=1), xlab = "x", ylab = "f(x;k,theta)", 0.4, 20, col = 7, lwd = 3, 
    add = TRUE)
curve(dgamma(x, shape=9, scale=0.5), xlab = "x", ylab = "f(x;k,theta)", 0.4, 20, col = 1, lwd = 3, 
    add = TRUE)
curve(dgamma(x, shape=7.5, scale=1), xlab = "x", ylab = "f(x;k,theta)", 0.4, 20, col = 2, lwd = 3, 
    add = TRUE)
legend("topright", c("k=1,theta=2", "k=2,theta=2", "k=2,theta=4","k=5,theta=1","k=9,theta=0.5", "k=7.5,theta=1.0"), col = c(3, 5,6, 7,1,2), lwd = 3, inset = 0.05)
```

## The basic parameters of the gamma.

Note: The **scale** is a dispersion index, and the higher the number, the longer the tail.

The average is equal to the multiplication of shape=k by scale = theta, $$average=k\cdot\theta$$.

The variance is equal to the multiplication of shape=k by scale = (theta)\^2, $$variance=k\cdot\theta^2$$

You can find the formulas to calculate other parameters on the Wikipedia page.\
<https://en.wikipedia.org/wiki/Gamma_distribution>


------------------------------------------------------------------------

### LThe inverse Gaussian distribution

In probability theory, the inverse Gaussian distribution is a two-parameter family of continuous probability distributions with support at (0, ∞).

The distribution follows the following form $$f\left(x;µ,\lambda\right)=\sqrt{\frac{\lambda}{2\pi x^3}}\exp\left(-\frac{\lambda\left(x-µ\right)^2}{2µ^2x}\right)$$

where $x>0$, $µ>0$, and $\lambda$ It is the form of distribution and is always greater than zero. The larger the $\lambda$ (the shape parameter) the more symmetrical the distribution.

As λ tends to infinity, the inverse Gaussian distribution looks like a normal (Gaussian) distribution. The inverse Gaussian distribution is also known as the Wald distribution.

The distribution is used when the population distribution where the lognormal distribution has too heavy a right tail. When referring to heavy tails in statistics, it means that there are more probabilities in this region than a normal distribution. Consequently, the probabilities in this region are greater.

The distribution is used to model non-negative data that is positively skewed. In other words, all values are positive and the tail tends to decrease more slowly than in a normal distribution.



#### Reverse Gaussian history

Historical information on the inverse Gaussian distribution is somewhat limited. The distribution was apparently first derived by Louis Bachelier in 1900, when he was trying to estimate the stock market price for different companies. But the name "Inverse Gaussian" was suggested by Maurice Tweedie in 1945. See this link for more details.

<https://en.wikipedia.org/wiki/Normal-inverse_Gaussian_distribution>

Examples that could be of this type of distribution

- the time it takes to get to a place.
- the distribution of house prices
- the number of children in a family
- the survival of organisms (survival data)

Note that the distribution is not symmetrical, and the larger the $\lambda$ the more symmetrical the distribution.

```{r, GLM_5}
library(SuppDists) 
#dinvGauss(x, nu, lambda, log=FALSE)

x = -1:3
curve(dinvGauss(x, nu=.7, lambda=0.2, log=FALSE), xlab = "x", ylab = "f(x;k,theta)", -0.3, 3, col = 3, lwd = 2, 
    main = "Gráfico distribución Gausiana Inversa")
curve(dinvGauss(x, nu=.7, lambda=1), xlab = "x", ylab = "f(x;k,theta)", -0.3, 3, col = 5, lwd = 2, 
    add = TRUE)
curve(dinvGauss(x, nu=.7, lambda=3), xlab = "x", ylab = "f(x;k,theta)", -0.3, 3, col = 6, lwd = 2, 
    add = TRUE)
curve(dinvGauss(x, nu=2, lambda=0.2), xlab = "x", ylab = "f(x;k,theta)", -0.3, 3, col = 7, lwd = 2, 
    add = TRUE)
curve(dinvGauss(x, nu=2, lambda=1), xlab = "x", ylab = "f(x;k,theta)", -0.3, 3, col = 1, lwd = 2, 
    add = TRUE)
curve(dinvGauss(x, nu=2, lambda=4), xlab = "x", ylab = "f(x;k,theta)", -0.3, 3, col = 2, lwd = 2, 
    add = TRUE)
curve(dinvGauss(x, nu=1, lambda=40), xlab = "x", ylab = "f(x;k,theta)", -0.3, 3, col = 4, lwd = 2, 
    add = TRUE)

legend("topright", c("nu=.7,lambda=0.2", "nu=.7,lambda=1", "nu=.7,lambda=3","nu=2,lambda=0.2","nu=2,lambda=1", "nu=2,lambda=4","nu=1,lambda=40"), 
       col = c(3, 5,6, 7,1,2,4), lwd = 3, inset = 0.05)
```

#### The parameters of the inverse Gaussian distribution

The average is calculated as follows

$$E\left[X\right]=\frac{1}{\mu}+\frac{1}{\lambda}$$

The variance is calculated in the following way

$$Var\left[X\right]=\frac{1}{\mu\lambda}+\frac{2}{\lambda^2}$$

For other parameters see this link

<https://en.wikipedia.org/wiki/Inverse_Gaussian_distribution>

------------------------------------------------------------------------

## Evaluating distributions

Evaluate a data set to visualize the distribution.

If you need to do regression analysis with data that has an inverse Gaussian distribution the following package is available **invGauss** and if you want to determine which is the best distribution for your data with the package **univariateML** there are some functions to help you determine the type of distribution of your data.

I suggest this link.

<https://www.cienciadedatos.net/documentos/55_ajuste_distribuciones_con_r.html>

```{r, GLM_6, warning=FALSE, message=FALSE}
library(invGauss)
library(univariateML)
data(d.oropha.rec)
#d.oropha.rec

ggplot(data = d.oropha.rec) +
  geom_histogram(aes(x = time, y =  after_stat(density)),
                 bins = 40,
                 alpha = 0.3, color = "black") +
  geom_rug(aes(x = time)) +
  stat_function(fun = function(.x){dml(x = .x, obj = mlnorm(d.oropha.rec$time))},
                aes(color = "normal"),
                size = 1) +
  stat_function(fun = function(.x){dml(x = .x, obj = mlinvgauss(d.oropha.rec$time))},
                aes(color = "inverse-normal"),
                size = 1) +
  scale_color_manual(breaks = c("normal", "inverse-normal"),
                     values = c("normal" = "red", "inverse-normal" = "blue")) +
  labs(title = "Distribution of time of development",
       color = "Distribución") +
  theme_bw() +
  theme(legend.position = "bottom")


```
Example of diamond prices from the ggplot2 package

```{r, GLM_7, warning=FALSE, message=FALSE}
#library(univariateML)
ggplot(data = diamonds) +
  geom_histogram(aes(x = price, y =  after_stat(density)),
                 bins = 40,
                 alpha = 0.3, color = "black") +
  geom_rug(aes(x = price)) +
  stat_function(fun = function(.x){dml(x = .x, obj = mllnorm(diamonds$price))},
                aes(color = "log-normal"),
                size = 1) +
  stat_function(fun = function(.x){dml(x = .x, obj = mlinvgauss(diamonds$price))},
                aes(color = "inverse-normal"),
                size = 1) +
  scale_color_manual(breaks = c("log-normal", "inverse-normal"),
                     values = c("log-normal" = "red", "inverse-normal" = "blue")) +
  labs(title = "Distribution of diamond prices",
       color = "Distribution") +
  theme_bw() +
  theme(legend.position = "bottom")
```

***


## Binomial regression

The binomial regression is used when the dependent variable is binary, that is, it has two possible outcomes, 0 or 1, dead or alive, success or failure, etc. The binomial distribution is used as a probability distribution and the **logit** function is used as a link function. The model is called **Logistic Regression**.

$$\log\frac{q_i}{1-q_i}=\beta_0+\beta_1X_i$$
where the distribution is a binomial with $y_i\sim Binomial\left(q_i\right)$.

Here let use the data from Karn and Penrose of the survival or death on infants and birth weight.

 - Survival:  1 = Infant survived, 0 = the Infant died
 - Weigth_lb = The weigth of the infant in pounds at birth.
 
 
```{r, GLM_8, warning=FALSE, message=FALSE}
library(readr)
Karn_Penrose_infant_survivorship <- read_csv("Data/Karn_Penrose_infant_survivorship.csv")

Infant= Karn_Penrose_infant_survivorship

head(Infant)

```

Visualize the data

- The data is not normally distributed.
- The data is binary, 0 or 1.
- The regression line is not linear and is probability
- The 95% confidence interval is shown in the graph.




```{r, GLM_9, warning=FALSE, message=FALSE}
ggplot(Infant, aes(x=Weigth_lb, y=Survival))+
  geom_jitter(width=0.1, height = 0.1)+ # point represents an infants outcome
  geom_smooth(method="glm", method.args=list(family="binomial"), se=T)+ 
    xlab("Weight in pounds")+
  ylab("Probability of Survival")+
  scale_x_continuous(breaks = seq(0, 14, by = 1))


```

Creating the binomial model


```{r, GLM_10, warning=FALSE, message=FALSE}
Infant_model=glm(Survival ~ Weigth_lb, data=Infant, family=binomial)

summary(Infant_model)
```

****


## Poisson regression

A poisson regression is used when the dependent variable is a count variable, that is, the number of events that occur in a fixed period of time. The Poisson distribution is used as a probability distribution and the **log** function is used as a link function. The model is called **Poisson Regression**.

$$\log\left(\lambda_i\right)=\beta_0+\beta_1X_i$$
where the distribution is a Poisson with $y_i\sim Poisson\left(\lambda_i\right)$.

Here let use the data from the number of accidents in a city and the number of cars in the city.

 - Brooklyn_Bridge:  The number cyclist on the Brooklyn Bridge on each day.
 - Precipitation = The precipitation in inches on that day
 
 
```{r, GLM_11, warning=FALSE, message=FALSE}
library(readr)
NY_CITY_CYCLIST <- read_csv("Data/NY_CITY_CYCLIST.csv")

NY=NY_CITY_CYCLIST

head(NY)
```
 
 
Visualize the data

- The data is not normally distributed.
- The data is a count variable.
- The regression line is not linear and is count (number of cylcist)
- The 95% confidence interval is shown in the graph.


```{r, warning=FALSE, message=FALSE}
ggplot(NY, aes(x=Precipitation, y=Brooklyn_Bridge))+
  geom_jitter(width=0.01, height = 0.1)+ # point represents a count of number of cyclist for a specific day
  geom_smooth(method="glm", method.args=list(family="poisson"), se=T)+ 
    xlab("Precipitation in inches")+
  ylab("Number of cyclist")+
  scale_x_continuous(breaks = seq(0, 2, by = 0.1))
```

Creating the Poisson model


```{r, GLM_12, warning=FALSE, message=FALSE}
NY_model=glm(Brooklyn_Bridge ~ Precipitation, data=NY, family=poisson)
summary(NY_model)
```

****

## Negative binomial regression

A negative binomial regression is used when the dependent variable is a count variable and that the data suggests overdipersion.  Overdispersion is when the variance is greater than the mean. The Poisson distribution is used as a probability distribution and the **log** function is used as a link function. The model is called **Negative Binomial Regression**.

$$\log\left(\lambda_i\right)=\beta_0+\beta_1X_i$$
where the distribution is a negative binomial with $y_i\sim NegBinomial\left(\lambda_i\right)$.

We will start by calculating the mean and vaiance of the above data set and detemrine if there is overdisperion

The function requiered **dispersiontest** is available in the package **AER**. The function will return the dispersion parameter and the p-value. If the p-value is less than 0.05, then there is overdispersion.


```{r, GLM_13, warning=FALSE, message=FALSE}
library(AER)
model=glm(Brooklyn_Bridge ~ Precipitation, data=NY, family=poisson)

disp=dispersiontest(model, trafo=1)

```

Note that the alpha is very large and the p-value is less than 0.05, which indicates that there is overdispersion. We will now create the negative binomial model. We will use the MASS library and the function **glm.nb** to create the model. The function is used to create a negative binomial model. The function is used in the same way as the glm function, but the family is set to negative binomial.


```{r, GLM_14, warning=FALSE, message=FALSE}
library(MASS)

ggplot(NY, aes(x=Precipitation, y=Brooklyn_Bridge))+
  geom_jitter(width=0.01, height = 0.1)+
  geom_smooth(method="glm.nb", se=T)+ 
    xlab("Precipitation in inches")+
  ylab("Number of cyclist")+
  scale_x_continuous(breaks = seq(0, 2, by = 0.1))

NY_model_nb=MASS::glm.nb(Brooklyn_Bridge ~ Precipitation, data=NY)

summary(NY_model_nb)
```

We interpret the results as follows

- The precipitation has a negative effect on the number of cyclist (negative sign). With precipitation (exp(coef) = -0.53) which states that there is 47% decrease in the number of cyclist for **each inch of precipitation**. 

We can reconvert the coefficient to a positive value by taking the exponential of the coefficient. 

```{r, GLM_15, warning=FALSE, message=FALSE}
exp(coef(model))
```



Overlaying the Poisson and the negative binomial models

NEED To FIND a better example


```{r, GLM_16, warning=FALSE, message=FALSE}
ggplot(NY, aes(x=Precipitation, y=Brooklyn_Bridge))+
  geom_jitter(width=0.01, height = 0.1)+
  geom_smooth(method="glm", method.args=list(family="poisson"), se=T, color="red")+ 
  geom_smooth(method="glm.nb", se=T, color="blue")+ 
    xlab("Precipitation in inches")+
  ylab("Number of cyclist")+
  scale_x_continuous(breaks = seq(0, 2, by = 0.1))
```

